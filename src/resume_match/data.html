<!doctype html><html lang="en"><head><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title data-rh="true">Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading…</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2019-01-20T13:00:17.542Z"/><meta data-rh="true" name="title" content="Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading…"/><meta data-rh="true" property="og:title" content="Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading…"/><meta data-rh="true" property="twitter:title" content="Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading…"/><meta data-rh="true" name="twitter:site" content="@Medium"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/38dc5cf5a5a3"/><meta data-rh="true" property="al:android:url" content="medium://p/38dc5cf5a5a3"/><meta data-rh="true" property="al:ios:url" content="medium://p/38dc5cf5a5a3"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="In this article, I will explain how we can create Deep Learning based Conversational AI. The basic definition of chatbot is, it is a computer software program designed to simulate human conversation…"/><meta data-rh="true" property="og:description" content="keywords: NLU, NLG, Word Embedding, RNN, Bi-directional LSTM, Generative Adversarial Network, Machine Reading Comprehension, Transfer…"/><meta data-rh="true" property="twitter:description" content="keywords: NLU, NLG, Word Embedding, RNN, Bi-directional LSTM, Generative Adversarial Network, Machine Reading Comprehension, Transfer…"/><meta data-rh="true" property="og:url" content="https://medium.com/@BhashkarKunal/conversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3"/><meta data-rh="true" property="al:web:url" content="https://medium.com/@BhashkarKunal/conversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*-1u87cEgguuCa5QgU93m7A.jpeg"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*-1u87cEgguuCa5QgU93m7A.jpeg"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" property="article:author" content="https://medium.com/@BhashkarKunal"/><meta data-rh="true" name="twitter:creator" content="@BhashkarKunal"/><meta data-rh="true" name="author" content="Kunal Bhashkar"/><meta data-rh="true" name="robots" content="index,follow"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" name="twitter:label1" value="Reading time"/><meta data-rh="true" name="twitter:data1" value="59 min read"/><meta data-rh="true" name="parsely-post-id" content="38dc5cf5a5a3"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"/><link data-rh="true" rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"/><link data-rh="true" rel="author" href="https://medium.com/@BhashkarKunal"/><link data-rh="true" rel="canonical" href="https://medium.com/@BhashkarKunal/conversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/38dc5cf5a5a3"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*-1u87cEgguuCa5QgU93m7A.jpeg"],"url":"https:\u002F\u002Fmedium.com\u002F@BhashkarKunal\u002Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3","dateCreated":"2019-01-13T07:36:22.973Z","datePublished":"2019-01-13T07:36:22.973Z","dateModified":"2019-01-20T13:00:17.542Z","headline":"Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading…","name":"Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading…","description":"In this article, I will explain how we can create Deep Learning based Conversational AI. The basic definition of chatbot is, it is a computer software program designed to simulate human conversation…","identifier":"38dc5cf5a5a3","keywords":["Lite:true","Tag:Deep Learning","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:0"],"author":{"@type":"Person","name":"Kunal Bhashkar","url":"https:\u002F\u002Fmedium.com\u002F@BhashkarKunal"},"creator":["Kunal Bhashkar"],"publisher":{"@type":"Organization","name":"Medium","url":"https:\u002F\u002Fmedium.com\u002F","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F616\u002F1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002F@BhashkarKunal\u002Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3"}</script><script data-rh="true" >(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="https://cdn.optimizely.com/js/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="470" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:fixed}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ai{max-width:1192px}.aj{min-width:0}.ak{width:100%}.al{height:65px}.ao{flex:1 0 auto}.ap{display:none}.ar{flex:0 0 auto}.as{visibility:hidden}.at{color:inherit}.au{fill:inherit}.av{font-size:inherit}.aw{border:inherit}.ax{font-family:inherit}.ay{letter-spacing:inherit}.az{font-weight:inherit}.ba{padding:0}.bb{margin:0}.bc:hover{cursor:pointer}.bd:hover{color:rgba(0, 0, 0, 0.9)}.be:hover{fill:rgba(0, 0, 0, 0.9)}.bf:focus{outline:none}.bg:disabled{cursor:default}.bh:disabled{color:rgba(0, 0, 0, 0.54)}.bi:disabled{fill:rgba(0, 0, 0, 0.54)}.bj{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.bk{font-style:normal}.bl{line-height:20px}.bm{font-size:15.8px}.bn{letter-spacing:0px}.bo{color:rgba(0, 0, 0, 0.54)}.bp{fill:rgba(0, 0, 0, 0.54)}.bq{margin-left:16px}.br{color:rgba(2, 158, 116, 1)}.bs{fill:rgba(3, 168, 124, 1)}.bt:hover{color:rgba(1, 143, 105, 1)}.bu:hover{fill:rgba(2, 158, 116, 1)}.bv:disabled{color:rgba(3, 168, 124, 0.5)}.bw:disabled{fill:rgba(3, 168, 124, 0.5)}.bx{padding:4px 12px}.by{background:0}.bz{border-color:rgba(3, 168, 124, 1)}.ca:hover{border-color:rgba(2, 158, 116, 1)}.cb{border-radius:4px}.cc{border-width:1px}.cd{border-style:solid}.ce{box-sizing:border-box}.cf{display:inline-block}.cg{text-decoration:none}.ch{margin-bottom:0px}.cj{padding-left:24px}.ck{padding-right:24px}.cl{margin-left:auto}.cm{margin-right:auto}.cn{max-width:728px}.co{flex-direction:column}.cp{position:absolute}.cq{top:calc(100vh + 100px)}.cr{bottom:calc(100vh + 100px)}.cs{width:10px}.ct{pointer-events:none}.cu{margin-bottom:20px}.cv{max-width:680px}.cw{margin-top:32px}.cx{justify-content:space-between}.db{border-radius:50%}.dc{height:48px}.dd{width:48px}.de{margin-left:12px}.df{color:rgba(0, 0, 0, 0.84)}.dg{margin-bottom:2px}.di{font-weight:300}.dj{font-size:16px}.dk{overflow:hidden}.dl{max-height:20px}.dm{text-overflow:ellipsis}.dn{display:-webkit-box}.do{-webkit-line-clamp:1}.dp{-webkit-box-orient:vertical}.dq:hover{text-decoration:underline}.dr{margin-left:8px}.ds{padding:0px 8px}.dt{border-color:rgba(0, 0, 0, 0.54)}.du:hover{color:rgba(0, 0, 0, 0.97)}.dv:hover{fill:rgba(0, 0, 0, 0.97)}.dw:hover{border-color:rgba(0, 0, 0, 0.84)}.dx:disabled{fill:rgba(0, 0, 0, 0.76)}.dy:disabled{border-color:rgba(0, 0, 0, 0.2)}.dz:disabled{cursor:inherit}.ea:disabled:hover{color:rgba(0, 0, 0, 0.54)}.eb:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.ec:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.ed{line-height:18px}.ee{font-size:15px}.ef{align-items:flex-end}.en{padding-right:8px}.eo{margin-right:-6px}.ep{word-break:break-word}.eq{word-wrap:break-word}.er:after{display:block}.es:after{content:""}.et:after{clear:both}.eu{max-width:1280px}.fa{clear:both}.fb{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.fc{cursor:zoom-in}.fd{position:relative}.fe{z-index:auto}.ff{opacity:0}.fg{transition:opacity 100ms 400ms}.fh{height:100%}.fi{will-change:transform}.fj{transform:translateZ(0)}.fk{margin:auto}.fl{background-color:rgba(0, 0, 0, 0.05)}.fm{padding-bottom:56.25%}.fn{filter:blur(20px)}.fo{transform:scale(1.1)}.fp{visibility:visible}.fq{background:rgba(255, 255, 255, 1)}.fr{line-height:1.4}.fs{margin-top:10px}.ft{text-align:center}.fw{background-repeat:repeat-x}.fx{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.fy{background-size:1px 1px}.fz{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.ga{line-height:1.18}.gb{letter-spacing:-0.022em}.gc{font-weight:600}.gn{margin-bottom:-0.31em}.go{box-shadow:inset 3px 0 0 0 rgba(0, 0, 0, 0.84)}.gp{padding-left:23px}.gq{margin-left:-20px}.gr{line-height:1.58}.gs{letter-spacing:-0.004em}.gt{font-style:italic}.gu{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.hf{margin-bottom:-0.46em}.hg{font-weight:700}.hh{list-style-type:decimal}.hi{margin-left:30px}.hj{padding-left:0px}.hk{max-width:1036px}.hq{padding-bottom:44.20849420849421%}.hr{max-width:1024px}.hs{padding-bottom:44.82421874999999%}.ht{max-width:800px}.hu{padding-bottom:45%}.hv{max-width:476px}.hw{padding-bottom:97.47899159663865%}.hx{max-width:650px}.hy{padding-bottom:55.07692307692308%}.ie{max-width:889px}.if{padding-bottom:71.8785151856018%}.il{padding:20px}.im{background:rgba(0, 0, 0, 0.05)}.in{overflow-x:auto}.io{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.ip{margin-top:-0.09em}.iq{margin-bottom:-0.09em}.ir{white-space:pre-wrap}.is{max-width:580px}.it{padding-bottom:67.41379310344827%}.iu{max-width:1022px}.iv{padding-bottom:41.68297455968688%}.iw{max-width:570px}.ix{padding-bottom:47.368421052631575%}.iy{max-width:948px}.iz{padding-bottom:39.24050632911393%}.ja{max-width:883px}.jb{padding-bottom:30.464326160815403%}.jc{max-width:827px}.jd{padding-bottom:48.36759371221282%}.je{max-width:725px}.jf{padding-bottom:79.0344827586207%}.jg{max-width:950px}.jh{padding-bottom:38.31578947368421%}.ji{max-width:1312px}.jj{padding-bottom:46.1890243902439%}.jk{list-style-type:disc}.jl{padding:2px 4px}.jm{font-size:75%}.jn> strong{font-family:inherit}.jo{max-width:747px}.jp{padding-bottom:83.13253012048193%}.jq{max-width:610px}.jr{padding-bottom:70.98360655737706%}.js{padding-bottom:45.375%}.jt{max-width:949px}.ju{padding-bottom:43.41412012644889%}.jv{max-width:851px}.jw{padding-bottom:50.29377203290247%}.jx{max-width:648px}.jy{padding-bottom:106.63580246913581%}.jz{max-width:711px}.ka{padding-bottom:48.52320675105485%}.kb{max-width:953px}.kc{padding-bottom:38.09024134312696%}.kd{max-width:495px}.ke{padding-bottom:58.18181818181818%}.kf{max-width:764px}.kg{padding-bottom:35.340314136125656%}.kh{max-width:637px}.ki{padding-bottom:32.182103610675036%}.kj{max-width:772px}.kk{padding-bottom:40.93264248704663%}.kl{max-width:842px}.km{padding-bottom:44.18052256532066%}.kn{max-width:593px}.ko{padding-bottom:103.20404721753795%}.kp{max-width:850px}.kq{padding-bottom:46.588235294117645%}.kr{max-width:575px}.ks{padding-bottom:51.826086956521735%}.kt{max-width:100%}.ku{max-width:550px}.kv{padding-bottom:81.63636363636364%}.kw{max-width:1060px}.kx{padding-bottom:44.528301886792455%}.ky{max-width:2268px}.kz{padding-bottom:13.668430335097002%}.la{max-width:1095px}.lb{padding-bottom:25.93607305936073%}.lc{max-width:1115px}.ld{padding-bottom:54.26008968609865%}.le{max-width:535px}.lf{padding-bottom:79.25233644859813%}.lg{max-width:768px}.lh{padding-bottom:96.35416666666666%}.li{max-width:778px}.lj{padding-bottom:73.39331619537275%}.lk{max-width:1366px}.ll{padding-bottom:56.22254758418741%}.lm{max-width:631px}.ln{padding-bottom:83.83518225039619%}.lo{max-width:1002px}.lp{padding-bottom:48.203592814371255%}.lq{max-width:682px}.lr{padding-bottom:51.90615835777126%}.ls{max-width:1300px}.lt{padding-bottom:67.38461538461539%}.lu{max-width:1011px}.lv{padding-bottom:46.587537091988125%}.lw{max-width:859px}.lx{padding-bottom:39.697322467986034%}.ly{padding-bottom:46.34420697412824%}.lz{max-width:854px}.ma{padding-bottom:41.92037470725995%}.mb{max-width:1097px}.mc{padding-bottom:36.28076572470374%}.md{max-width:769px}.me{padding-bottom:45.123537061118334%}.mf{max-width:808px}.mg{padding-bottom:39.48019801980198%}.mh{padding-bottom:63.75%}.mi{will-change:opacity}.mj{width:188px}.mk{left:50%}.ml{transform:translateX(406px)}.mm{top:calc(65px + 54px + 14px)}.mp{top:calc(65px + 54px + 40px)}.mr{width:131px}.ms{padding-top:28px}.mt{margin-bottom:19px}.mu{margin-left:-5px}.mv{margin-right:5px}.mw{outline:0}.mx{border:0}.my{user-select:none}.mz{cursor:pointer}.na> svg{pointer-events:none}.nb:active{border-style:none}.nc{-webkit-user-select:none}.nd{fill:rgba(0, 0, 0, 0.76)}.ne:focus{fill:rgba(0, 0, 0, 0.54)}.nf:hover{fill:rgba(0, 0, 0, 0.54)}.ng{margin-top:5px}.nh button{text-align:left}.ni{margin-top:40px}.nj{flex-wrap:wrap}.nk{margin-top:25px}.nl{list-style-type:none}.nm{margin-right:8px}.nn{margin-bottom:8px}.no{border-radius:3px}.np{padding:5px 10px}.nq{line-height:22px}.nr{margin-top:15px}.ns{margin-right:16px}.nt{border:1px solid rgba(0, 0, 0, 0.1)}.nu{height:60px}.nv{width:60px}.oi:hover{border-color:rgba(0, 0, 0, 0.54)}.oj:active{border-style:solid}.ok{z-index:2}.om{padding-top:32px}.on{border-top:1px solid rgba(0, 0, 0, 0.1)}.oo{margin-bottom:25px}.op{margin-bottom:32px}.oq{min-height:80px}.ov{height:80px}.ow{width:80px}.ox{padding-left:102px}.oz{text-transform:uppercase}.pa{letter-spacing:0.05em}.pb{margin-bottom:6px}.pc{font-size:28px}.pd{line-height:36px}.pe{max-width:555px}.pf{max-width:450px}.pg{font-size:18px}.ph{line-height:24px}.pj{padding-top:25px}.pk{color:rgba(0, 0, 0, 0.76)}.pl{opacity:1}.pm{border:1px solid rgba(3, 168, 124, 1)}.pn{margin-top:64px}.po{background-color:rgba(0, 0, 0, 0.02)}.pp{padding:60px 0}.pq{background-color:rgba(0, 0, 0, 0.9)}.qh{padding-bottom:48px}.qi{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.qj{margin:0 -12px}.qk{margin:0 12px}.ql{flex:1 1 0}.qm{padding-bottom:12px}.qn:hover{color:rgba(255, 255, 255, 0.99)}.qo:hover{fill:rgba(255, 255, 255, 0.99)}.qp:disabled{color:rgba(255, 255, 255, 0.7)}.qq:disabled{fill:rgba(255, 255, 255, 0.7)}.qr{color:rgba(255, 255, 255, 0.98)}.qs{fill:rgba(255, 255, 255, 0.98)}.qt{text-align:inherit}.qu{font-size:21.6px}.qv{letter-spacing:-0.32px}.qw{color:rgba(255, 255, 255, 0.7)}.qx{fill:rgba(255, 255, 255, 0.7)}.qy{text-decoration:underline}.qz{padding-bottom:8px}.ra{padding-top:8px}.rb{width:200px}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.ah{margin:0 64px}.em{margin-left:30px}.ez{margin-top:32px}.gl{font-size:26px}.gm{margin-top:1.72em}.hd{font-size:21px}.he{margin-top:2em}.hp{margin-top:56px}.id{margin-top:0.86em}.ik{margin-top:1.05em}.qe{padding-left:64px}.qf{padding-right:64px}.qg{max-width:1320px}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.el{margin-left:30px}.fu{margin-left:auto}.fv{text-align:center}.qb{padding-left:64px}.qc{padding-right:64px}.qd{max-width:1080px}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.ek{margin-left:30px}.py{padding-left:48px}.pz{padding-right:48px}.qa{max-width:904px}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.am{height:56px}.an{display:flex}.aq{display:block}.ci{margin-bottom:0px}.cz{margin-top:32px}.da{flex-direction:column-reverse}.ei{margin-bottom:30px}.ej{margin-left:0px}.or{margin-bottom:24px}.os{align-items:center}.ot{width:102px}.ou{position:relative}.oy{padding-left:0}.pi{margin-top:24px}.pr{padding:32px 0}.pv{padding-left:24px}.pw{padding-right:24px}.px{max-width:728px}.rc{width:140px}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.ac{margin:0 24px}.cy{margin-top:32px}.dh{margin-bottom:0px}.eg{margin-bottom:30px}.eh{margin-left:0px}.ev{margin-top:24px}.gd{font-size:24px}.ge{margin-top:1.23em}.gv{font-size:18px}.gw{margin-top:1.56em}.hl{margin-top:40px}.hz{margin-top:0.67em}.ig{margin-top:1.34em}.ps{padding-left:24px}.pt{padding-right:24px}.pu{max-width:552px}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ag{margin:0 64px}.ey{margin-top:32px}.gj{font-size:26px}.gk{margin-top:1.72em}.hb{font-size:21px}.hc{margin-top:2em}.ho{margin-top:56px}.ic{margin-top:0.86em}.ij{margin-top:1.05em}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.af{margin:0 48px}.ex{margin-top:32px}.gh{font-size:26px}.gi{margin-top:1.72em}.gz{font-size:21px}.ha{margin-top:2em}.hn{margin-top:56px}.ib{margin-top:0.86em}.ii{margin-top:1.05em}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ae{margin:0 24px}.ew{margin-top:24px}.gf{font-size:24px}.gg{margin-top:1.23em}.gx{font-size:18px}.gy{margin-top:1.56em}.hm{margin-top:40px}.ia{margin-top:0.67em}.ih{margin-top:1.34em}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="print">.ab{display:none}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.y{transition:transform 300ms ease}.z{will-change:transform}.mn{transition:opacity 200ms}.nw{transition:border-color 150ms ease}.nx::before{background:
      radial-gradient(circle, rgba(0, 0, 0, 0.84) 60%, transparent 70%)
    }.ny::before{border-radius:50%}.nz::before{content:""}.oa::before{display:block}.ob::before{z-index:0}.oc::before{left:0}.od::before{height:100%}.oe::before{position:absolute}.of::before{top:0}.og::before{width:100%}.oh:hover::before{animation:k1 2000ms infinite cubic-bezier(.1,.12,.25,1)}.ol{transition:fill 200ms ease}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (max-width: 1230px)">.mo{display:none}</style><style type="text/css" data-fela-rehydration="470" data-fela-type="RULE" media="all and (max-width: 1198px)">.mq{display:none}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><nav class="r s t u v c w x y z ab"><div class="branch-journeys-top"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="al n o am an"><div class="r ao w"><div class="n o"><a aria-label="Homepage" rel="noopener" href="/?source=post_page-----38dc5cf5a5a3----------------------"><div class="ap g"><svg height="22" width="112" viewBox="0 0 111.5 22" class="q"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></div><div class="r aq"><svg width="35" height="35" viewBox="5 5 35 35" class="q"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></div></a></div></div><div class="r ar w"><div class="n o"><div class="n g"><div class="as" id="lo-post-page-navbar-upsell-button"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><span class="bj b bk bl bm bn r bo bp">Become a member</span></a></div></div><div class="as" id="lo-post-page-navbar-sign-in-link"><div class="bq r"><span class="bj b bk bl bm bn r bo bp"><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40BhashkarKunal%2Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3&amp;source=post_page-----38dc5cf5a5a3---------------------nav_reg-" class="br bs av aw ax ay az ba bb bc bt bu bf bg bv bw" rel="noopener">Sign in</a></span></div></div></div><div class="as" id="lo-post-page-navbar-open-in-app-button"><div class="bq r"><a href="https://link.medium.com/a/key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm?%24fallback_url=https%3A%2F%2Fmedium.com%2Fp%2F38dc5cf5a5a3&amp;source=post_page-----38dc5cf5a5a3----------------------" class="bx by br bs bz bt bu ca bc cb bj b bk bl bm bn cc cd ce cf cg bf" rel="noopener">Open In App</a></div></div></div></div></div></div></div></div></nav><div class="ch al r ci am"></div><article><section class="cj ck cl cm ak cn ce n co"></section><span class="r"></span><div><div class="cp u cq cr cs ct"></div><section class="cu"><div class="n p"><div class="ac ae af ag ah cv aj ak"><div class="cw"><div class="n cx cy cz da"><div class="o n"><div><a rel="noopener" href="/@BhashkarKunal?source=post_page-----38dc5cf5a5a3----------------------"><img alt="Kunal Bhashkar" class="r db dc dd" src="https://miro.medium.com/fit/c/96/96/2*HoGS0ii9whuuXey5DW-URw.jpeg" width="48" height="48"/></a></div><div class="de ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r df q"><div class="dg n o dh"><span class="bj di dj bl dk dl dm dn do dp df"><a class="at au av aw ax ay az ba bb bc dq bf bg bh bi" rel="noopener" href="/@BhashkarKunal?source=post_page-----38dc5cf5a5a3----------------------">Kunal Bhashkar</a></span><div class="dr r ar h"><button class="ds df q by dt du dv dw bc bh dx dy dz ea eb ec cb bj b bk ed ee bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj di dj bl dk dl dm dn do dp bo"><div><a class="at au av aw ax ay az ba bb bc dq bf bg bh bi" rel="noopener" href="/@BhashkarKunal/conversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3?source=post_page-----38dc5cf5a5a3----------------------">Jan 13, 2019</a> <!-- -->·<!-- --> <!-- -->59<!-- --> min read</div></span></span></div></div><div class="n ef eg eh ei ej ek el em ab"><div class="n o"><div class="en r ar"><a href="//medium.com/p/38dc5cf5a5a3/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="en r ar"><a href="//medium.com/p/38dc5cf5a5a3/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="eo r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40BhashkarKunal%2Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></section><section class="ep eq er es et"><div class="n p"><div class="ac ae af ag ah cv aj ak"><figure class="ev ew ex ey ez fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm eu"><div class="fk r fd fl"><div class="fm r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*-1u87cEgguuCa5QgU93m7A.jpeg?q=20" width="1280" height="720" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1280" height="720" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2560/1*-1u87cEgguuCa5QgU93m7A.jpeg" width="1280" height="720" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://www.youtube.com/watch?v=8lG6qRIdSA0" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><h2 id="a2b8" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading Comprehension, Transfer Learning, Sequence to Sequence Model with multi-headed attention mechanism, Generative Adversarial Network, Self Learning based Sentiment Analysis and Deep Reinforcement Learning can help in Dialog Management for Conversational AI chatbot</h2><blockquote class="go gp gq"><p id="bc5b" class="gr gs df gt gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">keywords:</strong> NLU, NLG, Word Embedding, Tensorflow, RNN, Bi-directional LSTM, Generative Adversarial Network, Machine Reading Comprehension, Transfer Learning, Sequence to Sequence Model with multi-headed attention mechanism, Deep Reinforcement Learning, Self-learning based on Sentiment Analysis, Knowledge base, Recurrent Embedding Dialogue policy, Dual Encoder LSTM, Encoder-Decoder</p></blockquote><ol class=""><li id="c755" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf hh hi hj"><strong class="gu hg">Introduction</strong></li></ol><p id="b30a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In this article, I will explain how we can create Deep Learning based Conversational AI. The basic definition of chatbot is, it is a computer software program designed to simulate human conversation via text or audio messages. Today’s AI systems can interact with users, understand their needs, map their preferences and recommend an appropriate line of action with minimal or no human intervention. There are lot of popular conversational agents are available today like Apple’s Siri, Microsoft’s Cortana, Google Assistant, and Amazon’s Alexa.</p><p id="a441" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The basic foundation of chatbots is providing the best response of any query that it receives. The best response like answering the sender questions, providing sender relevant information, ask follow-up questions and do the conversation in realistic way.</p><p id="6bf6" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture illustrate the conceptual map of Chatbot using Deep learning,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm hk"><div class="fk r fd fl"><div class="hq r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*k4jQiwTE1SZEFcqi3Ayu0w.png?q=20" width="1036" height="458" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1036" height="458" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2072/1*k4jQiwTE1SZEFcqi3Ayu0w.png" width="1036" height="458" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://scholarworks.sjsu.edu/etd_projects/630/" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="4fb1" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The chatbot needs to be able to understand the intentions of the sender’s message, determine what type of response message (a follow-up question, direct response, etc.) is required, and follow correct grammatical and lexical rules while forming the response. Some models may use additional meta information from data, such as speaker id, gender, emotion. Sometimes, sentiment analysis is used to<strong class="gu hg"> </strong>allows the chatbot to ‘understand’ the mood of the user by analysing verbal and sentence structuring clues.</p><p id="a1d7" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following picture shows that how Deep learning based chatbot work internally,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm hr"><div class="fk r fd fl"><div class="hs r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*1X_eOEjeOfjZyRo68Ool1Q.jpeg?q=20" width="1024" height="459" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1024" height="459" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2048/1*1X_eOEjeOfjZyRo68Ool1Q.jpeg" width="1024" height="459" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://www.microsoft.com/en-us/research/project/deep-reinforcement-learning-goal-oriented-dialogue/" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="8d79" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">2. Role of NLU, NLG and Dialogue Management in Conversational AI</strong></p><p id="32e5" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Natural Language Understanding</strong></p><p id="72de" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The NLU unit is responsible for transforming the user utterance to a predefined semantic frame according to the system’s conventions, i.e. to a format understandable for the system. This includes a task of slot filling and intent detection. For example, the intent, could be a greeting, like Hello, Hi, Hey, or it could have an inform nature, for example I like Indian food, where the user is giving some additional information. Depending on the interests, the slots could be very diverse, like the actor name, price, start time, destination city etc. As we can see, the intents and the slots are defining the closed-domain nature of the Chatbot. The task of slot filling and intent detection is seen as a sequence tagging problem. For this reason, the NLU component is usually implemented as an LSTM-based recurrent neural network with a Conditional Random Field (CRF) layer on top of it. The model presented is a sequence-to-sequence model using bidirectional LSTM network, which fills the slots and predicts the intent in the same time. On the other hand, the model is doing the same using an attention-based RNN. To achieve such a task, the dataset labels consist of: concatenated B–I–O (Begin, Inside, Outside) slot tags, the intent tag and an additional end-of-string (EOS) tag. As an example, in a restaurant reservation scenario, given the sentence Are there any French restaurants in Toronto downtown?, the task is to correctly output, or fill, the following slots: {cuisine: French} and {location: Toronto downtown}.</p><p id="3f8a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following picture shows the classification process for intent classification using Neural Network as,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm ht"><div class="fk r fd fl"><div class="hu r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*6zmRX0OyRnEKM35DI0J8oA.png?q=20" width="800" height="360" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="800" height="360" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1600/1*6zmRX0OyRnEKM35DI0J8oA.png" width="800" height="360" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a class="at cg fw fx fy fz" target="_blank" rel="noopener" href="/skyshidigital/topic-and-intent-classifier-from-scratch-83278fb8cf3?fbclid=IwAR04l7oDNvmy6W4gHyKUoS8sgatgPzKJKD1Qvu6WqtGvGSm7neFbYInjFNc">source</a></figcaption></figure><p id="59f3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Natural Language Generator (NLG)</strong></p><p id="f319" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Natural Language Generation (NLG) is the process of generating text from a meaning representation. It can be taken as the reverse of the natural language understanding. NLG systems provide a critical role for text summarization, machine translation, and dialog systems. In the NLG, The system response as a semantic frame, it maps back to a natural language sentence, understandable for the end user. The NLG component can be rule-based or model-based. In some scenarios it can be a hybrid model, i.e. combination of both. The rule-based NLG outputs some predefined template sentences for a given semantic frame, thus they are very limited without any generalisation power. While several general-purpose rule-based generation systems have been developed, they are often quite difficult to adapt to small, task-oriented applications because of their generality. Machine learning based (trainable) NLG systems are more common in today’s dialog systems. Such NLG systems use several sources as input such as: content plan, representing meaning representation of what to communicate with the user, knowledge base, structured database to return domain-specific entities, user model, a model that imposes constraints on output utterance, dialog history, the information from previous turns to avoid repetitions, referring expressions, etc.</p><p id="1659" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Trainable NLG systems can produce various candidate utterances (e.g., scholastically or rule base) and use a statistical model to rank them. The statistical model assigns scores to each utterance and is learnt based on textual data. Most of these systems use bigram and trigram language models to generate utterances.</p><p id="9039" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">On the other hand, In NLG based on a semantically controlled Long Short-term Memory (LSTM) recurrent network, It can learn from unaligned data by jointly optimising its sentence planning and surface realisation components using a simple cross entropy training criterion without any heuristics, and good quality language variation is obtained simply by randomly sampling the network outputs.</p><p id="4a4e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following figure shows that the working of Semantic Controlled LSTM cell,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm hv"><div class="fk r fd fl"><div class="hw r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*zv4FHb-C6hPNyjJGTlr_Xg.png?q=20" width="476" height="464" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="476" height="464" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/952/1*zv4FHb-C6hPNyjJGTlr_Xg.png" width="476" height="464" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://arxiv.org/abs/1508.01745" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="a541" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Dialogue Management (DM)</strong></p><p id="1b82" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The DM could be connected to some external Knowledge Base (KB) or Data Base (DB), such that it can produce more meaningful answers. The Dialogue Manager consists the following two components: the Dialogue State Tracker (DST) and the Policy Learning which is the Reinforcement Learning (RL) agent. The Dialogue State Tracker (DST) is a complex and essential component that should correctly infer the belief about the state of the dialogue, given all the history up to that turn. The Policy Learning is responsible for selecting the best action, i.e. the system response to the user utterance, that should lead the user towards achieving the goal in a minimal number of dialogue turns.</p><p id="b31c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following figure is shows that how dialogue state Tracker and RL agent are working together,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm hx"><div class="fk r fd fl"><div class="hy r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*AdAo9Y8mrmyj75ZnZi7iGA.jpeg?q=20" width="650" height="358" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="650" height="358" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1300/1*AdAo9Y8mrmyj75ZnZi7iGA.jpeg" width="650" height="358" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><h2 id="c6f6" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Types of dialog management</h2><p id="af85" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">I will discuss the different types of dialog management and how they handle these principles.</p><h2 id="1684" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Finite state machine</h2><p id="f61a" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">The powers of a Finite State Machine are quite extensive. Most conversations can be implemented by a FSM. They are especially good when the number of things a user can say are limited. Most tools for building a conversational bot will also provide a tool to make a decision diagram. So most bots will have a FSM underneath their hood.</p><p id="82d4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A network with distributed terminals sometime can be modelled as a finite state machine with several ports. We define in the following the concept of multi-port finite state machines, which is a generalisation of finite state machines with two ports shows in following figure,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm ie"><div class="fk r fd fl"><div class="if r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*FiqG-5MYxAzDonYlE0sezQ.png?q=20" width="889" height="639" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="889" height="639" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1778/1*FiqG-5MYxAzDonYlE0sezQ.png" width="889" height="639" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=C2860BE9BDEE552C956B9FA8CA639335?doi=10.1.1.131.4341&amp;rep=rep1&amp;type=pdf" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><h2 id="ca28" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Switch statement</h2><p id="3684" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">The most basic type of dialog management is a large switch statement. Every <strong class="gu hg">intent </strong>triggers a different response. E.g. “Hallo” → “Hi!”, “What’s your name?” → “My name is chatbot”, “What does NLU mean?” → “Natural Language Understanding”, “How are you?” → “I’m doing great!”, etc….</p><h2 id="5f3c" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Goal based</h2><p id="76f8" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">In a complex conversation you cannot think about dialogs as a set of states because the number of states can quickly become unmanageable. So you need to approach conversations differently. A popular way of thinking about them is thinking about them in terms of goals.</p><p id="1a35" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Say that your user ask for the location of a restaurant without giving it’s name.</p><p id="5126" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">i. Your system will receive a “looking_for_restaurant”-intent and start a new goal “finding_restaurant”.</p><p id="873b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">ii. It will notice that to finish this goal it needs to know the name of the restaurant. It therefore will ask the user for the name.</p><p id="865a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">iii. When the user answers it will first analyze this response to see if it contains the name of the restaurant. If it does, it will save the name in its context.</p><p id="21da" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">iv. Finally the system will see if it now can finish the “finding_restaurant”-goal. Since the name of the restaurant is now known, it can lookup the restaurant’s location and tell it to the user.</p><p id="f93b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">This type of dialog management works based on behaviours instead of states. It’s easier to manage different ways of asking the same question, context switching or making decisions based on what you know about the user.</p><p id="05e5" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Belief based</strong></p><p id="7b45" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Most NLU will classify intents and entities with a certain degree of uncertainty. This means that dialog manager can only assume what the user said and actually can’t work with discrete rules but needs to work with beliefs.</p><p id="c95f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">3. Types of Conversational AI</strong></p><p id="c0f2" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Rule Based Chatbot</strong></p><p id="9e84" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In a rule-based approach, a bot answers questions based on some rules on which it is trained on. The rules defined can be very simple to very complex. The creation of these bots are relatively straightforward using some rule-based approach, but the bot is not efficient in answering questions, whose pattern does not match with the rules on which the bot is trained. However, these systems aren’t able to respond to input patterns or keywords that don’t match existing rules. One of such languages is AIML (Artificial Intelligence Markup Language): The AIML language´s purpose is to make the task of dialog modeling easy, according to the stimulus-response approach. Moreover, it is a XML-based markup language and it is a tag based. Tags are identifiers that are responsible to make code snippets and insert commands in the chatterbot. AIML defines a data object class called AIML objects, which is responsible for modelling patterns of conversation.</p><p id="ebea" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Example of AIML Code,</p><p id="e67c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Basic Tags:</strong></p><ol class=""><li id="840e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf hh hi hj"><strong class="gu hg">&lt;aiml&gt;: </strong>Defines the beginning and end of an AIML document</li><li id="78d8" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf hh hi hj"><strong class="gu hg">&lt;category&gt;: </strong>Defines the knowledge in a knowledge base.</li><li id="76e2" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf hh hi hj"><strong class="gu hg">&lt;pattern&gt;: </strong>Defines the pattern to match what a user may input.</li><li id="a176" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf hh hi hj"><strong class="gu hg">&lt;template&gt;: </strong>Defines the response of an Alicebot to user’s input.</li></ol><pre class="hl hm hn ho hp il im in"><span id="4847" class="ga gb df bk io b dj ip iq r ir">&lt;aiml version=”1.0.1&quot; encoding=”UTF-8&quot;?&gt;<br/>&lt;category&gt;<br/>      &lt;pattern&gt; HELLO BOT &lt;/pattern&gt;<br/>      &lt;template&gt;<br/>      Hello my new friend!<br/>     &lt;/template&gt;<br/>&lt;/category&gt;<br/>&lt;/aiml&gt;</span></pre><p id="4bee" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following figure is the Decision tree of rule based conversational AI,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm is"><div class="fk r fd fl"><div class="it r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*axxF04wGGvg1aA1fLlI7Mw.jpeg?q=20" width="580" height="391" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="580" height="391" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1160/1*axxF04wGGvg1aA1fLlI7Mw.jpeg" width="580" height="391" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://www.topbots.com/building-conversational-ai/" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="77e2" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Retrieval Based Conversational AI</strong></p><p id="437e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">When given user input, the system uses heuristics to locate the best response from its database of pre-defined responses. Dialogue selection is essentially a prediction problem, and using heuristics to identify the most appropriate response template may involve simple algorithms like keywords matching or it may require more complex processing with machine learning or deep learning. Regardless of the heuristic used, these systems only regurgitate pre-defined responses and do not generate new output.</p><p id="2912" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">With massive data available, it is intuitive to build a retrieval based conversational system as information retrieval techniques are developing fast. Given a user input utterance as the query, the system searches for candidate responses by matching metrics. The core of retrieval based conversational systems is formulated as a matching problem between the query utterance and the candidate responses. A typical way for matching is to measure the inner-product of two representing feature vectors for queries and candidate responses in a transformed Hilbert space. The modelling effort boils down to finding the mapping from the original inputs to the feature vectors , which is known as representation learning.There is two-step retrieval technique to find appropriate responses from the massive data repository. The retrieval process consists of a fast ranking by standard<strong class="gu hg"> TF-IDF</strong> measurement and the re-ranking process using conversation-oriented features designed with human expertise. The systems to select the most suitable response to the query from the question-answer pairs using a statistical language model as <em class="gt">cross-lingual information retrieval</em>. These methods are based on shallow representations, which basically utilises one-hot representation of words. Most strong retrieval systems learn representations with deep neural networks (DNNs). DNNs are highly automated learning machines; they can extract underlying abstract features of data automatically by exploring multiple layers of non-linear transformation. Prevailing DNNs for sentence level modelling include convolution neural networks (C-NNs) and recurrent neural networks (RNNs). A series of matching methods can be applied to short-text conversations for retrieval-based systems. Basically, these methods model sentences using convolutional or recurrent networks to construct abstractive representations. Although not all of these methods are originally designed for conversation, they are effective for short-text matching tasks and are included as strong baselines for retrieval-based conversational studies.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm iu"><div class="fk r fd fl"><div class="iv r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*7UFopLzF-mYqm5SjKpC6ZA.jpeg?q=20" width="1022" height="426" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1022" height="426" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2044/1*7UFopLzF-mYqm5SjKpC6ZA.jpeg" width="1022" height="426" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://arxiv.org/abs/1612.01627" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="e7c4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Response Selection with Topic Clues for Retrieval-based</strong></p><p id="d635" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">If we have incorporating topic information into message response matching to boost responses with rich content in retrieval-based chatbots.</p><p id="ba6e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Topic Word Generation</strong></p><p id="411b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">There is LDA model , which is the state-of-the-art topic model for short texts, to generate topic words for messages and responses. LDA assumes that each piece of text (a message or a response) corresponds to one topic, and each word in the text is either a background word or a topic word under the topic of the text.</p><p id="98a8" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Topic-aware Convolutional Neural Tensor Network</strong></p><p id="4577" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">There is a topic-aware convolutional neural tensor network (TACNTN) to leverage the topic words obtained from LDA in message-response matching.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm iw"><div class="fk r fd fl"><div class="ix r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*cQCjcHZynSi86pDzlQrOlA.jpeg?q=20" width="570" height="270" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="570" height="270" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1140/1*cQCjcHZynSi86pDzlQrOlA.jpeg" width="570" height="270" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://www.sciencedirect.com/science/article/pii/S0925231218309093" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="cd75" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Generative Based</strong></p><p id="d5e4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A generative model chatbot doesn’t use any predefined repository. This kind of chatbot is more advanced, because it learns from scratch using a process called “Deep Learning.” Generative models are typically based on Machine Translation techniques, but instead of translating from one language to another, we “translate” from an input to an output (response).</p><p id="419b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Another way to build a conversational system is to use language generation techniques.We can combine language template generation with the search-based methods. With deep learning techniques applied, generation-based systems are greatly advanced.</p><p id="9e37" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">We have a sequence-to-sequence (seq2seq) framework that emerged in the neural machine translation field and was successfully adapted to dialogue problems. The architecture consists of two RNNs with different sets of parameters.The approach involves two recurrent neural networks, one to encode the source sequence, called the encoder, and a second to decode the encoded source sequence into the target sequence, called the decoder.It was originally developed for machine translation problems, although it has proven successful at related sequence-to-sequence prediction problems such as text summarization and question answering.</p><p id="81b7" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Encoder</strong>:The encoder simply takes the input data, and train on it then it passes the last state of its recurrent layer as an initial state to the first recurrent layer of the decoder part.</p><h2 id="3cbc" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Working of Encoder</h2><p id="5cc4" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">The encoder RNN conceives a sequence of context tokens one at a time and updates its hidden state. After processing the whole context sequence, it produces a final hidden state, which incorporates the sense of context and is used for generating the answer.</p><p id="39b9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Decoder: </strong>The decoder takes the last state of encoder’s last recurrent layer and uses it as an initial state to its first recurrent layer , the input of the decoder is the sequences that we want to get ( in our case French sentences).</p><p id="befc" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">How Does the Decoder Work?</strong></p><p id="6447" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The goal of the decoder is to take context representation from the encoder and generate an answer. For this purpose, a softmax layer over vocabulary is maintained in the decoder RNN. At each time step, this layer takes the decoder hidden state and outputs a probability distribution over all words in its vocabulary.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm iy"><div class="fk r fd fl"><div class="iz r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*wEU6xtMZ2tZ02D2YIPOyYw.jpeg?q=20" width="948" height="372" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="948" height="372" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1896/1*wEU6xtMZ2tZ02D2YIPOyYw.jpeg" width="948" height="372" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="3b6e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Ensemble of Retrieval- and Generation-Based Dialog Systems</strong></p><p id="6b3a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Typically, a recurrent neural network (RNN) captures the query’s semantics with one or a few distributed, real-valued vectors (also known as embedding); another RNN decodes the query embedding to a reply. Deep neural networks allow complicated interaction by multiple non-linear transformations; RNNs are further suitable for modelling time-series data (e.g., a sequence of words) especially when enhanced with long short term memory (LSTM) or gated recurrent units (GRUs). Despite these, RNN also has its own weakness when applied to dialog systems: the generated sentence tends to be short, universal, and meaningless, for example, “I don’t know” or “something” . This is probably because chatbot-like dialogs are highly diversified and a query may not convey sufficient information for the reply. Even though such universal utterances may be suited in certain dialog context, they make users feel boring and lose interest, and thus are not desirable in real applications.</p><p id="66e4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In ensemble of retrieval and generative dialog systems. Given a user issued query, we first obtain a candidate reply by information retrieval from a large database. The query, along with the candidate reply, is then fed to an utterance generator based on the “bi-sequence to sequence” (biseq2seq) model. Such sequence generator takes into consideration the information contained in not only the query but also the retrieved reply; hence, it alleviates the low-substance problem and can synthesize replies that are more meaningful. After that we use the scorer in the retrieval system again for post-reranking. This step can filter out less relevant retrieved replies or meaningless generated ones. The higher ranked candidate (either retrieved or generated) is returned to the user as the reply. Basically, the retrieval and generative systems are integrated by two mechanisms:</p><p id="e6f8" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">(1) The retrieved candidate is fed to the sequence generator to mitigate the “low-substance” problem; (2) The post-reranker can make better use of both the retrieved candidate and the generated utterance.</p><p id="4052" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following Figure depicts the overall framework of ensemble of retrieval and generative dialog systems.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm ja"><div class="fk r fd fl"><div class="jb r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*esFyddkRdqjam4ETSgjDYQ.jpeg?q=20" width="883" height="269" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="883" height="269" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1766/1*esFyddkRdqjam4ETSgjDYQ.jpeg" width="883" height="269" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://arxiv.org/abs/1610.07149" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="f2c0" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">AIML</strong> <strong class="gu hg">Knowledge base (KB) Conversational AI</strong></p><p id="4a59" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A KB in this form is often called a Knowledge Graph (KG) due to its graphical representation, i.e., the entities are nodes and the relations the directed edges that link the nodes.</p><p id="8d63" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The basic concept of Knowledge base is shown as following figure,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm jc"><div class="fk r fd fl"><div class="jd r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*XbtDQ5nGEKF1I739ovs7rQ.jpeg?q=20" width="827" height="400" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="827" height="400" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1654/1*XbtDQ5nGEKF1I739ovs7rQ.jpeg" width="827" height="400" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="f167" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Most state-of-the-art symbolic approaches to KB-QA are based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a KB query. The answers to the question can then be obtained by finding a set of paths in the KB that match the query and retrieving the end nodes of these paths. Knowledge based systems have been helping humans to solve problems which are intellectually difficult, but easy for machines. These problems typically are easily represented with a set of formal rules.</p><p id="c124" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following figure we have KB representation graph centred on the question Q1. In the graph nodes are patterns (P) and templates (T), and edges are P-T associations and T-P semantic recursions.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm je"><div class="fk r fd fl"><div class="jf r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*ElhTTEFhEwiABXFL2OK3jg.png?q=20" width="725" height="573" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="725" height="573" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1450/1*ElhTTEFhEwiABXFL2OK3jg.png" width="725" height="573" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://www.researchgate.net/publication/298129630_Building_an_AIML_chatter_bot_knowledge-base_starting_from_a_FAQ_and_a_glossary" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="cbc4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Knowledge bases (KB) are powerful tools that can be used to augment conversational models. Since knowledge bases usually entail some kind of domain specific information, these techniques are mainly used for task-oriented dialog systems. In a KB, information related to the task at hand can be stored, for example information about nearby restaurants or about public transportation routes. Simple dictionaries or look-up-tables can be used to match an entity with information about it. Since KBs store information discretely, their integration with neural network based encoder-decoder models is not trivial.</p><p id="2445" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following figure shown that how the KB searching happens,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm jg"><div class="fk r fd fl"><div class="jh r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*jPVpqpuJr3-byFqHSAkddw.png?q=20" width="950" height="364" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="950" height="364" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1900/1*jPVpqpuJr3-byFqHSAkddw.png" width="950" height="364" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="337f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In Restaurant finding Knowledge base mechanism example, the encoder-decoder model produces a response that also uses general tokens for locations and times, and a special placeholder token for the KB result. Finally, the general tokens are transformed back to actual words using the stored table, a KB is employed which uses these general tokens to search for a route between the two places and its output is incorporated in the response. One more similar KB augmented encoder-decoder model is used for the task of recommending restaurants. Here, besides a standard encoder RNN the source utterance is also processed with a belief tracker, implemented as a convolutional neural network (CNN). Convolutional neural networks applied to encoder-decoder models.<em class="gt"> Belief tracking</em> is an important part of task oriented spoken dialog systems. The belief tracker network produces a query for a database containing information about restaurants. The final input to the decoder RNN is the weighted sum consisting of the last state of the decoder RNN and a categorical probability vector from the belief tracker. Then the decoder outputs a response in the same way as in the previous example, with lexicalised general tokens. These tokens are then replaced with the actual information that they point to in the KB.</p><p id="ad33" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Self Learning: Recurrent Embedding Dialogue policy (REDP)</strong></p><p id="f604" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Natural Language Processing with a Training Model to enable the bot to ‘learn’ to understand a sentence, Context to be able to perform a conversation and History to learn from previous conversations. A grand challenge in this field is to create software which is capable of holding extended conversations, carrying out tasks, keeping track of conversation history, and coherently responding to new information. The aim is to learn vector embeddings for dialogue states and system actions in a supervised setting.</p><p id="3b60" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following figure shown that how the chatbot response machine are associated with all components,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm ji"><div class="fk r fd fl"><div class="jj r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*uY-w_wt4cBSzHboZhR8j1A.jpeg?q=20" width="1312" height="606" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1312" height="606" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2624/1*uY-w_wt4cBSzHboZhR8j1A.jpeg" width="1312" height="606" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="271a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">When we ask a user “what price range are you looking for?”, they might respond with:</p><ul class=""><li id="8a16" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf jk hi hj">“Why do you need to know that?” (<code class="fl jl jm jn io b">narrow context</code>)</li><li id="4f25" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">“Can you show me some restaurants yet?” (<code class="fl jl jm jn io b">broad context</code>)</li><li id="c6eb" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">“Actually no I want Chinese food” (<code class="fl jl jm jn io b">correction</code>)</li><li id="b323" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">“I should probably cook for myself more” (<code class="fl jl jm jn io b">chitchat</code>)</li></ul><p id="cf15" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">We call all of this <em class="gt">uncooperative</em> behaviour. There are many other ways a user might respond. Here’s an example conversation:</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm jo"><div class="fk r fd fl"><div class="jp r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*JywMGefOdEiZKGAqaR9tVw.jpeg?q=20" width="747" height="621" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="747" height="621" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1494/1*JywMGefOdEiZKGAqaR9tVw.jpeg" width="747" height="621" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">s<a class="at cg fw fx fy fz" target="_blank" rel="noopener" href="/rasa-blog/attention-dialogue-and-learning-reusable-patterns-5d6bd18ef9f0">o</a>urce</figcaption></figure><p id="c80c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">At inference time, the current state of the dialogue is compared to all possible system actions, and the one with the highest cosine similarity is selected.</p><p id="f02f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">REDP</strong>, new dialogue policy, has two benefits: (1) it’s much better at learning how to deal with uncooperative behaviour, and (2) it can re-use this information when learning a new task.</p><p id="3c5f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">It uses the same idea to deal with uncooperative users. After responding correctly to a user’s uncooperative message, the assistant should return to the original task and be able to continue <em class="gt">as though the deviation never happened</em>. REDP achieves this by adding an attention mechanism to the neural network, allowing it to ignore the irrelevant parts of the dialogue history. The image below is an illustration of the REDP architecture (a full description is in the paper). The attention mechanism is based on a modified version of the Neural Turing Machine, and instead of a classifier we use an embed-and-rank approach.</p><p id="d1c1" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following figure shown the working of REDP,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm jq"><div class="fk r fd fl"><div class="jr r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*iQ928te8htdI4e4lCi0FMQ.jpeg?q=20" width="610" height="433" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="610" height="433" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1220/1*iQ928te8htdI4e4lCi0FMQ.jpeg" width="610" height="433" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="e83f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Attention has been used in dialogue research before, but the embedding policy is the first model which uses attention specifically for dealing with uncooperative behaviour, and also to reuse that knowledge in a different task. One advantage of this approach is that target labels can be represented as a bag of multiple features, allowing us to represent system actions as a composition of features. In general, the features describing a particular action can come from a number of sources, including the class hierarchy, the name of the action, and even features derived from the code itself (such as which functions are called). Whatever the source of the features, similar actions should have more features in common than dissimilar actions, and ideally reflect the structure of the domains. In our experiments we only derive features from the action name, either taking the whole name as a single feature, or splitting the name into tokens and representing it as a bag of words.</p><p id="fd20" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">4. Intent Identification and Information Extraction</strong></p><p id="951d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The machine algorithm for Intent Identification can be either supervised or unsupervised. If we implement the supervised approach, we need to manually give labels to hundreds of data for training purpose which going to be tiring and boring, but if we implement the unsupervised one, there were several critical knowledge gaps that we can’t cover in just 3 weeks especially regarding the design of training process. Therefore, even though we need to manually give labels to our data, we chose to go with the supervised one. This picture below illustrate how text classifier using supervised ML works:</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm ht"><div class="fk r fd fl"><div class="js r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*1Aqe_iigspkY5wJ5HEX5EA.png?q=20" width="800" height="363" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="800" height="363" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1600/1*1Aqe_iigspkY5wJ5HEX5EA.png" width="800" height="363" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="aed8" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">At this point, we have several machine learning algorithms that we could choose to implement supervised learning which is Naive Bayesian, LDA, SVM and Neural Network. But before we choose the algorithm, we need to find a method to translate words into an array (word embedding) since all algorithms that I mention previously need input in form of array or at least numbers. There are 2 options that we had to do that, by using one hot encoded <a href="https://en.wikipedia.org/wiki/Bag-of-words_model" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">bag of words (bow)</a> or <a href="https://en.wikipedia.org/wiki/Word2vec" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">word2vec</a> (CBOW). If we had more times, we definitely would choose word2vec to embed the input since the size of array would be significantly smaller compared to BOW, but we had limited time and to implement word2vec we need to use Java (<a href="https://deeplearning4j.org/word2vec.html" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">deeplearn4j</a>) or Python(<a href="https://radimrehurek.com/gensim/models/word2vec.html" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">gensim</a>) which no one between us had any experience making an API using these languages. Actually, it is possible for us to create the classifier by using Python but the problem will occur in the process of making an API out of it, especially in the deployment process. To deploy Python in the live server, there are several <a href="https://www.analyticsvidhya.com/blog/2017/09/machine-learning-models-as-apis-using-flask/" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">configurations</a> that need to be done and we don’t have the courage to play around with our company server since everyone else is also using it for other projects. So for the sake of familiarity, we decide to use BOW which we manage to find a node package to implement it called <a href="https://www.npmjs.com/package/mimir" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">mimir</a>.</p><p id="c3cc" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In Deep learning, Intent Identification works as a three layers of processing: encoder network, intention network, and decoder network. The encoder network has inputs from the current source side input. Because the source side in the current turn is also dependent on the previous turn, the source side encoder network is linked with the output from the previous target side. The encoder network creates a representation of the source side in the current turn. The intention network is dependent on its past state, so that it memories the history of intentions. It therefore is a recurrent network, taking a representation of the source side in the current turn and updating its hidden state. The decoder is a recurrent network for language modelling that outputs symbol at each time. This output is dependent on the current intention from the intention network. It also pays attention to particular words in the source side. In NLU, the functions / dialogue acts are often domain specific. In other words, instead of asking whether the function of the user’s utterance is a question or answer, we ask whether the function is to, for example, find flights or cancel a reservation in a flight reservation program. Domain-specific dialogue acts are called intents. Intent identifying has been most prominently used by call centre bots, which ask the user “how can I help you?” and subsequently use intent identification to re-direct the user to one of N pre-defined re-direction options. Many of the same machine learning algorithms used for DA classification are used for intent identification.</p><p id="0a8d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Regarding Information Extraction,</strong> The primary responsibility of the NLU is not just to understand phrase function, but to understand the meaning of the text itself. To extract meaning from text, we convert unstructured text — text written into a text-only chatbot — into structured grammatical data objects, which will be further processed by the Dialogue Manager. The first step in this process is breaking down a sentence into tokens that represent each of its component parts: words, punctuation marks, numbers, etc. Tokenization is difficult because of the frequency of ambiguous or malformed inputs including: (i) phrases , (ii) contractions , abbreviations , and periods. These tokens can be analyzed using a number of techniques, described below, to create a number of different data structures that be processed by the dialogue manager.</p><p id="4195" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">There are few approach which can be use for Information retrieval as below,</p><p id="bc52" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Bag of Words:</strong> We ignore sentence structure, order, and syntax, and count the number of occurrences of each word. We use this to form a vector space model, in which stop words are removed, and morphological variants go through a process call lemmatization and are stored as instances of the basic lemma . In the dialogue manager phase, assuming a rule-based bot, these resulting words will be matched against documents stored in the bot’s knowledge database to find the documents with inputs containing similar keywords. The bag of words approach is simple because it does not require knowledge of syntax, but, for this same reason, is not precise enough to solve more complex problems.</p><p id="f86e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Latent Semantic Analysis</strong> : This approach is similar to the bag of words. Meanings / concepts, however, not words, are the basic unit of comparison parsed from a given sentence or utterance. Second, groups of words that co-occur frequently are grouped together. In LSA, we create a matrix where each row represents a unique word, each column represents a document, and the value of each cell is the frequency of the word in the document. We compute the distance between the vector representing each utterance and document, using singular value decomposition to reduce the dimensionality of the matrix, and determine the closest document.</p><p id="8263" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Regular Expressions: </strong>Sentences / utterances can be treated as regular expressions, and can be pattern matched against the documents in the bot’s knowledge database. For example, imagine that one of the documents in the bot’s knowledge database handles the case where the user inputs the phrase: “my name is *”. “*” is the wildcard character, and indicates that this regular expression should be triggered whenever the bot hears the phrase “my name is” followed by anything. If the user says “my name is Jack”, this phrase will be parsed into a number of regular expressions, including “my name is *” and will trigger the retrieval of that document.</p><p id="cf33" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Part of Speech (POS) Tagging:</strong> POS tagging labels each word in the input string with its part of speech (e.g. noun, verb, adjective, etc.). These labels can be rule-based (a manually-created set of rules is created to specify part of speech for ambiguous words given their context). They can also be created using stochastic models which train on sentences labeled with correct POS. In the dialogue manager, POS can be used to store relevant information in the dialogue history. POS is also used in response generation to indicate the POS object type of the desired response.</p><p id="df87" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Named/Relation Entity Recognition: </strong>In named entity recognition (NER), the names of people, places, groups, and locations are extracted and labeled accordingly. NER-name pairs can be stored by the dialogue manager in the dialogue history to keep track of the context of the bot’s conversation. Relation extraction goes one step further to identity relations (e.g. “who did what to whom”) and label each word in these phrases.</p><p id="358b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Semantic Role Labelling:</strong> The arguments of a verb are labelled based on their semantic role (e.g. subject, theme, etc.). In this process, the predicate is labelled first followed by its arguments. Prominent classifiers for semantic role labelling have been trained on FrameNet and PropBank, databases with sentences already labelled with their semantic roles. These semantic role-word pairs can be stored by the dialogue manager in the dialogue history to keep track of context.</p><p id="1829" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Creation of Grammatical Data Structures:</strong> Sentences and utterances can be stored in a structured way in grammar formalism such as context-free grammars (CFGs) and dependency grammars (DGs). Context-free grammars are tree-like data structures that represent sentences as containing noun phrases and verb phrases, each of which contain nouns, verbs, subjects, and other grammatical constructs. Dependency grammars, by contrast, focus on the relationships between words.</p><p id="5c7a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Statistical Methods for Information Extraction</strong></p><p id="1450" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Hidden Vector State (HVS) Model: The </strong>goal of the statistical hidden vector state models is to automatically produce some accurate structured meaning. Consider an example as “I want to return to Dallas on Thursday.” The parse tree below represents one way of representing the structured meaning of the sentence. SS represents the initial node send_start, and SE represents the end node send_end. We view each leaf node as a vector state, described by its parent nodes: the vector state of Dallas is [CITY, TOLOC, RETURN, SS].</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm jt"><div class="fk r fd fl"><div class="ju r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*FCgLUOCm3XLCLljQhtwIdg.png?q=20" width="949" height="412" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="949" height="412" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1898/1*FCgLUOCm3XLCLljQhtwIdg.png" width="949" height="412" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://static1.squarespace.com/static/569293741c1210fdda37b429/t/59160b6bff7c50104e601a85/1494616940469/CHATBOT_thesis_final.pdf" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="eebe" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The whole parse-tree can then be thought of a sequence of vector states, represented by the sequence of squares above. If each vector state is thought of as a hidden variable, then the sequence of vector states (e.g. squares above) can be thought of as a Hidden Markov Model: we start at SS, and have certain probabilities of reaching a number of possible hidden states as the next state. Each vector state can be thought of as a “push-down automaton” or stack. <strong class="gu hg">Support Vector Machine (SVM) Model:</strong> Support Vector Machines are a supervised machine learning tool. Given a set of labeled training data, the algorithm generates the optimal hyperplane that divides the sample into their proper labels. Traditionally, SVMs are thought of as solving binary classification problems, however multiple hyperplanes can be used to divide the data into more than two label categories. The optimal hyperplane is defined as the hyperplane that creates the maximum margin, or distance, between different-labeled data point sets.</p><p id="a4d9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Conditional Random Field Models:</strong> CRFs are log-linear statistical models often applied for structured prediction. Unlike the average classifier, which predicts a label for a single object and ignores context, CRF’s take into account previous features of the input sequence through the use of conditional probabilities. A number of different features can be used to train the model, including lexical information, prefixes and suffixes, capitalization and other features.</p><p id="f3b9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Deep Learning:</strong> The most recent advancement in the use of statistical models for concept structure prediction is deep learning for natural language processing, or deep NLP. Deep learning neural network architectures differ from traditional neural networks in that they use more hidden layers, with each layer handling increasingly complex features. As a result, the networks can learn from patterns and unlabelled data, and deep learning can be used for unsupervised learning. Deep learning methods have been used to generate POS tags of sentences (chunk text into noun phrases, verb phrases, etc.) and for named-entity recognition and semantic role labelling.</p><p id="ee18" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture illustrate the working of Deep learning based <strong class="gu hg">Statistical Model,</strong></p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm jv"><div class="fk r fd fl"><div class="jw r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*KbdHfa8dGji_Kqhh0sd45Q.png?q=20" width="851" height="428" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="851" height="428" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1702/1*KbdHfa8dGji_Kqhh0sd45Q.png" width="851" height="428" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://dl.acm.org/citation.cfm?id=3210183" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="482a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">5. Self-learning Based on Sentiment Analysis</strong></p><p id="92aa" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Initially, the development of a bot was based on two fundamental components :</p><p id="983e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Natural Language Understanding module</strong>, used by the Dialogue Manager, that processes the user input to search for keywords through which to understand the action to be taken.</p><p id="55c0" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Natural Language Generation module</strong> that generates answers from the information gathered by the Dialogue Manager.</p><p id="2fb2" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Over time, we have faced a real evolution in the development of task-oriented conversational agents because of the availability of deep learning techniques.</p><p id="dba7" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">This picture below illustrate the process of sentiment analysis in user generated content,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm jx"><div class="fk r fd fl"><div class="jy r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/56/1*-dV66RkiDhpR-hnAeBSCNw.jpeg?q=20" width="648" height="691" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="648" height="691" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1296/1*-dV66RkiDhpR-hnAeBSCNw.jpeg" width="648" height="691" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="http://ceur-ws.org/Vol-2244/paper_01.pdf" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="cc43" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The training process for sentiment analysis it will provide for automatic labeling of new instances. In the sentiment analysis method each sentence is analyzed against two classification sub-systems: one for identifying the class of the answers, one for assessing the sentiment of the sentence. At the end of the processing of each sentence the learning model is updated according to the detected sentiment. This is based on a data structure formed by intents (An intent is a semantic label representing an intention of the end-user) . For each intent, there is a set of sentences that represent it. Each sentence that describes an intent contains entities (Entities are the parameters of the intent that help in defining the specific user request) that are attributes specific to the given intent.</p><p id="1d7b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">An example taken from the dataset is shown below.</p><p id="db16" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Take an example as, Request: ”I need days off from tomorrow to the day after tomorrow”.</p><p id="6d2d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Intent: LEAV E REQU EST</p><p id="181d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Entities:</p><p id="fe23" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">– start date: tomorrow.</p><p id="9dc3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">– end date: day after tomorrow.</p><p id="82de" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In this example, the scenario of an employee requesting holidays is repre- sented by the LEAV E REQU EST intent and by the start date and end date entities.</p><p id="2fdc" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">An example of how the method works in case of positive sentiment is shown below.</p><p id="ed57" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">example: Time off request</p><p id="23f3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Request: ”Hi” {intent: [HELLO] detected: [Hi] sentiment: [neutral]}</p><p id="3cdc" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Bot: ”Hi, Dave”</p><p id="abe9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Request: ”I’m stuck in traffic, I’ll be there soon” {intent: [TIMEOFF REQUEST]</p><p id="f8dc" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">detected: [there,soon] new words: [stuck,traffic] <strong class="gu hg">sentiment: [neutral]}</strong></p><p id="81f9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Bot: ”Ok, do you want to create a time off request?”</p><p id="4580" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Request: ”Yes, thank you!” {intent: [CONFIRM] detected: [Yes, thank, you] <strong class="gu hg">sentiment: [positive]}</strong></p><p id="b58a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In this example the agent detects the correct intent by the words there and soon and enriches the dictionary with stuck and traffic. If these words are often used for a time off request, they will become characteristic for this intent. (i.g. Dave in the future he will can write ”I’m stuck in traffic” or ”There is traffic” to request a time off).</p><p id="0743" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">An example of how the proposed method works in case of negative sentiment is as below.</p><p id="aa0d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">example: Time off request</p><p id="4613" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Request: ”Hi” {intent: [HELLO] detected: [Hi] <strong class="gu hg">sentiment: [neutral]}</strong></p><p id="ce72" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Bot: ”Hi, Dave”</p><p id="008b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Request: ”Tomorrow I’ll be busy” {intent: [LEAVE REQUEST] detected: [to-</p><p id="f05d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">morrow,busy] <strong class="gu hg">sentiment: [neutral]}</strong></p><p id="3e9a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Bot: ”Ok, do you want to create a leave request?”</p><p id="f66a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Request: ”No, that’s not what I want!” {intent: [NOT CONFIRM] detected: [No,that’s not, what, I, want] <strong class="gu hg">sentiment: [negative]</strong>}</p><p id="d699" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In this example the agent detects the incorrect intent by the words tomorrow and busy. In the future, if the bot will always receive a negative response to the request that he proposes then the words found will no longer be characteristics of the intent found and can be totally eliminated.</p><p id="bc21" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">We can have also defined a dictionary that the bot uses to translate the type of some words. For example, the terms ”tomorrow” and ”day after tomorrow” are assigned to the type date.</p><p id="2023" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Classification of intents</strong></p><p id="99e4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The classification problem considered in our method is determining the intent and the entities associated to a given user sentence. User sentences are represented with bag of words, without considering the order of the words. To improve classification accuracy, we also use a vocabulary of n-words with an N-gram model . The classification algorithm is based on Naive Bayes Text Classifier , a statistical technique able to estimate the probability of an element belonging to a certain class. The Naive Bayes technique estimates the conditional probabilities of each word given the classification category by associating every word that convey the same meaning in the intents, a numerical value that we will consider as a weight. The words that characterize an intent will have greater weight because they will only be found within that intent, so their occurrence is limited compared to non-characterizing words that we find in numerous intents.</p><p id="d7bd" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Take an example as, Given an intent Leave representing requests from a user regarding leaves, we would like sentences such as ”I want go to holidays”, ”I’m tired, I need to rests”, ”I want holidays for this month”, etc. to be classified as Leave. The main idea developed is to provide the agent with the ability to automatically collect feedback about its answers in order to improve its knowledge base. To this end, we experimented the use of sentiment analysis. To detect the sentiment from user sentences, we have defined another classification problem from user input to three classes: Positive, Negative and Neutral and use again a Naive Bayes approach to train this classifier on a specific dataset. For any user’s sentence, we keep track of local and global sentiment score, local score is about the last sentence, global score is an average value across the dialogue. Furthermore, to improve the idea, we can define some particular intents that act as modifiers.</p><p id="07db" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Take an example, when the user corrects the bot with phrases like ”I’m sorry I did not mean this”, this is considered as a negative feedback, while phrases containing specific thanks, such as ”Thank you! I was trying to do exactly this!” provide for positive feedback.</p><p id="ec93" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">6. Sequence2Sequence Model with Multihead Attention Mechanism</strong></p><p id="42fb" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Recurrent Neural Network</strong></p><p id="ce58" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many NLP tasks. The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a very bad idea. If you want to predict the next word in a sentence you better know which words came before it. RNNs are called <em class="gt">recurrent </em>because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps. Here is what a typical RNN looks like:</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm jz"><div class="fk r fd fl"><div class="ka r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*GqIz--afNilA1Pz9bAPrbQ.jpeg?q=20" width="711" height="345" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="711" height="345" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1422/1*GqIz--afNilA1Pz9bAPrbQ.jpeg" width="711" height="345" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="297f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Let’s consider the following sequence — <em class="gt">Bangalore is the largest city of ______</em>. It is easy to fill the blank with <em class="gt">India</em>. This means that there is information about the last word encoded in the previous elements of the sequence.</p><p id="fbf1" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The idea behind this architecture is to exploit this sequential structure of the data. The name of this neural networks comes from the fact that they operate in a recurrent way. This means that the same operation is performed for every element of a sequence, with its output depending on the current input, and the previous operations.</p><p id="9f2e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following picture shows the working of RNN for language modeling,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm kb"><div class="fk r fd fl"><div class="kc r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*y-eIxknba2Hyv1wtSKKP8Q.png?q=20" width="953" height="363" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="953" height="363" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1906/1*y-eIxknba2Hyv1wtSKKP8Q.png" width="953" height="363" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://blog.paperspace.com/recurrent-neural-networks-part-1-2/" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="345a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Recurrent Neural Networks can be used in a variety of scenarios depending in how the inputs are fed and the outputs are interpreted. These scenarios can be divided into three main different classes:</p><p id="9b4d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Sequential input to sequential output</strong></p><p id="e4ca" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Machine translation / part-of-speech tagging and language modeling tasks lie within this class.</p><p id="699c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Sequential input to single output</strong></p><p id="9386" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">One task with this property is sentiment analysis, in which we fed a sentence and we want to classify it as positive, neutral or negative.</p><p id="c38b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Single input to sequential output</strong></p><p id="c613" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">This is, for example, the case of image captioning: where we fed a picture to the RNN and want to generate a description of it.</p><p id="62e0" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Deep RNN with Multilayer Perceptron</strong></p><p id="2de3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Deep architectures of neural networks can represent a function exponentially more efficient than shallow architectures. While recurrent networks are inherently deep in time given each hidden state is a function of all previous hidden states , it has been shown that the internal computation is in fact quite shallow. It is argued that adding one or more nonlinear layers in the transition stages of a RNN can improve overall performance by better disentangling the underlying variations the original input. The deep structures in RNNs with perceptron layers can fall under three categories: input to hidden, hidden to hidden, and hidden to output.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm kd"><div class="fk r fd fl"><div class="ke r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*tik7NsPCsIBLcUW1XyCxVg.jpeg?q=20" width="495" height="288" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="495" height="288" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/990/1*tik7NsPCsIBLcUW1XyCxVg.jpeg" width="495" height="288" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="73d2" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Bi-Directional Recurrent Neural Network</strong></p><p id="ddf8" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The structure of BRNN is an to split the state neurons of a regular RNN in a part that is responsible for the positive time direction (forward states) and a part for the negative time direction (backward states). Outputs from forward states are not connected to inputs of backward states, and vice versa. If you might have to learn representations from future time steps to better understand the context and eliminate ambiguity. Take the following examples, “He said, Teddy bears are on sale” and “He said, Teddy Roosevelt was a great President”. In the above two sentences, when we are looking at the word “Teddy” and the previous two words “He said”, we might not be able to understand if the sentence refers to the President or Teddy bears. Therefore, to resolve this ambiguity, we need to look ahead. This is what Bidirectional RNNs accomplish.</p><p id="2573" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following picture illustrate the General structure of the bidirectional recurrent neural network,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm kf"><div class="fk r fd fl"><div class="kg r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*B5NHtY8_Y4we0DE4Y-acBA.jpeg?q=20" width="764" height="270" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="764" height="270" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1528/1*B5NHtY8_Y4we0DE4Y-acBA.jpeg" width="764" height="270" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="20e0" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Multidimentional Recurrent Neural Network</strong></p><p id="8a1a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The basic idea of multidimensional recurrent neural networks (MDRNNs) is to replace the single recurrent connection found in standard recurrent networks with as many connections as there are spatio-temporal dimensions in the data. These connections allow the network to create a flexible internal representation of surrounding context, which is robust to localised distortions. An MDRNN hidden layer scans through the input in 1D strips, storing its activations in a buffer. The strips are ordered in such a way that at every point the layer has already visited the points one step back along every dimension. The hidden activations at these previous points are fed to the current point through recurrent connections, along with the input.</p><p id="4184" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">RNN architectures used so far have been explicitly one dimensional, meaning that in order to use them for multi-dimensional tasks, the data must be preprocessed to one dimension, for example, by presenting one vertical line of an image at a time to the network. The most successful use of neural networks for multi-dimensional data has been the application of convolution networks to image processing tasks such as digit recognition . One disadvantage of convolution nets is that because they are not recurrent, they rely on hand specified kernel sizes to introduce context. Another disadvantage is that they don’t scale well to large images. For example, sequences of handwritten digits must be pre-segment.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm kh"><div class="fk r fd fl"><div class="ki r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*NM8sIB-FXDd6nF2HtPgIMQ.jpeg?q=20" width="637" height="205" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="637" height="205" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1274/1*NM8sIB-FXDd6nF2HtPgIMQ.jpeg" width="637" height="205" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="5399" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Long Short-Term Memory or LSTM Network</strong></p><p id="4e77" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">An LSTM network is a recurrent neural network that has LSTM cell blocks in place of our standard neural network layers. These cells have various components called the input gate, the forget gate and the output gate. RNNs are good in handling sequential data but they run into problem when the context is far away.</p><p id="f32f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Example: I live France and I know ____.</strong> The answer must be ‘<strong class="gu hg">French</strong>’ here but if the there are some more words in between ‘<strong class="gu hg">I live in France</strong>’ &amp; ‘<strong class="gu hg">I know ____</strong>’. It’ll be difficult for RNNs to predict ‘French’. <strong class="gu hg">This is the problem of Long-Term Dependencies. </strong>Hence we come to LSTMs.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm kj"><div class="fk r fd fl"><div class="kk r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*AqkZ_mXi_5oy4MOjBdbSlg.png?q=20" width="772" height="316" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="772" height="316" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1544/1*AqkZ_mXi_5oy4MOjBdbSlg.png" width="772" height="316" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="f46c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">LSTMs are explicitly designed to avoid the long-term dependency problem. LSTMs also provide solution to Vanishing/Exploding Gradient problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p><p id="8567" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The picture below illustrate that how input gate, forget get and output gate are working together,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm kl"><div class="fk r fd fl"><div class="km r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*w8tI_s56iaq9rOpirydphA.png?q=20" width="842" height="372" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="842" height="372" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1684/1*w8tI_s56iaq9rOpirydphA.png" width="842" height="372" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="ba94" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Bidirectional LSTM</strong></p><p id="84a0" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The basic idea of bidirectional recurrent neural nets (BRNNs) is to present each training sequence forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer. (In some cases a third network is used in place of the output layer, but here we have used the simpler model). This means that for every point in a given sequence, the BRNN has complete, sequential information about all points before and after it. Also, because the net is free to use as much or as little of this context as necessary, there is no need to find a (task-dependent) time-window or target delay size. BRNNs have given improved results in sequence learning tasks, It is possible to increase capacity of BRNNs by stacking hidden layers of LSTM cells in space, called deep bidirectional LSTM (BLSTM) .</p><p id="9b9a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">BLSTM networks are more powerful than unidirectional LSTM networks. These networks theoretically involve all information of input sequences during computation. The distributed representation feature of BLSTM is crucial for different applications such as language understanding . In BLSTM, The forward and backward passes over the unfolded network over time are carried out in a similar way to regular network forward and backward passes, except that we need to unfold the hidden states for all time steps. We also need a special treatment at the beginning and the end of the data points.</p><p id="c51e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The picture below illustrate BLSTM for tagging named entities. Multiple tables look up word-level feature vectors,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm kn"><div class="fk r fd fl"><div class="ko r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/58/1*qJAZ9tGiKfH6pwl3CTnVxg.png?q=20" width="593" height="612" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="593" height="612" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1186/1*qJAZ9tGiKfH6pwl3CTnVxg.png" width="593" height="612" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00104" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="58a9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The long-short term memory (LSTM) unit with the forget gate allows highly non-trivial long-distance dependencies to be easily learned . For sequential labelling tasks such as NER and speech recognition, a bi-directional LSTM model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context that applies to any feed-forward model. While LSTMs have been studied in the past for the NER task by Hammerton, the lack of computational power (which led to the use of very small models) and quality word embeddings limited their effectiveness.</p><p id="5fd0" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The picture below illustrate fully connected LSTM works,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm kp"><div class="fk r fd fl"><div class="kq r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*jDHVuXdVjOhAxnzKuuuY3A.jpeg?q=20" width="850" height="396" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="850" height="396" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1700/1*jDHVuXdVjOhAxnzKuuuY3A.jpeg" width="850" height="396" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="bbcb" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">LSTM-CRF networks</strong></p><p id="5687" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In the <strong class="gu hg">CRF networks</strong> there are two different ways to make use of neighbor tag information in predicting current tags. The first is to predict a distribution of tags for each time step and then use beam-like decoding to find optimal tag sequences. The work of maximum entropy classifier and Maximum entropy Markov models fall in this category. The second one is to focus on sentence level instead of individual positions, thus leading to Conditional Random Fields (CRF) models. Note that the inputs and outputs are directly connected, as opposed to LSTM and bidirectional LSTM networks where memory cells/recurrent components are employed. CRFs can produce higher tagging accuracy in general. It is interesting that the relation between these two ways of using tag information bears resemblance to two ways of using input features , and the results in this paper confirms the superiority of BI-LSTM compared to LSTM.</p><p id="e4eb" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The picture below illustrate working of CRF model,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm kr"><div class="fk r fd fl"><div class="ks r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*-5wU43qeITguMirBlq6BbA.png?q=20" width="575" height="298" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="575" height="298" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1150/1*-5wU43qeITguMirBlq6BbA.png" width="575" height="298" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://arxiv.org/pdf/1508.01991.pdf" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="f4d9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In LSTM-CRF networks, It can efficiently use past input features via a LSTM layer and sentence level tag information via a CRF layer. A CRF layer is repre- sented by lines which connect consecutive output layers. A CRF layer has a state transition matrix as parameters. With such a layer, we can efficiently use past and future tags to predict the current tag,</p><p id="97f4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The picture below illustrate working of LSTM-CRF model,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="n p aq"><img class="kt" src="https://miro.medium.com/proxy/1*0kD9s1iD9LhJA13Rmq99Rg.png" role="presentation"/></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://arxiv.org/pdf/1508.01991.pdf" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="1906" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The sequence of word representation is regarded as inputs to a bi-directional LSTM, and its output results from the right and left context for each word in a sentence. The output representation from bi-directional LSTM fed onto a CRF layer, the size of representation and its labels are equivalent. In order to consider the neighboring labels, instead of the softmax, we chose CRF as a decision function to yield final label sequence.</p><p id="838f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The picture below illustrate working of character level vector concatenated with word embedding as word representation with BLSTM with CRF model,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm ku"><div class="fk r fd fl"><div class="kv r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*qbBNzLwB3xDa-OQ8o4t7-A.jpeg?q=20" width="550" height="449" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="550" height="449" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1100/1*qbBNzLwB3xDa-OQ8o4t7-A.jpeg" width="550" height="449" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">sour<a href="https://www.mdpi.com/1099-4300/19/6/283/htm" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">c</a>e</figcaption></figure><p id="b829" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Gated Recurrent Unit</strong></p><p id="90d8" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A GRU has two gates, a reset gate , and an update gate . Intuitively, the reset gate determines how to combine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. If we set the reset to all 1’s and update gate to all 0’s we again arrive at our plain RNN model. The basic idea of using a gating mechanism to learn long-term dependencies is the same as in a LSTM.</p><p id="7f95" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In GRU the RNN cell as a computation in which we update the memory vector deciding, at each timestep, which information we want to keep, which information is not relevant anymore and we would like to forget and which information to add from the new input. The RNN cell also creates an output vector which is tightly related to the current hidden state (or memory vector).</p><p id="6e90" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The picture below illustrate the working of GRU,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm kw"><div class="fk r fd fl"><div class="kx r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*rbW4F0626s4-zrgExdmYKQ.jpeg?q=20" width="1060" height="472" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1060" height="472" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2120/1*rbW4F0626s4-zrgExdmYKQ.jpeg" width="1060" height="472" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="1524" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Comparison between LSTM and GRU,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm ky"><div class="fk r fd fl"><div class="kz r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*4qaHeoX6MvJxqdC1Eyu9rw.jpeg?q=20" width="2268" height="310" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="2268" height="310" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/4536/1*4qaHeoX6MvJxqdC1Eyu9rw.jpeg" width="2268" height="310" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://arxiv.org/pdf/1612.07778.pdf" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="d42c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In Emotion classification example from noisy speech, we simulate noisy speech upon superimposing various environmental noises on clean speech. Features are extracted from the noisy speech and feed to the GRU for emotion classification.</p><p id="ede1" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The picture below illustrate an example of emotion classification from noisy speech using LSTM-GRU,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm la"><div class="fk r fd fl"><div class="lb r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*MgHI83_gsoi5kP7K1tsGaw.jpeg?q=20" width="1095" height="284" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1095" height="284" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2190/1*MgHI83_gsoi5kP7K1tsGaw.jpeg" width="1095" height="284" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://arxiv.org/pdf/1612.07778.pdf" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="70e2" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Character based convolutional gated recurrent encoder with word based gated recurrent decoder with attention (CCEAD)</strong></p><p id="ecf7" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">This model has the similar underlying architecture of the sequence-to sequence models . In this model a character based sequence-to-sequence architecture with a convolutional neural network(CNN)-gated recurrent unit (GRU) encoder that captures error representations in noisy text. The decoder of this model is a word based gated recurrent unit (GRU) that gets its initial state from the character encoder and implicitly behaves like a language model.</p><p id="5d70" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following is the Architectural diagram of our character based convolutional gated recurrent encoder with word based gated recurrent<br/>decoder with attention (CCED),</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm lc"><div class="fk r fd fl"><div class="ld r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*S9ueXPSmOLAOwhXSMWRtTQ.jpeg?q=20" width="1115" height="605" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1115" height="605" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2230/1*S9ueXPSmOLAOwhXSMWRtTQ.jpeg" width="1115" height="605" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://arxiv.org/abs/1709.06429" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="a963" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following picture Illustrate the CNN module comprising the encoder of our CCEAD model used for capturing hidden representations in data as,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm le"><div class="fk r fd fl"><div class="lf r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*jFH3gLT1r3ucKXgJO7GLww.png?q=20" width="535" height="424" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="535" height="424" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1070/1*jFH3gLT1r3ucKXgJO7GLww.png" width="535" height="424" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://arxiv.org/abs/1709.06429" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="533e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Word Embedding</strong></p><p id="151a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Word Embedding is a technique for learning dense representation of words in a low dimensional vector space. Each word can be seen as a point in this space, represented by a fixed length vector. Semantic relations between words are captured by this technique. Word Embedding is typically done in the first layer of the network : Embedding layer, that maps a word (index to word in vocabulary) from vocabulary to a dense vector of given size.</p><p id="3057" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Word2Vec</strong></p><p id="742b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Word2Vec is a method to construct word embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW).</p><p id="d141" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">We can perform some amazing tasks from word embeddings of Word2Vec.</p><ol class=""><li id="460c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf hh hi hj">Finding the degree of similarity between two words.<br/><code class="fl jl jm jn io b">model.similarity(&#x27;woman&#x27;,&#x27;man&#x27;)</code><br/><code class="fl jl jm jn io b">0.73723527</code></li><li id="e2e9" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf hh hi hj">Finding odd one out.<br/><code class="fl jl jm jn io b">model.doesnt_match(&#x27;breakfast cereal dinner lunch&#x27;;.split())</code><br/><code class="fl jl jm jn io b">&#x27;cereal&#x27;</code></li><li id="9c4a" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf hh hi hj">Amazing things like woman+king-man =queen<br/><code class="fl jl jm jn io b">model.most_similar(positive=[&#x27;woman&#x27;,&#x27;king&#x27;],negative=[&#x27;man&#x27;],topn=1)</code><br/><code class="fl jl jm jn io b">queen: 0.508</code></li><li id="c612" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf hh hi hj">Probability of a text under the model<br/><code class="fl jl jm jn io b">model.score([&#x27;The fox jumped over the lazy dog&#x27;.split()])</code><br/><code class="fl jl jm jn io b">0.21</code></li></ol><p id="e6d1" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">GloVe</strong></p><p id="56e9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">GloVe is a new model for word representation , for Global Vectors, because the global corpus statistics are captured directly by the model. It is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. It is Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval , document classification , question answering , named entity recognition. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favours models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations.</p><h2 id="94fb" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">FastText</h2><p id="bd53" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">FastText is an extension to Word2Vec proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word <em class="gt">apple</em> is<em class="gt"> app, ppl</em>, and <em class="gt">ple</em> (ignoring the starting and ending of boundaries of words). The word embedding vector for <em class="gt">apple</em> will be the sum of all these n grams. After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words.</p><p id="e8af" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">There are many different types of word embeddings:</p><p id="eb87" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">i. Frequency based embedding</p><p id="25e1" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">ii. Prediction based embedding</p><p id="7f0c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Frequency based embedding</strong></p><h2 id="5607" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Count vector</h2><p id="7a83" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">count vector model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears.</p><p id="75ee" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">TF-IDF vectorization</strong></p><p id="cd3c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Tf means <strong class="gu hg">term-frequency</strong> while tf–idf means term-frequency times <strong class="gu hg">inverse document-frequency.</strong></p><p id="37e6" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.</p><p id="096f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform. This method takes into account not just the occurrence of a word in a single document but in the entire corpus. lets take a business article this article will contain more business related terms like Stock-market, Prices, shares etc in comparison to any other article. but terms like “a, an, the” will come in each article with high frequency. so this method will penalize these type of high frequency words.</p><h2 id="ae5f" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Co-Occurrence Matrix with a fixed context window</h2><p id="5006" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">Words co-occurrence matrix describes how words occur together that in turn captures the relationships between words. Words co-occurrence matrix is computed simply by counting how two or more words occur together in a given corpus.</p><p id="8287" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Prediction based embedding</strong></p><p id="62f1" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Continuous Bag of Words(CBOW)</strong></p><p id="df5e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">CBOW</strong> is learning to predict the word by the context. A context may be single word or multiple word for a given target words.</p><p id="ffa9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">lets see this by an example “The cat jumped over the puddle.”</p><p id="4b28" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">So one approach is to treat {“The”, “cat”, ’over”, “the’, “puddle”} as a context and from these words, be able to predict or generate the centre word “jumped”. This type of model we call a Continuous Bag of Words (CBOW) Model.</p><p id="16cd" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture illustrate the representation of CBOW,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm lg"><div class="fk r fd fl"><div class="lh r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*pgVXxtiHPqzRCQXPtKEBRA.png?q=20" width="768" height="740" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="768" height="740" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1536/1*pgVXxtiHPqzRCQXPtKEBRA.png" width="768" height="740" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://iksinc.online/tag/continuous-bag-of-words-cbow/" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><h2 id="8fcb" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Skip-gram</h2><p id="d78c" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">For skip-gram, the input is the target word, while the outputs are the words surrounding the target words. For instance, in the sentence “<em class="gt">I have a cute dog</em>”, the input would be “<em class="gt">a</em>”, whereas the output is “<em class="gt">I</em>”, “<em class="gt">have</em>”, “<em class="gt">cute</em>”, and “<em class="gt">dog</em>”, assuming the window size is 5. All the input and output data are of the same dimension and one-hot encoded. The network contains 1 hidden layer whose dimension is equal to the embedding size, which is smaller than the input/ output vector size. At the end of the output layer, a softmax activation function is applied so that each element of the output vector describes how likely a specific word will appear in the context. The graph below visualizes the network structure.</p><p id="3fb7" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">given the sentence above (<em class="gt">“The fluffy dog barked as it chased a cat”) </em>as input a run of the model would look like this:</p><p id="f15d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Here’s the architecture of our neural network of Skip-gram model,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm li"><div class="fk r fd fl"><div class="lj r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*aE6pVoYwKqtaNmANw1WVhQ.png?q=20" width="778" height="571" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="778" height="571" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1556/1*aE6pVoYwKqtaNmANw1WVhQ.png" width="778" height="571" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a class="at cg fw fx fy fz" target="_blank" rel="noopener" href="/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf">source</a></figcaption></figure><p id="be3c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Tensorflow implementation of word Embedding</strong></p><p id="a0e8" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">You can create word embeddings in TensorFlow, we first split the text into words and then assign an integer to every word in the vocabulary. For example, the sentence “I have a cat.” could be split into <code class="fl jl jm jn io b"><strong class="gu hg">[“I”, “have”, “a”, “cat”, “.”]</strong></code> and then the corresponding <code class="fl jl jm jn io b"><strong class="gu hg">word_ids</strong></code> tensor would have shape <code class="fl jl jm jn io b"><strong class="gu hg">[5]</strong></code> and consist of 5 integers. To map these word ids to vectors, we need to create the embedding variable and use the<code class="fl jl jm jn io b"><a href="https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow"><strong class="gu hg">tf.nn.embedding_lookup</strong></a></code> function as follows:</p><pre class="hl hm hn ho hp il im in"><span id="7314" class="ga gb df bk io b dj ip iq r ir">word_embeddings = tf.get_variable(“word_embeddings”,<br/>    [vocabulary_size, embedding_size])<br/>embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, word_ids)</span></pre><p id="842d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">After this, the tensor <code class="fl jl jm jn io b"><strong class="gu hg">embedded_word_ids</strong></code> will have shape <code class="fl jl jm jn io b"><strong class="gu hg">[5,embedding_size]</strong></code> in our example and contain the embeddings (dense vectors) for each of the 5 words. At the end of training, <code class="fl jl jm jn io b"><strong class="gu hg">word_embeddings</strong></code> will contain the embeddings for all words in the vocabulary.</p><p id="3fb5" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">For vector representation of word on Tensorboard, we use following code,</p><pre class="hl hm hn ho hp il im in"><span id="95ae" class="ga gb df bk io b dj ip iq r ir"># Merge all the summaries and write them out to /tmp/logs (by default)<br/>merged = tf.summary.merge_all()<br/>train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + &#x27;/train&#x27;,<br/>                                      sess.graph)<br/>test_writer = tf.summary.FileWriter(FLAGS.summaries_dir + &#x27;/test&#x27;)<br/>tf.global_variables_initializer().run()</span></pre><h2 id="55d1" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Launching TensorBoard</h2><p id="a78e" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">To run TensorBoard, use the following command (alternatively <code class="fl jl jm jn io b">python -m tensorboard.main</code>)</p><pre class="hl hm hn ho hp il im in"><span id="ed92" class="ga gb df bk io b dj ip iq r ir">tensorboard --logdir=path/to/log-directory</span></pre><p id="5eb7" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture shows that vector representation of word on Tensorboard,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm lk"><div class="fk r fd fl"><div class="ll r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*F-AMMSFrPJ-LfbYmKFtbew.jpeg?q=20" width="1366" height="768" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1366" height="768" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2732/1*F-AMMSFrPJ-LfbYmKFtbew.jpeg" width="1366" height="768" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">screenshot</figcaption></figure><p id="3876" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Word representation is central to natural language processing. The default approach of representing words as discrete and distinct symbols is insufficient for many tasks, and suffers from poor generalization. For example, the symbolic representation of the words “pizza” and “hamburger” are completely unrelated: even if we know that the word “pizza” is a good argument for the verb “eat”, we cannot infer that “hamburger” is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words.</p><p id="754d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture illustrate the vector representation of word,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm lm"><div class="fk r fd fl"><div class="ln r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*zw-uVf2VANA9VC_Qg7fbKQ.jpeg?q=20" width="631" height="529" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="631" height="529" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1262/1*zw-uVf2VANA9VC_Qg7fbKQ.jpeg" width="631" height="529" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="a3e3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">7. Topic aware Sequence to Sequence Model with Multihead Attention Mechanism</strong></p><p id="5f23" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The Sequence to Sequence model (seq2seq) consists of two RNNs — an encoder and a decoder. The encoder reads the input sequence, word by word and emits a context (a function of final hidden state of encoder), which would ideally capture the essence (semantic summary) of the input sequence. Based on this context, the decoder generates the output sequence, one word at a time while looking at the context and the previous word during each timestep. This is a ridiculous over simplification, but it gives you an idea of what happens in seq2seq.</p><p id="6bef" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Sequence To Sequence model introduced in Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation has since then, become the Go-To model for Dialogue Systems and Machine Translation. It consists of two RNNs (Recurrent Neural Network) : An Encoder and a Decoder. The encoder takes a sequence(sentence) as input and processes one symbol(word) at each timestep. Its objective is to convert a sequence of symbols into a fixed size feature vector that encodes only the important information in the sequence while losing the unnecessary information. You can visualise data flow in the encoder along the time axis, as the flow of local information from one end of the sequence to another.</p><p id="eeb9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Topic aware</strong> sequence-to-sequence (TA-Seq2Seq) model in order to leverage topic information as prior knowledge in response generation. TA-Seq2Seq is built on the sequence-to-sequence framework. In encoding, the model represents an input message as hidden vectors by a message encoder, and acquires embeddings of the topic words of the message from a pre-trained LDA model. The topic words are used as a simulation of topical concepts in people’s minds, and obtained from a LDA model which is pre-trained using large scale social media data outside the conversation data. In decoding, each word is generated according to both the message and the topics through a joint attention mechanism. In joint attention, hidden vectors of the message are summarized as context vectors by message attention which follows the existing attention techniques, and embeddings of topic words are synthesized as topic vectors by topic attention. Different from existing attention, in topic attention, the weights of the topic words are calculated by taking the final state of the message as an extra input in order to strengthen the effect of the topic words relevant to the message. The joint attention lets the context vectors and the topic vectors jointly affect response generation, and makes words in responses not only relevant to the input message, but also relevant to the correlated topic information of the message. To model the behavior of people using topical concepts as “building blocks” of their responses, we modify the generation probability of a topic word by adding another probability item which biases the overall distribution and further increases the possibility of the topic word appearing in the response. The results on both automatic evaluation metrics and human annotations show that TA-Seq2Seq can generate more informative, diverse, and topic relevant responses and significantly outperforms state-of-the-art methods for response generation.</p><p id="54d0" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Seq2Seq Attention mechanism</strong></p><p id="3b09" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The traditional Seq2Seq model assumes that every word is generated from the same context vector. In practice, however, different words in Y could be semantically related to different parts of X. To tackle this issue, attention mechanism is introduced into Seq2Seq.</p><p id="0880" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture illustrate the seq2seq attention mechanism,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm lo"><div class="fk r fd fl"><div class="lp r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*vUmtgeeWvgfSUqj5_xpvyg.jpeg?q=20" width="1002" height="483" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1002" height="483" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2004/1*vUmtgeeWvgfSUqj5_xpvyg.jpeg" width="1002" height="483" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="00c4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Sequence-to-sequence model (Seq2Seq) was first proposed in machine translation. The idea was to translate one sequence to another sequence through an encoder-decoder neural architecture. Recently, dialog generation has been treated as sequence translation from a query to a reply.</p><p id="61ee" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Multi-head Attention Mechanism</strong></p><p id="744b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The context vector obtained by traditional attention mechanism focuses on a specific representation subspace of the input sequence. Such context vector is expected to reflect one aspect of the semantics in the input. However, a sentence usually involves multiple semantics spaces, especially for a long sentence. In multi-head attention mechanism for Seq2Seq model to allow the decoder RNN to jointly attend to information from different representation subspaces of the encoder hidden states at the decoding process. The idea of multi-head has been applied to learn the sentence representation in self-attention.</p><p id="bf86" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture illustrate the working of Multihead encoder-decoder attention mechanism,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="cl cm lq"><div class="fk r fd fl"><div class="lr r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*98QyMYjfclvFXWzG_7YrQA.jpeg?q=20" width="682" height="354" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="682" height="354" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1364/1*98QyMYjfclvFXWzG_7YrQA.jpeg" width="682" height="354" role="presentation"/></noscript></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="5f3b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Dual Encoder LSTM (DE)</strong></p><p id="8e29" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The DE model consists of two RNNs which respectively compute the vector representation of an input context and response.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm ls"><div class="fk r fd fl"><div class="lt r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*TmFYsMQp31j1YZOrAtyL5g.jpeg?q=20" width="1300" height="876" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1300" height="876" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2600/1*TmFYsMQp31j1YZOrAtyL5g.jpeg" width="1300" height="876" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di">source</figcaption></figure><p id="1aa2" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><em class="gt">Dual Encoder LSTM</em> network is just one of many we could apply to this problem and it’s not necessarily the best one. You can come up with all kinds of Deep Learning architectures that haven’t been tried yet — it’s an active research area. For example, the <a href="https://www.tensorflow.org/versions/r0.9/tutorials/seq2seq/index.html" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">seq2seq model</a> often used in Machine Translation would probably do well on this task. The reason we are going for the Dual Encoder is because it has been <a href="http://arxiv.org/abs/1510.03753" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">reported</a> to give decent performance on this data set. This means we know what to expect and can be sure that our implementation is correct.</p><p id="edff" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The following are the working of Dual Encoder,</p><p id="c07a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">i. Both the context and the response text are split by words, and each word is <a href="https://en.wikipedia.org/wiki/Word_embedding" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">embedded</a> into a vector. The word embeddings are initialized with Stanford’s <a href="http://nlp.stanford.edu/projects/glove/" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">GloVe</a> vectors and are fine-tuned during training.</p><p id="050a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">ii. Both the embedded context and response are fed into the same Recurrent Neural Network word-by-word. The RNN generates a vector representation that, loosely speaking, captures the “meaning” of the context and response.</p><p id="04cc" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">iii. We measure the similarity of the predicted response <code class="fl jl jm jn io b">r&#x27;</code> and the actual response <code class="fl jl jm jn io b">r</code> by taking the dot product of these two vectors. A large dot product means the vectors are similar and that the response should receive a high score. We then apply a sigmoid function to convert that score into a probability.</p><p id="44bb" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">8. Neural Response Generation via Generative Adversarial Network</strong></p><p id="1479" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Generative Adversarial Nets (GANs) offers an effective architecture of jointly training a generative model and a discriminative classifier to generate sharp and realistic images. This architecture could also potentially be applied to conversational response generation to relieve the safe response problem, where the generative part can be an Seq2Seq-based model that generates response utterances for given queries, and the discriminative part can evaluate the quality of the generated utterances from diverse dimensions according to human-produced responses. However, unlike the image generation problems, training such a GAN for text generation here is not straightforward. The decoding phase of the Seq2Seq model usually involves sampling discrete words from the predicted distributions, which will be fed into the training of the discriminator. The sampling procedure is non-differentiable, and will therefore break the back-propagation. Inspired by recent advances in Neural Machine Translation (NMT). Earlier works focused on paired word sequences only, now we have the mechanism that the comprehensibility of the generated responses can benefit from multiview training with respect to words, coarse tokens and utterances.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm lu"><div class="fk r fd fl"><div class="lv r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*m8aJLFGpxaeKrwIWT4ageQ.jpeg?q=20" width="1011" height="471" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1011" height="471" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2022/1*m8aJLFGpxaeKrwIWT4ageQ.jpeg" width="1011" height="471" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="http://www.aclweb.org/anthology/D17-1065" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="9ab1" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Generative model</strong></p><p id="9569" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The generative model G defines the policy that generates a response y given dialogue history x. It takes a form similar to seq2seq models, which first map the source input to a vector representation using a recurrent net and then compute the probability of generating each token in the target using a softmax function.</p><p id="d243" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Discriminative model</strong></p><p id="3af3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The discriminative model D is a binary classifier that takes as input a sequence of dialogue utterances {x, y} and outputs a label indicating whether the input is generated by humans or machines. The input dialogue is encoded into a vector representation using a hierarchical encoder 2 which is then fed to a 2-class softmax function, returning the probability of the input dialogue episode being a machine-generated dialogue (denoted Q − ({x, y})) or a human-generated dialogue (denoted Q + ({x, y})).</p><p id="4b63" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Policy Gradient Training</strong></p><p id="aa5e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The key idea of the system is to encourage the generator to generate utterances that are indistinguishable from human generated dialogues. We use policy gradient methods to achieve such a goal, in which the score of current utterances being human-generated ones assigned by the discriminator is used as a reward for the generator, which is trained to maximize the expected reward of generated utterance(s) using the REINFORCE algorithm.</p><p id="0106" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Reward for Every Generation Step</strong></p><p id="37f4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Suppose, for example, the input history is what’s your name, the human-generated response is I am John, and the machine-generated response is I don’t know. The vanilla REINFORCE model assigns the same negative reward to all tokens within the human-generated response (i.e., I, don’t, know), whereas proper credit assignment in training would give separate rewards, most likely a neutral reward for the token I, and negative rewards to don’t and know. We call this reward for every generation step, abbreviated REGS. Rewards for intermediate steps or partially decoded sequences are thus necessary. Unfortunately, the discriminator is trained to assign scores to fully generated sequences, but not partially decoded ones. We propose two strategies for computing intermediate step rewards by (1) using Monte Carlo (MC) search and (2) training a discriminator that is able to assign rewards to partially decoded sequences.</p><p id="c78c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">9. Machine Reading for Question Answering</strong></p><p id="a01e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Machine Reading Comprehension (MRC) is a challenging task: the goal is to have machines read a (set of) text passage(s) and then answer any question about the passage(s). The MRC model is the core component of text-QA agents.</p><p id="a4e4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Consider an example </strong>as given the question “will I qualify for OSAP if I’m new in Canada”, one might first locate the relevant passage that include: “you must be a 1 Canadian citizen; 2 permanent resident; or 3 protected person…” and reason that being new to the country is usually the opposite of citizen, permanent resident etc., thus determine the correct answer: “no, you won’t qualify”.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm lw"><div class="fk r fd fl"><div class="lx r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*gG78r87vdPVl69NN0w8wLA.jpeg?q=20" width="859" height="341" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="859" height="341" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1718/1*gG78r87vdPVl69NN0w8wLA.jpeg" width="859" height="341" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://dl.acm.org/citation.cfm?id=3210183" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="55ea" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Neural MRC Models</strong></p><p id="2579" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">In spite of the variety of model structures and attention types , a typical neural MRC model performs reading comprehension in three steps, as (1) encoding the symbolic representation of the questions and passages into a set of vectors in a neural space; (2) reasoning in the neural space to identify the answer vector (e.g., in SQuAD, this is equivalent to ranking and re-ranking the embedded vectors of all possible text spans in P ). and (3) decoding the answer vector into a natural language output in the symbolic space (e.g., this is equivalent to mapping the answer vector to its text span in P ).</p><p id="a49d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture illustrate the working of Machine reading comprehension,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm ie"><div class="fk r fd fl"><div class="ly r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*FWNrrd1l4xlaAitmly6_QQ.jpeg?q=20" width="889" height="412" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="889" height="412" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1778/1*FWNrrd1l4xlaAitmly6_QQ.jpeg" width="889" height="412" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://dl.acm.org/citation.cfm?id=3210183" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="976a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Encoding in MRC</strong></p><p id="95b7" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Most MRC models encode questions and passages through three layers: lexicon embedding layer,contextual embedding layer and attention layer.</p><p id="2877" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Lexicon Embedding Layer</strong></p><p id="85c2" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">It extracts information from Q and P at the word level and normalizes for lexical variants. It typically maps each word to a vector space using a pre-trained word embedding model, such as word2vec or GloVe. such that semantically similar words are mapped to the vectors that are close to each other in the neural space. Word embedding can be enhanced by concatenating each word embedding vector with other linguistic embeddings such as those derived from characters, Part-Of-Speech (POS) tags, and named entities etc.</p><p id="0e6a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Contextual Embedding Layer</strong></p><p id="d11d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">It utilizes contextual cues from surrounding words to refine the embedding of the words. As a result, the same word might map to different vectors in a neural space depending on its context, such as “bank of a river” vs. “ bank of America”. This is typically achieved by using a Bi-directional Long Short-Term Memory (BiLSTM) network.</p><p id="c03c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Attention Layer</strong></p><p id="e8cd" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">It couples the question and passage vectors and produces a set of query-aware feature vectors for each word in the passage, and generates the working memory M over which reasoning is performed.</p><p id="86a4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Reasoning</strong></p><p id="61f0" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">MRC models can be grouped into different categories based on how they perform reasoning to generate the answer: single-step and multi-step models.</p><p id="a17f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Single-Step Reasoning</strong></p><p id="a2d8" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A single-step reasoning model matches the question and document only once and produce the final answers.</p><p id="9f2a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Multi-Step Reasoning.</strong></p><p id="d8f8" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Multi-step reasoning models are the dynamic multi-step reasoning models have to be trained using RL methods, e.g., policy gradient, which are tricky to implement due to the instability issue. SAN combines the strengths of both types of multi-step reasoning models.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm lz"><div class="fk r fd fl"><div class="ma r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*Y9YLiuCn6zXeQhs9W-QAWA.jpeg?q=20" width="854" height="358" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="854" height="358" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1708/1*Y9YLiuCn6zXeQhs9W-QAWA.jpeg" width="854" height="358" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://dl.acm.org/citation.cfm?id=3210183" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="c618" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">10. Goal-Oriented Dialog Management for Conversational AI with Transfer Learning</strong></p><p id="3ad2" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Transfer Learning</strong></p><p id="1f47" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The main goal of this work is to study the impact of a widely used technique Transfer Learning on goal oriented bots. As the name suggests, transfer learning transfers knowledge from one neural network to another. The former is known as the source, while the latter is the target. The goal of the transfer is to achieve better performance on the target domain with limited amount of training data, while benefiting from additional information from the source domain. In the case of dialogue systems, the input space for both source and target nets are their respective dialogue spaces.</p><p id="f96d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The klGoal-oriented bots contain an initial natural understanding (NLU) component, that is tasked with determining the user’s intent and its parameters, also known as slots . The usual practice in the RL-based Goal-Oriented Chatbots is to define the user-bot interactions as semantic frames. The entire dialogue can be reduced to a set of slot-value pairs, called semantic frames. Consequently, the conversation can be executed on two distinct levels: <strong class="gu hg">Semantic level:</strong> In this level the user sends and receives only a semantic frames as messages.</p><p id="7d75" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Natural language level: </strong>In this level<strong class="gu hg"> </strong>the user sends and receives natural language sentences, which are reduced to, or derived from a semantic frame by using Natural Language Understanding (NLU) and Natural Language Generation (NLG) units respectively. It consists of two independent units which are the User Simulator on the left side and the Dialogue Manager (DM) on the right side. We operate on the semantic level, removing the noise introduced by the NLU and NLG units.</p><p id="a44d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">There are some mechanism which is used in Goal-Oriented Dialog Management for Conversational AI with Transfer Learning which explain as below</p><p id="ba3e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">User Simulator</strong></p><p id="6ded" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The User Simulator creates a user — bot conversation, given the semantic frames. Because the model is based on Reinforcement Learning, a dialogue simulation is necessary to successfully train the model. The user goal consists of two different sets of slots as<strong class="gu hg"> </strong>inform slots and request slots. Inform slots are the slots for which the user knows the value, i.e. they represent the user constraints (e.g. {movie name: “avengers”, number of people: “3”, date: “tomorrow”}) and Request slots are ones for which the user is looking for an answer (e.g. { city, theater, start time } }).</p><p id="200b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Dialogue Manager</strong></p><p id="d6c3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The Dialogue Manager (DM), as its name suggests, manages the dialogue flow in order to conduct a proper dialogue with the user. The DM is composed by two trainable sub components: the Dialogue State Tracker (DST) and the Policy Learning Module, i.e. the agent. Additionally, the Dialogue Manager exploits an external Knowledge Base (KB), to find and suggest values for the user requests. Therefore, it plays a central role in the entire Dialogue System.</p><p id="942f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Dialogue State Tracker</strong></p><p id="c17e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The responsibility of the Dialogue State Tracker (DST) is to build a reliable and robust representation of the current state of the dialogue. All system actions are based on the current dialogue state. It keeps track of the history of the user utterances, system actions and the querying results from the Knowledge Base. It extracts features and creates a vector embedding of the current dialogue state, which is exposed and used by the Policy Learning module later on. In order to produce the embeddings, the Dialogue State Tracker must know the type of all slots and intents that might occur during the dialogue. Since we operate on a semantic level (i.e. not introducing any additional noise), we employ a rule-based state tracker.</p><p id="a402" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Policy Learning</strong></p><p id="e131" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The Policy Learning module selects the next system actions to drive the user towards the goal in the smallest number of steps. It does that by using the deep reinforcement neural networks, called Deep Q-Networks (DQN) .</p><p id="36dc" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The picture illustrate below shows the difference between with and without Transfer Learning Technique,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm mb"><div class="fk r fd fl"><div class="mc r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*poRTfXSDPUAofHti5QiYPw.jpeg?q=20" width="1097" height="398" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="1097" height="398" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/2194/1*poRTfXSDPUAofHti5QiYPw.jpeg" width="1097" height="398" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://dl.acm.org/citation.cfm?id=3210183" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="a90b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">11. Deep Reinforcement Learning Chatbot Model</strong></p><p id="46c9" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowd sourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data. Our system consists of an ensemble of response models. The response models take as input a dialogue and output a response in natural language text. In addition, the response models may also output one or several scalar values, indicating their internal confidence. As will be explained later, the response models have been engineered to generate responses on a diverse set of topics using a variety of strategies.</p><p id="711c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture illustrate the work flow of response generation and evaluation of in Deep reinforcement learning algorithm,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm md"><div class="fk r fd fl"><div class="me r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*zKtuBsBSnpPzOfasZ2yx-A.jpeg?q=20" width="769" height="347" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="769" height="347" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1538/1*zKtuBsBSnpPzOfasZ2yx-A.jpeg" width="769" height="347" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://dl.acm.org/citation.cfm?id=3210183" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="2905" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The dialogue manager is responsible for combining the response models together. As input, the dialogue manager expects to be given a dialogue history (i.e. all utterances recorded in the dialogue so far, including the current user utterance) and confidence values of the automatic speech recognition system (ASR confidences) or text based generated response. To generate a response, the dialogue manager follows a three-step procedure. First, it uses all response models to generate a set of candidate responses. Second, if there exists a priority response in the set of candidate responses (i.e. a response which takes precedence over other responses), this response will be returned by the system. For example, for the question “What is your name?”, the response “I am an Alexa Prize socialbot” is a priority response. Third, if there are no priority responses, the response is selected by the model selection policy. For example, the model selection policy may select a response by scoring all candidate responses and picking the highest-scored response.</p><p id="ae9d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Response Models</strong></p><p id="6d85" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">There are 22 response models in the system, including retrieval-based neural networks, generation-based neural networks, knowledge base question answering systems and template-based systems.</p><p id="01dc" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Template-based Models</strong></p><p id="cb70" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Templates to produce a response given the dialogue history and user utterance By default all templates generate non-priority responses, so we configure templates related to the socialbot’s name, age and location to output priority responses. We modify a few templates further to make them consistent with the challenge (e.g. to avoid obscene language and to encourage the user to discuss certain topics, such as news, politics and movies). The majority of templates remain unchanged.</p><p id="44f2" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Knowledge Base-based Question Answering</strong></p><p id="be42" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">They use a policy-based agent with continuous states based on KB embeddings to traverse the knowledge graph to identify the answer node (entity) for an input query. The RL-based methods are as robust as the neural methods due to the use of continuous vectors for state representation, and are as interpretable as symbolic methods because the agents explicitly traverse the paths in the graph.</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm mf"><div class="fk r fd fl"><div class="mg r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*xgti_ctfHtVY_p9hsgY8WQ.jpeg?q=20" width="808" height="319" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="808" height="319" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1616/1*xgti_ctfHtVY_p9hsgY8WQ.jpeg" width="808" height="319" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://dl.acm.org/citation.cfm?id=3210183" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">source</a></figcaption></figure><p id="84d3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Retrieval-based Neural Networks</strong></p><p id="7c45" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">VHRED models:</strong> The system contains several VHRED models (Latent Variable Hierarchical Recurrent Encoder-Decoder) , sequence-to-sequence models with Gaussian latent variables trained as variational auto-encoders . The trained VHRED models generate candidate responses as follows. First, a set of K model responses are retrieved from a dataset using cosine similarity between the current dialogue history and the dialogue history in the dataset based on bag-of-words TF-IDF Glove word embeddings. An approximation of the log-likelihood for each of the 20 responses is computed by VHRED, and the response with the highest log-likelihood is returned.</p><p id="5093" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Bag-of-words Retrieval Models: </strong>The system contains three bag-of-words retrieval models based on TF-IDF Glove word embeddings and Word2Vec embeddings. Similar to the VHRED models, these models retrieve the response with the highest cosine similarity.</p><p id="576d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Retrieval-based Logistic Regression</strong></p><p id="169a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The system contains a response model, called BoWEscapePlan, which returns a response from a set of 35 topic-independent, generic pre-defined responses, such as “Could you repeat that again”, “I don’t know” and “Was that a question?”. Its main purpose is to maintain user engagement and keep the conversation going, when other models are unable to provide meaningful responses. This model uses a logistic regression classifier to select its response based on a set of higher-level features.</p><p id="2458" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Search Engine-based Neural Networks</strong></p><p id="6343" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The system contains a deep classifier model, called LSTMClassifierMSMarco, which chooses its response from a set of search engine results. The system searches the web with the last user utterance. as query, and retrieves the first 10 search snippets. The retrieved snippets are preprocessed by stripping trailing words, removing unnecessary punctuation and truncating to the last full sentence. The model uses a bidirectional LSTM to separately map the last dialogue utterance and the snippet to their own embedding vectors. The resulting two representations are concatenated and passed through an MLP to predict a scalar-value between 0 − 1 indicating how appropriate the snippet is as a response to the utterance.</p><p id="b93c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Generation-based Neural Networks</strong></p><p id="38fa" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The system contains a generative recurrent neural network language model, called GRUQuestion-Generator, which can generate follow-up questions word-by-word, conditioned on the dialogue history. The input to the model consists of three components: a one-hot vector of the current word, a binary question label and a binary speaker label. The model contains two GRU layers and softmax output layer.</p><p id="d064" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Model Selection Policy and Architecture</strong></p><p id="388d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">After generating the candidate response set, the dialogue manager uses a model selection policy to select the response it returns to the user. The dialogue manager must select a response which increases the satisfaction of the user for the entire dialogue. It must make a trade-off between immediate and long-term user satisfaction. For example, suppose the user asks to talk about politics. If the dialogue manager chooses to respond with a political joke, the user may be pleased for one turn. Afterwards, however, the user may be disappointed with the system’s inability to debate political topics. Instead, if the dialogue manager chooses to respond with a short news story, the user may be less pleased for one turn. However, the news story may influence the user to follow up with factual questions, which the system may be better adept at handling. To make the trade-off between immediate and long-term user satisfaction, we consider selecting the appropriate response as a sequential decision making problem. This section describes five approaches to learn the model selection policy. These approaches are all evaluated with real-world users in the next section.</p><p id="38c4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Action-value Parametrization</strong></p><p id="a386" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The use of an action-value function for selecting dialogue responses is closely related to where a model is learned to predict the quality of a dialogue system response.</p><p id="c5c1" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Model Architecture</strong></p><p id="695b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below diagram represents the flow of a single round, one complete loop, in training with Reinforcement Learning algorithm,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="fb fc fd fe ak"><div class="cl cm ht"><div class="fk r fd fl"><div class="mh r"><div class="ff fg cp t u fh ak dk fi fj"><img class="cp t u fh ak fn fo fp" src="https://miro.medium.com/max/60/1*nXo_4fkIorrGdsFGRdhJ2w.png?q=20" width="800" height="510" role="presentation"/></div><img class="ff fg cp t u fh ak fq" width="800" height="510" role="presentation"/><noscript><img class="cp t u fh ak" src="https://miro.medium.com/max/1600/1*nXo_4fkIorrGdsFGRdhJ2w.png" width="800" height="510" role="presentation"/></noscript></div></div></div></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://towardsdatascience.com/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383" class="at cg fw fx fy fz" target="_blank" rel="noopener">source</a></figcaption></figure><p id="1b72" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Chatbot Query and Response</strong></p><p id="4432" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Example 1</strong> (A &amp; B are both a trained chatbot)</p><p id="f51c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A: Can you do me a favor?</p><p id="6194" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">B: Anything.</p><p id="b1b4" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A: I want you to bend a father.</p><p id="8220" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">B: Of course.</p><p id="7cce" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A: I’m so sorry I didn’t mean to be rude but.</p><p id="f5ab" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">B: Don’t get it at least it’s about as much as I can.</p><p id="b92c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Example 2</strong> (A &amp; B are both a trained chatbot)</p><p id="32e3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A: I didn’t do it! I haven’t got a gun!</p><p id="6fe7" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">B: You liar. You still want to get your money back.</p><p id="a2a3" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A: Something wrong with the truth.</p><p id="9e2a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">B: I don’t want any money just a little I can’t take just out of this.</p><p id="b849" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A: What’s wrong.</p><p id="0345" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">B: Listen to me. I’m scared now.</p><p id="4ab5" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">A: Ah you’ll need all the money I need your help.</p><p id="95ef" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">12. Coding and Implementation</strong></p><h2 id="c13e" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn">Dataset</h2><p id="bfc9" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">In this post I’ll work with the Cornell_Movie-Dialogs_Corpus (<a href="http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">link</a>).</p><p id="c571" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">DESCRIPTION:</strong></p><p id="5a57" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:</p><p id="444c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- 220,579 conversational exchanges between 10,292 pairs of movie characters</p><p id="464d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- involves 9,035 characters from 617 movies</p><p id="24b7" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- in total 304,713 utterances</p><p id="033e" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- movie metadata included:</p><p id="dd67" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- genres</p><p id="860c" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- release year</p><p id="b40a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- IMDB rating</p><p id="c54d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- number of IMDB votes</p><p id="467d" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- IMDB rating</p><p id="41ff" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- character metadata included:</p><p id="f269" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- gender (for 3,774 characters)</p><p id="48ce" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">- position on movie credits (3,321 characters)</p><p id="0b8a" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Preprocessing</strong></p><p id="dc93" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The <a href="http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">Cornell_Movie-Dialogs_Corpus</a> dataset is a natural language dataset and can’t be used in its exact form. It needs to be converted in a suitable data structure in order to use it for further computation and processing.</p><p id="9414" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Tokenize- </strong>First step in the pre-processing is to tokenize the sentences into different words. For example, ‘Bob dropped the apple. Where is the apple?’ is tokenized to [‘Bob’, ‘dropped’, ‘the’, ‘apple’, ‘.’, ‘Where’, ‘is’, ‘the’, ‘apple’, ‘ ?’]</p><p id="c105" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Splitting into Story, Questions, and answers:</strong> Next, the sentences were split into stories, questions and answers so that they can be fed to the proposed models.</p><p id="e2ec" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Combining all the stories-</strong> All the stories were then combined up to the point that the question was asked. This finally becomes the story for that particular question.</p><p id="08c5" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Indexing the stories, questions, and answers-</strong> Finally, the questions and stories are indexed according to their time of occurrence and are eventually processed via word2vec model. The answers are transformed to one hot encoded vector.</p><h2 id="02ed" class="ga gb df bk bj gc gd ge gf gg gh gi gj gk gl gm gn"><strong class="az">Creating the model</strong></h2><p id="1845" class="gr gs df bk gu b gv hz gx ia gz ib hb ic hd id hf ep">Now that we have inputs, parsing, evaluation and training it’s time to write code for our Dual LSTM neural network. Because we have different formats of training and evaluation data I’ve written a <code class="fl jl jm jn io b"><a href="https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_model.py" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">c</a>hatbot_model.py</code> wrapper that takes care of bringing the data into the right format for us. It takes a <code class="fl jl jm jn io b">model_impl</code> argument, which is a function that actually makes predictions.</p><p id="2874" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">Defining Evaluation Metrics</strong></p><p id="5788" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">Tensorflow already comes with many standard evaluation metrics that we can use. To use these metrics we need to create a dictionary that maps from a metric name to a function that takes the predictions and label.</p><pre class="hl hm hn ho hp il im in"><span id="7ae0" class="ga gb df bk io b dj ip iq r ir">tf.metrics.accuracy(<br/>    labels,<br/>    predictions,<br/>    weights=None,<br/>    metrics_collections=None,<br/>    updates_collections=None,<br/>    name=None<br/>)</span></pre><p id="e312" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The code is available on my <a href="https://github.com/kunalBhashkar/seq2seq_chatbot_tensorflow" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">github</a> Profile:<a href="https://github.com/kunalBhashkar/seq2seq_chatbot_tensorflow" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow"><em class="gt"> github link</em></a></p><p id="598f" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep">The below picture are the some Screenshots of the output,</p><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="n p aq"><img class="kt" src="https://miro.medium.com/proxy/1*OfWZKmm36T3W2UK2lekPPg.png" role="presentation"/></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://github.com/kunalBhashkar/seq2seq_chatbot_tensorflow/tree/master/chatbot_screenshots" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">screenshot</a></figcaption></figure><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="n p aq"><img class="kt" src="https://miro.medium.com/proxy/1*bNsR9wdKfNE_pt1k-tkCMg.png" role="presentation"/></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://github.com/kunalBhashkar/seq2seq_chatbot_tensorflow" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">screenshot</a></figcaption></figure><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="n p aq"><img class="kt" src="https://miro.medium.com/proxy/1*Tf-kCVxWEbb2Wgq7xG7WbA.png" role="presentation"/></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://github.com/kunalBhashkar/seq2seq_chatbot_tensorflow" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">screenshots</a></figcaption></figure><figure class="hl hm hn ho hp fa cl cm paragraph-image"><div class="n p aq"><img class="kt" src="https://miro.medium.com/proxy/1*vlrl52ujalDaTAxdyXzEdQ.png" role="presentation"/></div><figcaption class="bo dj fr fs ft cn cl cm fu fv bj di"><a href="https://github.com/kunalBhashkar/seq2seq_chatbot_tensorflow" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">screenshots</a></figcaption></figure><p id="2f0b" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf ep"><strong class="gu hg">REFERENCES</strong></p><ul class=""><li id="afc5" class="gr gs df bk gu b gv gw gx gy gz ha hb hc hd he hf jk hi hj"><a href="https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=1645&amp;context=etd_projects" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">Deep Learning for Chatbots</a> (<a href="https://scholarworks.sjsu.edu/etd_projects/630/" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="9898" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Dialogue Intent Classification with Long Short-Term Memory Networks (<a href="http://tcci.ccf.org.cn/conference/2017/papers/1158.pdf" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="6c15" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Sequence to Sequence Learning with Neural Networks(<a href="https://arxiv.org/abs/1409.3215" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="869b" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">A Neural Conversational Model (<a href="https://arxiv.org/abs/1506.05869" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="69d2" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Neural Machine Translation by Jointly Learning to Align and Translate (<a href="https://arxiv.org/abs/1409.0473" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="5bf3" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Effective Approaches to Attention-based Neural Machine Translation (<a href="https://arxiv.org/abs/1508.04025" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="fe8c" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Neural Approaches to Conversational AI (<a href="https://dl.acm.org/citation.cfm?id=3210183" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="6d06" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Neural Response Generation via GAN with an Approximate Embedding Layer (<a href="http://www.aclweb.org/anthology/D17-1065" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="46e3" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Task-oriented Conversational Agent Self-learning Based on Sentiment Analysis (<a href="http://ceur-ws.org/Vol-2244/paper_01.pdf" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="c1aa" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Deep Reinforcement Learning for Dialogue Generation (<a href="https://arxiv.org/abs/1606.01541" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="c8c8" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Topic Aware Neural Response Generation (<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14563/14260" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="79af" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Response Selection with Topic Clues for Retrieval-based Chatbots (<a href="https://www.sciencedirect.com/science/article/pii/S0925231218309093" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="4175" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Bidirectional Recurrent Neural Networks as Generative Models (<a href="http://papers.nips.cc/paper/5651-bidirectional-recurrent-neural-networks-as-generative-models" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li><li id="0078" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj"><a href="https://github.com/oswaldoludwig/Adversarial-Learning-for-Generative-Conversational-Agents" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">Adversarial-Learning-for-Generative-Conversational-Agents</a> (<a href="https://github.com/oswaldoludwig/Adversarial-Learning-for-Generative-Conversational-Agents" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">link</a>)</li><li id="f9dc" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Few-Shot Generalization Across Dialogue Tasks (<a href="https://arxiv.org/abs/1811.11707" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">link</a>)</li><li id="e239" class="gr gs df bk gu b gv ig gx ih gz ii hb ij hd ik hf jk hi hj">Neural Networks for Text Correction and Completion in Keyboard Decoding (<a href="https://arxiv.org/abs/1709.06429" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow">paper</a>)</li></ul></div></div></section></div></article><div class="ff ct mi s ak mp mn mq" data-test-id="post-sidebar"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="mr n co"><div class="ct"><div class="ms mt mu n"><div class="n o"><div class="mv r fd"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40BhashkarKunal%2Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3&amp;source=post_sidebar-----38dc5cf5a5a3---------------------clap_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><div class="ba mw mx my mz na nb nc nd ne nf"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></div><div class="ng r"><div class="nh"><h4 class="bj di dj bl bo"><button class="at au av aw ax ay az ba bb bc bd be bf bg bh bi">1.1K </button></h4></div></div></div></div><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40BhashkarKunal%2Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3&amp;source=post_sidebar--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div><div class="ff ct mi s mj mk ml mm mn mo"></div><div><div class="ni fa n co p"><div class="n p"><div class="ac ae af ag ah cv aj ak"><div class="n nj"></div><div class="n o nj"></div><div class="nk r"><ul class="ba bb"><li class="cf nl nm nn"><a href="/tag/deep-learning" class="no np cg bo r im nq a b ee">Deep Learning</a></li></ul></div><div class="nr n cx ab"><div class="n o"><div class="ns r fd"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40BhashkarKunal%2Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3&amp;source=post_actions_footer-----38dc5cf5a5a3---------------------clap_footer-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><div class="c nt db n o nu fd nv nw nx ny nz oa ob oc od oe of og oh oi"><div class="ba mw mx my mz na oj nc o fq db n p ok u fh cp t ak nd ne nf ol"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></div></div></a></div><div class="ng r"><div class="nh"><h4 class="bj di dj bl df"><button class="at au av aw ax ay az ba bb bc bd be bf bg bh bi">1.1K claps</button></h4></div></div></div><div class="n o"><div class="en r ar"><a href="//medium.com/p/38dc5cf5a5a3/share/twitter?source=post_actions_footer---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="en r ar"><a href="//medium.com/p/38dc5cf5a5a3/share/facebook?source=post_actions_footer---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="en r ar"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40BhashkarKunal%2Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3&amp;source=post_actions_footer--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div><div class="cf" aria-hidden="true"><div class="r ar"><button class="at au av aw ax ay az ba bb bc bd be bf bg bh bi"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div><div class="om on oo nk r ab"><div class="op oq r fd"><span class="r or an os"><div class="r cp ot ou"><a rel="noopener" href="/@BhashkarKunal?source=follow_footer--------------------------follow_footer-"><img alt="Kunal Bhashkar" class="r db ov ow" src="https://miro.medium.com/fit/c/160/160/2*HoGS0ii9whuuXey5DW-URw.jpeg" width="80" height="80"/></a></div><span class="r"><div class="ox r oy"><p class="bj di ee bl bo oz pa">Written by</p></div><div class="ox pb n oy"><div class="ak n o cx"><h2 class="bj gc pc pd df"><a class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener" href="/@BhashkarKunal?source=follow_footer--------------------------follow_footer-">Kunal Bhashkar</a></h2><div class="r g"><button class="bx df q by dt du dv dw bc bh dx dy dz ea eb ec cb bj b bk bl bm bn cc cd ce cf cg bf">Follow</button></div></div></div></span></span><div class="ox pe r oy aq"><div class="pf r"><h4 class="bj di pg ph bo">#DataScientist #PursuingPhD #DeepLearning #JNU #NewDelhi</h4></div><div class="ap pi aq"><button class="bx df q by dt du dv dw bc bh dx dy dz ea eb ec cb bj b bk bl bm bn cc cd ce cf cg bf">Follow</button></div></div></div></div><div class="pj on r ab"><a href="https://medium.com/p/38dc5cf5a5a3/responses/show?source=follow_footer--------------------------follow_footer-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><span class="pk pl mz"><div class="il pm cb r ft aq"><span class="br">See responses (3)</span></div></span></a></div></div></div><div class="pn r po ab"><div class="n p"><div class="ac ae af ag ah ai aj ak"></div></div></div></div></div><div class="pp r pq pr"><section class="cl cm ak ce r ps pt pu pv pw px py pz qa qb qc qd qe qf qg"><div class="qh qi op n cx g"><div class="qj n cx"><div class="qk r ql"><div class="qm r"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----38dc5cf5a5a3----------------------" class="at au av aw ax ay az ba bb bc qn qo bf bg qp qq" rel="noopener"><h4 class="qr qs qt bj gc bk ph qu qv r">Discover <!-- -->Medium</h4></a></div><span class="bj b bk bl bm bn r qw qx">Welcome to a place where words matter. On <!-- -->Medium<!-- -->, smart voices and original ideas take center stage - with no ads in sight.<!-- --> <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----38dc5cf5a5a3----------------------" class="at au av aw ax ay az ba bb bc bf bg qp qq qy" rel="noopener">Watch</a></span></div><div class="qk r ql"><div class="qz r"><a href="https://medium.com/topics?source=post_page-----38dc5cf5a5a3----------------------" class="at au av aw ax ay az ba bb bc qn qo bf bg qp qq" rel="noopener"><h4 class="qr qs qt bj gc bk ph qu qv r">Make <!-- -->Medium<!-- --> yours</h4></a></div><span class="bj b bk bl bm bn r qw qx">Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox.<!-- --> <a href="https://medium.com/topics?source=post_page-----38dc5cf5a5a3----------------------" class="at au av aw ax ay az ba bb bc bf bg qp qq qy" rel="noopener">Explore</a></span></div><div class="qk r ql"><div class="qm r"><a href="https://medium.com/membership?source=post_page-----38dc5cf5a5a3----------------------" class="at au av aw ax ay az ba bb bc qn qo bf bg qp qq" rel="noopener"><h4 class="qr qs qt bj gc bk ph qu qv r">Become a member</h4></a></div><span class="bj b bk bl bm bn r qw qx">Get unlimited access to the best stories on <!-- -->Medium<!-- --> — and support writers while you’re at it. Just $5/month.<!-- --> <a href="https://medium.com/membership?source=post_page-----38dc5cf5a5a3----------------------" class="at au av aw ax ay az ba bb bc bf bg qp qq qy" rel="noopener">Upgrade</a></span></div></div></div><div class="n o cx"><a class="at au av aw ax ay az ba bb bc qn qo bf bg qp qq" rel="noopener" href="/?source=post_page-----38dc5cf5a5a3----------------------"><svg height="22" width="112" viewBox="0 0 111.5 22" class="qs"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></a><span class="bj b bk bl bm bn r qw qx"><div class="ra rb n cx rc an"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----38dc5cf5a5a3----------------------" class="at au av aw ax ay az ba bb bc dq bf bg qp qq" rel="noopener">About</a><a href="https://help.medium.com/?source=post_page-----38dc5cf5a5a3----------------------" class="at au av aw ax ay az ba bb bc dq bf bg qp qq" rel="noopener">Help</a><a class="at au av aw ax ay az ba bb bc dq bf bg qp qq" rel="noopener" href="/policy/9db0094a1e0f?source=post_page-----38dc5cf5a5a3----------------------">Legal</a></div></span></div></section></div></div></div><script src="https://cdn.optimizely.com/js/16180790160.js"></script><script>window.__BUILD_ID__ = "development"</script><script>window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20200215-001122-8354a55ad7","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20200215-001122-8354a55ad7"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"internalLinksPostIds":["35d26bf7d678","ba103974d8e8","42988b81376e","ca70a379ce08","a987957e885d"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440}},"performanceTags":[]},"debug":{"requestId":"ce4cead7-2b13-49c2-86f6-bb6c9032e399","originalSpanCarrier":{"ot-tracer-spanid":"3f519a325495263f","ot-tracer-traceid":"503e40867f46356e","ot-tracer-sampled":"true"}},"session":{"user":{"id":"lo_sgas4TGuPPTV"},"xsrf":""},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":true},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"currentLocation":"https:\u002F\u002Fmedium.com\u002F@BhashkarKunal\u002Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3","host":"medium.com","hostname":"medium.com","susiModal":{"step":null,"operation":"register","reportEventInfo":{"eventName":"","data":{}}}},"client":{"isBot":false,"isEu":false,"isLinkedin":false,"isNativeMedium":false,"isCustomDomain":false},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY.variantFlags.0":{"name":"add_friction_to_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.3":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.4":{"name":"assign_default_topic_to_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.5":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.6":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.7":{"name":"branch_seo_metadata","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.9":{"name":"disable_android_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.10":{"name":"disable_go_social_jubilee","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"disable_ios_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"disable_post_recommended_from_friends_provider","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_android_local_currency","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_auto_tier","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_curation_priority_queue_experiment","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_expanded_feature_chunk_pool","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_filter_by_resend_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.37":{"name":"enable_json_logs_trained_ranker","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"enable_kbfd_rex","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_kbfd_rex_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_kbfd_rex_daily_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_lite_server_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_lo_homepage","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_lo_meter_swap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_membership_thank_you_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_miro_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_mk_branch_cleanup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagString","value":"app-button"},"ROOT_QUERY.variantFlags.62":{"name":"enable_ml_rank_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_more_branch_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_new_pub_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_new_three_dot_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.71":{"name":"enable_popularity_feature","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"enable_post_seo_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.74":{"name":"enable_post_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"enable_rito_caching","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"enable_rito_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.79":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"enable_sign_up_with_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"enable_suggest_account","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.82":{"name":"enable_suggest_account_li","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.83":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.84":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.84.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.84.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.85":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.85.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.85.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.86":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.86.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.86.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.87":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.87.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.87.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.88":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.88.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.88.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.89":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.89.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.89.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.90":{"name":"iceland_home_page_loadtest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.90.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.90.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.91":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.91.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.91.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.92":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.92.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.92.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.93":{"name":"pardon_the_interruption_4","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.93.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.93.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.94":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.94.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.94.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.95":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.95.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.95.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.96":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.96.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.96.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.97":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.97.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.97.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.98":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.98.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.98.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.99":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.99.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.99.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY":{"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.84","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.85","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.86","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.87","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.88","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.89","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.90","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.91","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.92","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.93","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.94","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.95","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.96","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.97","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.98","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.99","typename":"VariantFlag"}],"meterPost({\"postId\":\"38dc5cf5a5a3\",\"postMeteringOptions\":{}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"38dc5cf5a5a3\"})":{"type":"id","generated":false,"id":"Post:38dc5cf5a5a3","typename":"Post"},"viewer":null},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":3,"unlocksRemaining":3},"Post:38dc5cf5a5a3":{"__typename":"Post","visibility":"PUBLIC","latestPublishedVersion":"fb82ec0892d3","collection":null,"id":"38dc5cf5a5a3","creator":{"type":"id","generated":false,"id":"User:11efd7fa5964","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","sequence":null,"mediumUrl":"https:\u002F\u002Fmedium.com\u002F@BhashkarKunal\u002Fconversational-ai-chatbot-using-deep-learning-how-bi-directional-lstm-machine-reading-38dc5cf5a5a3","canonicalUrl":"","content({\"postMeteringOptions\":{}})":{"type":"id","generated":true,"id":"$Post:38dc5cf5a5a3.content({\"postMeteringOptions\":{}})","typename":"PostContent"},"firstPublishedAt":1547364982973,"isPublished":true,"layerCake":0,"primaryTopic":null,"title":"Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading…","isLimitedState":false,"pendingCollection":null,"shareKey":null,"statusForCollection":null,"readingTime":58.7254716981132,"readingList":"READING_LIST_NONE","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"tags":[{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"}],"viewerClapCount":0,"clapCount":1110,"voterCount":191,"recommenders":[],"responsesCount":3,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"isDistributionAlertDismissed":false,"audioVersionUrl":"","seoTitle":"","socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1547989217542,"previewContent":{"type":"id","generated":true,"id":"$Post:38dc5cf5a5a3.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*-1u87cEgguuCa5QgU93m7A.jpeg","typename":"ImageMetadata"},"updatedAt":1547989217542,"topics":[],"seoDescription":"","responses":{"type":"id","generated":true,"id":"$Post:38dc5cf5a5a3.responses","typename":"StreamConnection"},"isSuspended":false},"User:11efd7fa5964":{"id":"11efd7fa5964","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Kunal Bhashkar","isFollowing":false,"username":"BhashkarKunal","bio":"#DataScientist #PursuingPhD #DeepLearning #JNU #NewDelhi","imageId":"2*HoGS0ii9whuuXey5DW-URw.jpeg","mediumMemberAt":0,"isBlocking":false,"isPartnerProgramEnrolled":false,"twitterScreenName":"BhashkarKunal"},"$Post:38dc5cf5a5a3.content({\"postMeteringOptions\":{}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:38dc5cf5a5a3.content({\"postMeteringOptions\":{}}).bodyModel","typename":"RichText"}},"$Post:38dc5cf5a5a3.content({\"postMeteringOptions\":{}}).bodyModel.sections.0":{"name":"d9ff","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:38dc5cf5a5a3.content({\"postMeteringOptions\":{}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:38dc5cf5a5a3.content({\"postMeteringOptions\":{}}).bodyModel.sections.0","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_60","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_61","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_62","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_63","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_64","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_65","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_66","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_67","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_68","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_69","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_70","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_71","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_72","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_73","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_74","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_75","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_76","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_77","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_78","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_79","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_80","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_81","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_82","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_83","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_84","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_85","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_86","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_87","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_88","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_89","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_90","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_91","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_92","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_93","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_94","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_95","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_96","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_97","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_98","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_99","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_100","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_101","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_102","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_103","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_104","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_105","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_106","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_107","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_108","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_109","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_110","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_111","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_112","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_113","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_114","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_115","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_116","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_117","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_118","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_119","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_120","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_121","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_122","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_123","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_124","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_125","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_126","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_127","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_128","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_129","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_130","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_131","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_132","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_133","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_134","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_135","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_136","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_137","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_138","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_139","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_140","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_141","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_142","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_143","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_144","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_145","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_146","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_147","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_148","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_149","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_150","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_151","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_152","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_153","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_154","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_155","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_156","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_157","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_158","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_159","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_160","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_161","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_162","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_163","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_164","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_165","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_166","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_167","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_168","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_169","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_170","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_171","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_172","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_173","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_174","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_175","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_176","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_177","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_178","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_179","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_180","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_181","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_182","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_183","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_184","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_185","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_186","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_187","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_188","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_189","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_190","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_191","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_192","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_193","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_194","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_195","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_196","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_197","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_198","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_199","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_200","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_201","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_202","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_203","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_204","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_205","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_206","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_207","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_208","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_209","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_210","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_211","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_212","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_213","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_214","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_215","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_216","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_217","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_218","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_219","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_220","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_221","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_222","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_223","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_224","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_225","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_226","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_227","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_228","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_229","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_230","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_231","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_232","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_233","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_234","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_235","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_236","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_237","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_238","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_239","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_240","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_241","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_242","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_243","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_244","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_245","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_246","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_247","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_248","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_249","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_250","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_251","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_252","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_253","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_254","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_255","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_256","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_257","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_258","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_259","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_260","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_261","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_262","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_263","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_264","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_265","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_266","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_267","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_268","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_269","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_270","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_271","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_272","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_273","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_274","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_275","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_276","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_277","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_278","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_279","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_280","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_281","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_282","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_283","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_284","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_285","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_286","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_287","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_288","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_289","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_290","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_291","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_292","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_293","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_294","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_295","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_296","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_297","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_298","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_299","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_300","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_301","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_302","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_303","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_304","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_305","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_306","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_307","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_308","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_309","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_310","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_311","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_312","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_313","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_314","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_315","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_316","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_317","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_318","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_319","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_320","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_321","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_322","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_323","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_324","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_325","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_326","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_327","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_328","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_329","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_330","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_331","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_332","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_333","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_334","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_335","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_336","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_337","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_338","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_339","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_340","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_341","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_342","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_343","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_344","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_345","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_346","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_347","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_348","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_349","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_350","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_351","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_352","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_353","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_354","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_355","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_356","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_357","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_358","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_359","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_360","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_361","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_362","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_363","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_364","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_365","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_366","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_367","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_368","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_369","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_370","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_371","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_372","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_373","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_374","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_375","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_376","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_377","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_378","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_379","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_380","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_381","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_382","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_383","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_384","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_385","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_386","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_387","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_388","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_389","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_390","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_391","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_392","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_393","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_394","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_395","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_396","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_397","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_398","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_399","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_400","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_401","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_402","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_403","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_404","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_405","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_406","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_407","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_408","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_409","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_410","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_411","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_412","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_413","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_414","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_415","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_416","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_417","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_418","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_419","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_420","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_421","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_422","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_423","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_424","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_425","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_426","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_427","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_428","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_429","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_430","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_431","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_432","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_433","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_434","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_435","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_436","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_437","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_438","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_439","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_440","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_441","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_442","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_443","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_444","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_445","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_446","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_447","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_448","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_449","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_450","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_451","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_452","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_453","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_454","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:fb82ec0892d3_455","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:fb82ec0892d3_0":{"id":"fb82ec0892d3_0","name":"3ae7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*-1u87cEgguuCa5QgU93m7A.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_0.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-1u87cEgguuCa5QgU93m7A.jpeg":{"id":"1*-1u87cEgguuCa5QgU93m7A.jpeg","originalHeight":720,"originalWidth":1280,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_0.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=8lG6qRIdSA0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_1":{"id":"fb82ec0892d3_1","name":"a2b8","type":"H4","href":null,"layout":null,"metadata":null,"text":"Conversational AI Chatbot using Deep Learning: How Bi-directional LSTM, Machine Reading Comprehension, Transfer Learning, Sequence to Sequence Model with multi-headed attention mechanism, Generative Adversarial Network, Self Learning based Sentiment Analysis and Deep Reinforcement Learning can help in Dialog Management for Conversational AI chatbot","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_2":{"id":"fb82ec0892d3_2","name":"bc5b","type":"BQ","href":null,"layout":null,"metadata":null,"text":"keywords: NLU, NLG, Word Embedding, Tensorflow, RNN, Bi-directional LSTM, Generative Adversarial Network, Machine Reading Comprehension, Transfer Learning, Sequence to Sequence Model with multi-headed attention mechanism, Deep Reinforcement Learning, Self-learning based on Sentiment Analysis, Knowledge base, Recurrent Embedding Dialogue policy, Dual Encoder LSTM, Encoder-Decoder","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_2.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_2.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_3":{"id":"fb82ec0892d3_3","name":"c755","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Introduction","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_3.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_3.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_4":{"id":"fb82ec0892d3_4","name":"b30a","type":"P","href":null,"layout":null,"metadata":null,"text":"In this article, I will explain how we can create Deep Learning based Conversational AI. The basic definition of chatbot is, it is a computer software program designed to simulate human conversation via text or audio messages. Today’s AI systems can interact with users, understand their needs, map their preferences and recommend an appropriate line of action with minimal or no human intervention. There are lot of popular conversational agents are available today like Apple’s Siri, Microsoft’s Cortana, Google Assistant, and Amazon’s Alexa.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_5":{"id":"fb82ec0892d3_5","name":"a441","type":"P","href":null,"layout":null,"metadata":null,"text":"The basic foundation of chatbots is providing the best response of any query that it receives. The best response like answering the sender questions, providing sender relevant information, ask follow-up questions and do the conversation in realistic way.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_6":{"id":"fb82ec0892d3_6","name":"6bf6","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture illustrate the conceptual map of Chatbot using Deep learning,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_7":{"id":"fb82ec0892d3_7","name":"8baf","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*k4jQiwTE1SZEFcqi3Ayu0w.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_7.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*k4jQiwTE1SZEFcqi3Ayu0w.png":{"id":"1*k4jQiwTE1SZEFcqi3Ayu0w.png","originalHeight":458,"originalWidth":1036,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_7.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fscholarworks.sjsu.edu\u002Fetd_projects\u002F630\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_8":{"id":"fb82ec0892d3_8","name":"4fb1","type":"P","href":null,"layout":null,"metadata":null,"text":"The chatbot needs to be able to understand the intentions of the sender’s message, determine what type of response message (a follow-up question, direct response, etc.) is required, and follow correct grammatical and lexical rules while forming the response. Some models may use additional meta information from data, such as speaker id, gender, emotion. Sometimes, sentiment analysis is used to allows the chatbot to ‘understand’ the mood of the user by analysing verbal and sentence structuring clues.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_8.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_8.markups.0":{"type":"STRONG","start":395,"end":396,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_9":{"id":"fb82ec0892d3_9","name":"a1d7","type":"P","href":null,"layout":null,"metadata":null,"text":"The following picture shows that how Deep learning based chatbot work internally,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_10":{"id":"fb82ec0892d3_10","name":"d430","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*1X_eOEjeOfjZyRo68Ool1Q.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_10.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*1X_eOEjeOfjZyRo68Ool1Q.jpeg":{"id":"1*1X_eOEjeOfjZyRo68Ool1Q.jpeg","originalHeight":459,"originalWidth":1024,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_10.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fwww.microsoft.com\u002Fen-us\u002Fresearch\u002Fproject\u002Fdeep-reinforcement-learning-goal-oriented-dialogue\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_11":{"id":"fb82ec0892d3_11","name":"8d79","type":"P","href":null,"layout":null,"metadata":null,"text":"2. Role of NLU, NLG and Dialogue Management in Conversational AI","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_11.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_11.markups.0":{"type":"STRONG","start":0,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_12":{"id":"fb82ec0892d3_12","name":"32e5","type":"P","href":null,"layout":null,"metadata":null,"text":"Natural Language Understanding","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_12.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_12.markups.0":{"type":"STRONG","start":0,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_13":{"id":"fb82ec0892d3_13","name":"72de","type":"P","href":null,"layout":null,"metadata":null,"text":"The NLU unit is responsible for transforming the user utterance to a predefined semantic frame according to the system’s conventions, i.e. to a format understandable for the system. This includes a task of slot filling and intent detection. For example, the intent, could be a greeting, like Hello, Hi, Hey, or it could have an inform nature, for example I like Indian food, where the user is giving some additional information. Depending on the interests, the slots could be very diverse, like the actor name, price, start time, destination city etc. As we can see, the intents and the slots are defining the closed-domain nature of the Chatbot. The task of slot filling and intent detection is seen as a sequence tagging problem. For this reason, the NLU component is usually implemented as an LSTM-based recurrent neural network with a Conditional Random Field (CRF) layer on top of it. The model presented is a sequence-to-sequence model using bidirectional LSTM network, which fills the slots and predicts the intent in the same time. On the other hand, the model is doing the same using an attention-based RNN. To achieve such a task, the dataset labels consist of: concatenated B–I–O (Begin, Inside, Outside) slot tags, the intent tag and an additional end-of-string (EOS) tag. As an example, in a restaurant reservation scenario, given the sentence Are there any French restaurants in Toronto downtown?, the task is to correctly output, or fill, the following slots: {cuisine: French} and {location: Toronto downtown}.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_14":{"id":"fb82ec0892d3_14","name":"3f8a","type":"P","href":null,"layout":null,"metadata":null,"text":"The following picture shows the classification process for intent classification using Neural Network as,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_15":{"id":"fb82ec0892d3_15","name":"d4a2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*6zmRX0OyRnEKM35DI0J8oA.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_15.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*6zmRX0OyRnEKM35DI0J8oA.png":{"id":"1*6zmRX0OyRnEKM35DI0J8oA.png","originalHeight":360,"originalWidth":800,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_15.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fmedium.com\u002Fskyshidigital\u002Ftopic-and-intent-classifier-from-scratch-83278fb8cf3?fbclid=IwAR04l7oDNvmy6W4gHyKUoS8sgatgPzKJKD1Qvu6WqtGvGSm7neFbYInjFNc","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_16":{"id":"fb82ec0892d3_16","name":"59f3","type":"P","href":null,"layout":null,"metadata":null,"text":"Natural Language Generator (NLG)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_16.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_16.markups.0":{"type":"STRONG","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_17":{"id":"fb82ec0892d3_17","name":"f319","type":"P","href":null,"layout":null,"metadata":null,"text":"Natural Language Generation (NLG) is the process of generating text from a meaning representation. It can be taken as the reverse of the natural language understanding. NLG systems provide a critical role for text summarization, machine translation, and dialog systems. In the NLG, The system response as a semantic frame, it maps back to a natural language sentence, understandable for the end user. The NLG component can be rule-based or model-based. In some scenarios it can be a hybrid model, i.e. combination of both. The rule-based NLG outputs some predefined template sentences for a given semantic frame, thus they are very limited without any generalisation power. While several general-purpose rule-based generation systems have been developed, they are often quite difficult to adapt to small, task-oriented applications because of their generality. Machine learning based (trainable) NLG systems are more common in today’s dialog systems. Such NLG systems use several sources as input such as: content plan, representing meaning representation of what to communicate with the user, knowledge base, structured database to return domain-specific entities, user model, a model that imposes constraints on output utterance, dialog history, the information from previous turns to avoid repetitions, referring expressions, etc.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_18":{"id":"fb82ec0892d3_18","name":"1659","type":"P","href":null,"layout":null,"metadata":null,"text":"Trainable NLG systems can produce various candidate utterances (e.g., scholastically or rule base) and use a statistical model to rank them. The statistical model assigns scores to each utterance and is learnt based on textual data. Most of these systems use bigram and trigram language models to generate utterances.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_19":{"id":"fb82ec0892d3_19","name":"9039","type":"P","href":null,"layout":null,"metadata":null,"text":"On the other hand, In NLG based on a semantically controlled Long Short-term Memory (LSTM) recurrent network, It can learn from unaligned data by jointly optimising its sentence planning and surface realisation components using a simple cross entropy training criterion without any heuristics, and good quality language variation is obtained simply by randomly sampling the network outputs.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_20":{"id":"fb82ec0892d3_20","name":"4a4e","type":"P","href":null,"layout":null,"metadata":null,"text":"The following figure shows that the working of Semantic Controlled LSTM cell,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_21":{"id":"fb82ec0892d3_21","name":"7514","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*zv4FHb-C6hPNyjJGTlr_Xg.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_21.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*zv4FHb-C6hPNyjJGTlr_Xg.png":{"id":"1*zv4FHb-C6hPNyjJGTlr_Xg.png","originalHeight":464,"originalWidth":476,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_21.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1508.01745","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_22":{"id":"fb82ec0892d3_22","name":"a541","type":"P","href":null,"layout":null,"metadata":null,"text":"Dialogue Management (DM)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_22.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_22.markups.0":{"type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_23":{"id":"fb82ec0892d3_23","name":"1b82","type":"P","href":null,"layout":null,"metadata":null,"text":"The DM could be connected to some external Knowledge Base (KB) or Data Base (DB), such that it can produce more meaningful answers. The Dialogue Manager consists the following two components: the Dialogue State Tracker (DST) and the Policy Learning which is the Reinforcement Learning (RL) agent. The Dialogue State Tracker (DST) is a complex and essential component that should correctly infer the belief about the state of the dialogue, given all the history up to that turn. The Policy Learning is responsible for selecting the best action, i.e. the system response to the user utterance, that should lead the user towards achieving the goal in a minimal number of dialogue turns.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_24":{"id":"fb82ec0892d3_24","name":"b31c","type":"P","href":null,"layout":null,"metadata":null,"text":"The following figure is shows that how dialogue state Tracker and RL agent are working together,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_25":{"id":"fb82ec0892d3_25","name":"0a6b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*AdAo9Y8mrmyj75ZnZi7iGA.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*AdAo9Y8mrmyj75ZnZi7iGA.jpeg":{"id":"1*AdAo9Y8mrmyj75ZnZi7iGA.jpeg","originalHeight":358,"originalWidth":650,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_26":{"id":"fb82ec0892d3_26","name":"c6f6","type":"H4","href":null,"layout":null,"metadata":null,"text":"Types of dialog management","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_27":{"id":"fb82ec0892d3_27","name":"af85","type":"P","href":null,"layout":null,"metadata":null,"text":"I will discuss the different types of dialog management and how they handle these principles.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_28":{"id":"fb82ec0892d3_28","name":"1684","type":"H4","href":null,"layout":null,"metadata":null,"text":"Finite state machine","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_29":{"id":"fb82ec0892d3_29","name":"f61a","type":"P","href":null,"layout":null,"metadata":null,"text":"The powers of a Finite State Machine are quite extensive. Most conversations can be implemented by a FSM. They are especially good when the number of things a user can say are limited. Most tools for building a conversational bot will also provide a tool to make a decision diagram. So most bots will have a FSM underneath their hood.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_30":{"id":"fb82ec0892d3_30","name":"82d4","type":"P","href":null,"layout":null,"metadata":null,"text":"A network with distributed terminals sometime can be modelled as a finite state machine with several ports. We define in the following the concept of multi-port finite state machines, which is a generalisation of finite state machines with two ports shows in following figure,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_31":{"id":"fb82ec0892d3_31","name":"c333","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*FiqG-5MYxAzDonYlE0sezQ.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_31.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*FiqG-5MYxAzDonYlE0sezQ.png":{"id":"1*FiqG-5MYxAzDonYlE0sezQ.png","originalHeight":639,"originalWidth":889,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_31.markups.0":{"type":"A","start":0,"end":6,"href":"http:\u002F\u002Fciteseerx.ist.psu.edu\u002Fviewdoc\u002Fdownload;jsessionid=C2860BE9BDEE552C956B9FA8CA639335?doi=10.1.1.131.4341&rep=rep1&type=pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_32":{"id":"fb82ec0892d3_32","name":"ca28","type":"H4","href":null,"layout":null,"metadata":null,"text":"Switch statement","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_33":{"id":"fb82ec0892d3_33","name":"3684","type":"P","href":null,"layout":null,"metadata":null,"text":"The most basic type of dialog management is a large switch statement. Every intent triggers a different response. E.g. “Hallo” → “Hi!”, “What’s your name?” → “My name is chatbot”, “What does NLU mean?” → “Natural Language Understanding”, “How are you?” → “I’m doing great!”, etc….","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_33.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_33.markups.0":{"type":"STRONG","start":76,"end":83,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_34":{"id":"fb82ec0892d3_34","name":"5f3c","type":"H4","href":null,"layout":null,"metadata":null,"text":"Goal based","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_35":{"id":"fb82ec0892d3_35","name":"76f8","type":"P","href":null,"layout":null,"metadata":null,"text":"In a complex conversation you cannot think about dialogs as a set of states because the number of states can quickly become unmanageable. So you need to approach conversations differently. A popular way of thinking about them is thinking about them in terms of goals.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_36":{"id":"fb82ec0892d3_36","name":"1a35","type":"P","href":null,"layout":null,"metadata":null,"text":"Say that your user ask for the location of a restaurant without giving it’s name.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_37":{"id":"fb82ec0892d3_37","name":"5126","type":"P","href":null,"layout":null,"metadata":null,"text":"i. Your system will receive a “looking_for_restaurant”-intent and start a new goal “finding_restaurant”.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_38":{"id":"fb82ec0892d3_38","name":"873b","type":"P","href":null,"layout":null,"metadata":null,"text":"ii. It will notice that to finish this goal it needs to know the name of the restaurant. It therefore will ask the user for the name.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_39":{"id":"fb82ec0892d3_39","name":"865a","type":"P","href":null,"layout":null,"metadata":null,"text":"iii. When the user answers it will first analyze this response to see if it contains the name of the restaurant. If it does, it will save the name in its context.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_40":{"id":"fb82ec0892d3_40","name":"21da","type":"P","href":null,"layout":null,"metadata":null,"text":"iv. Finally the system will see if it now can finish the “finding_restaurant”-goal. Since the name of the restaurant is now known, it can lookup the restaurant’s location and tell it to the user.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_41":{"id":"fb82ec0892d3_41","name":"f93b","type":"P","href":null,"layout":null,"metadata":null,"text":"This type of dialog management works based on behaviours instead of states. It’s easier to manage different ways of asking the same question, context switching or making decisions based on what you know about the user.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_42":{"id":"fb82ec0892d3_42","name":"05e5","type":"P","href":null,"layout":null,"metadata":null,"text":"Belief based","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_42.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_42.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_43":{"id":"fb82ec0892d3_43","name":"7b45","type":"P","href":null,"layout":null,"metadata":null,"text":"Most NLU will classify intents and entities with a certain degree of uncertainty. This means that dialog manager can only assume what the user said and actually can’t work with discrete rules but needs to work with beliefs.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_44":{"id":"fb82ec0892d3_44","name":"c95f","type":"P","href":null,"layout":null,"metadata":null,"text":"3. Types of Conversational AI","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_44.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_44.markups.0":{"type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_45":{"id":"fb82ec0892d3_45","name":"c0f2","type":"P","href":null,"layout":null,"metadata":null,"text":"Rule Based Chatbot","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_45.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_45.markups.0":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_46":{"id":"fb82ec0892d3_46","name":"9e84","type":"P","href":null,"layout":null,"metadata":null,"text":"In a rule-based approach, a bot answers questions based on some rules on which it is trained on. The rules defined can be very simple to very complex. The creation of these bots are relatively straightforward using some rule-based approach, but the bot is not efficient in answering questions, whose pattern does not match with the rules on which the bot is trained. However, these systems aren’t able to respond to input patterns or keywords that don’t match existing rules. One of such languages is AIML (Artificial Intelligence Markup Language): The AIML language´s purpose is to make the task of dialog modeling easy, according to the stimulus-response approach. Moreover, it is a XML-based markup language and it is a tag based. Tags are identifiers that are responsible to make code snippets and insert commands in the chatterbot. AIML defines a data object class called AIML objects, which is responsible for modelling patterns of conversation.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_47":{"id":"fb82ec0892d3_47","name":"ebea","type":"P","href":null,"layout":null,"metadata":null,"text":"Example of AIML Code,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_48":{"id":"fb82ec0892d3_48","name":"e67c","type":"P","href":null,"layout":null,"metadata":null,"text":"Basic Tags:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_48.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_48.markups.0":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_49":{"id":"fb82ec0892d3_49","name":"840e","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u003Caiml\u003E: Defines the beginning and end of an AIML document","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_49.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_49.markups.0":{"type":"STRONG","start":0,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_50":{"id":"fb82ec0892d3_50","name":"78d8","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u003Ccategory\u003E: Defines the knowledge in a knowledge base.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_50.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_50.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_51":{"id":"fb82ec0892d3_51","name":"76e2","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u003Cpattern\u003E: Defines the pattern to match what a user may input.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_51.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_51.markups.0":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_52":{"id":"fb82ec0892d3_52","name":"a176","type":"OLI","href":null,"layout":null,"metadata":null,"text":"\u003Ctemplate\u003E: Defines the response of an Alicebot to user’s input.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_52.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_52.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_53":{"id":"fb82ec0892d3_53","name":"4847","type":"PRE","href":null,"layout":null,"metadata":null,"text":"\u003Caiml version=”1.0.1\" encoding=”UTF-8\"?\u003E\n\u003Ccategory\u003E\n      \u003Cpattern\u003E HELLO BOT \u003C\u002Fpattern\u003E\n      \u003Ctemplate\u003E\n      Hello my new friend!\n     \u003C\u002Ftemplate\u003E\n\u003C\u002Fcategory\u003E\n\u003C\u002Faiml\u003E","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_54":{"id":"fb82ec0892d3_54","name":"4bee","type":"P","href":null,"layout":null,"metadata":null,"text":"The following figure is the Decision tree of rule based conversational AI,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_55":{"id":"fb82ec0892d3_55","name":"7b70","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*axxF04wGGvg1aA1fLlI7Mw.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_55.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*axxF04wGGvg1aA1fLlI7Mw.jpeg":{"id":"1*axxF04wGGvg1aA1fLlI7Mw.jpeg","originalHeight":391,"originalWidth":580,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_55.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fwww.topbots.com\u002Fbuilding-conversational-ai\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_56":{"id":"fb82ec0892d3_56","name":"77e2","type":"P","href":null,"layout":null,"metadata":null,"text":"Retrieval Based Conversational AI","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_56.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_56.markups.0":{"type":"STRONG","start":0,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_57":{"id":"fb82ec0892d3_57","name":"437e","type":"P","href":null,"layout":null,"metadata":null,"text":"When given user input, the system uses heuristics to locate the best response from its database of pre-defined responses. Dialogue selection is essentially a prediction problem, and using heuristics to identify the most appropriate response template may involve simple algorithms like keywords matching or it may require more complex processing with machine learning or deep learning. Regardless of the heuristic used, these systems only regurgitate pre-defined responses and do not generate new output.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_58":{"id":"fb82ec0892d3_58","name":"2912","type":"P","href":null,"layout":null,"metadata":null,"text":"With massive data available, it is intuitive to build a retrieval based conversational system as information retrieval techniques are developing fast. Given a user input utterance as the query, the system searches for candidate responses by matching metrics. The core of retrieval based conversational systems is formulated as a matching problem between the query utterance and the candidate responses. A typical way for matching is to measure the inner-product of two representing feature vectors for queries and candidate responses in a transformed Hilbert space. The modelling effort boils down to finding the mapping from the original inputs to the feature vectors , which is known as representation learning.There is two-step retrieval technique to find appropriate responses from the massive data repository. The retrieval process consists of a fast ranking by standard TF-IDF measurement and the re-ranking process using conversation-oriented features designed with human expertise. The systems to select the most suitable response to the query from the question-answer pairs using a statistical language model as cross-lingual information retrieval. These methods are based on shallow representations, which basically utilises one-hot representation of words. Most strong retrieval systems learn representations with deep neural networks (DNNs). DNNs are highly automated learning machines; they can extract underlying abstract features of data automatically by exploring multiple layers of non-linear transformation. Prevailing DNNs for sentence level modelling include convolution neural networks (C-NNs) and recurrent neural networks (RNNs). A series of matching methods can be applied to short-text conversations for retrieval-based systems. Basically, these methods model sentences using convolutional or recurrent networks to construct abstractive representations. Although not all of these methods are originally designed for conversation, they are effective for short-text matching tasks and are included as strong baselines for retrieval-based conversational studies.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_58.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_58.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_58.markups.0":{"type":"STRONG","start":875,"end":882,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_58.markups.1":{"type":"EM","start":1121,"end":1156,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_59":{"id":"fb82ec0892d3_59","name":"583b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*7UFopLzF-mYqm5SjKpC6ZA.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_59.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*7UFopLzF-mYqm5SjKpC6ZA.jpeg":{"id":"1*7UFopLzF-mYqm5SjKpC6ZA.jpeg","originalHeight":426,"originalWidth":1022,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_59.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1612.01627","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_60":{"id":"fb82ec0892d3_60","name":"e7c4","type":"P","href":null,"layout":null,"metadata":null,"text":"Response Selection with Topic Clues for Retrieval-based","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_60.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_60.markups.0":{"type":"STRONG","start":0,"end":55,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_61":{"id":"fb82ec0892d3_61","name":"d635","type":"P","href":null,"layout":null,"metadata":null,"text":"If we have incorporating topic information into message response matching to boost responses with rich content in retrieval-based chatbots.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_62":{"id":"fb82ec0892d3_62","name":"ba6e","type":"P","href":null,"layout":null,"metadata":null,"text":"Topic Word Generation","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_62.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_62.markups.0":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_63":{"id":"fb82ec0892d3_63","name":"411b","type":"P","href":null,"layout":null,"metadata":null,"text":"There is LDA model , which is the state-of-the-art topic model for short texts, to generate topic words for messages and responses. LDA assumes that each piece of text (a message or a response) corresponds to one topic, and each word in the text is either a background word or a topic word under the topic of the text.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_64":{"id":"fb82ec0892d3_64","name":"98a8","type":"P","href":null,"layout":null,"metadata":null,"text":"Topic-aware Convolutional Neural Tensor Network","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_64.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_64.markups.0":{"type":"STRONG","start":0,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_65":{"id":"fb82ec0892d3_65","name":"4577","type":"P","href":null,"layout":null,"metadata":null,"text":"There is a topic-aware convolutional neural tensor network (TACNTN) to leverage the topic words obtained from LDA in message-response matching.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_66":{"id":"fb82ec0892d3_66","name":"4723","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*cQCjcHZynSi86pDzlQrOlA.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_66.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*cQCjcHZynSi86pDzlQrOlA.jpeg":{"id":"1*cQCjcHZynSi86pDzlQrOlA.jpeg","originalHeight":270,"originalWidth":570,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_66.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fwww.sciencedirect.com\u002Fscience\u002Farticle\u002Fpii\u002FS0925231218309093","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_67":{"id":"fb82ec0892d3_67","name":"cd75","type":"P","href":null,"layout":null,"metadata":null,"text":"Generative Based","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_67.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_67.markups.0":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_68":{"id":"fb82ec0892d3_68","name":"d5e4","type":"P","href":null,"layout":null,"metadata":null,"text":"A generative model chatbot doesn’t use any predefined repository. This kind of chatbot is more advanced, because it learns from scratch using a process called “Deep Learning.” Generative models are typically based on Machine Translation techniques, but instead of translating from one language to another, we “translate” from an input to an output (response).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_69":{"id":"fb82ec0892d3_69","name":"419b","type":"P","href":null,"layout":null,"metadata":null,"text":"Another way to build a conversational system is to use language generation techniques.We can combine language template generation with the search-based methods. With deep learning techniques applied, generation-based systems are greatly advanced.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_70":{"id":"fb82ec0892d3_70","name":"9e37","type":"P","href":null,"layout":null,"metadata":null,"text":"We have a sequence-to-sequence (seq2seq) framework that emerged in the neural machine translation field and was successfully adapted to dialogue problems. The architecture consists of two RNNs with different sets of parameters.The approach involves two recurrent neural networks, one to encode the source sequence, called the encoder, and a second to decode the encoded source sequence into the target sequence, called the decoder.It was originally developed for machine translation problems, although it has proven successful at related sequence-to-sequence prediction problems such as text summarization and question answering.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_71":{"id":"fb82ec0892d3_71","name":"81b7","type":"P","href":null,"layout":null,"metadata":null,"text":"Encoder:The encoder simply takes the input data, and train on it then it passes the last state of its recurrent layer as an initial state to the first recurrent layer of the decoder part.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_71.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_71.markups.0":{"type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_72":{"id":"fb82ec0892d3_72","name":"3cbc","type":"H4","href":null,"layout":null,"metadata":null,"text":"Working of Encoder","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_73":{"id":"fb82ec0892d3_73","name":"5cc4","type":"P","href":null,"layout":null,"metadata":null,"text":"The encoder RNN conceives a sequence of context tokens one at a time and updates its hidden state. After processing the whole context sequence, it produces a final hidden state, which incorporates the sense of context and is used for generating the answer.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_74":{"id":"fb82ec0892d3_74","name":"39b9","type":"P","href":null,"layout":null,"metadata":null,"text":"Decoder: The decoder takes the last state of encoder’s last recurrent layer and uses it as an initial state to its first recurrent layer , the input of the decoder is the sequences that we want to get ( in our case French sentences).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_74.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_74.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_75":{"id":"fb82ec0892d3_75","name":"befc","type":"P","href":null,"layout":null,"metadata":null,"text":"How Does the Decoder Work?","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_75.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_75.markups.0":{"type":"STRONG","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_76":{"id":"fb82ec0892d3_76","name":"6447","type":"P","href":null,"layout":null,"metadata":null,"text":"The goal of the decoder is to take context representation from the encoder and generate an answer. For this purpose, a softmax layer over vocabulary is maintained in the decoder RNN. At each time step, this layer takes the decoder hidden state and outputs a probability distribution over all words in its vocabulary.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_77":{"id":"fb82ec0892d3_77","name":"0850","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*wEU6xtMZ2tZ02D2YIPOyYw.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*wEU6xtMZ2tZ02D2YIPOyYw.jpeg":{"id":"1*wEU6xtMZ2tZ02D2YIPOyYw.jpeg","originalHeight":372,"originalWidth":948,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_78":{"id":"fb82ec0892d3_78","name":"3b6e","type":"P","href":null,"layout":null,"metadata":null,"text":"Ensemble of Retrieval- and Generation-Based Dialog Systems","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_78.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_78.markups.0":{"type":"STRONG","start":0,"end":58,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_79":{"id":"fb82ec0892d3_79","name":"6b3a","type":"P","href":null,"layout":null,"metadata":null,"text":"Typically, a recurrent neural network (RNN) captures the query’s semantics with one or a few distributed, real-valued vectors (also known as embedding); another RNN decodes the query embedding to a reply. Deep neural networks allow complicated interaction by multiple non-linear transformations; RNNs are further suitable for modelling time-series data (e.g., a sequence of words) especially when enhanced with long short term memory (LSTM) or gated recurrent units (GRUs). Despite these, RNN also has its own weakness when applied to dialog systems: the generated sentence tends to be short, universal, and meaningless, for example, “I don’t know” or “something” . This is probably because chatbot-like dialogs are highly diversified and a query may not convey sufficient information for the reply. Even though such universal utterances may be suited in certain dialog context, they make users feel boring and lose interest, and thus are not desirable in real applications.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_80":{"id":"fb82ec0892d3_80","name":"66e4","type":"P","href":null,"layout":null,"metadata":null,"text":"In ensemble of retrieval and generative dialog systems. Given a user issued query, we first obtain a candidate reply by information retrieval from a large database. The query, along with the candidate reply, is then fed to an utterance generator based on the “bi-sequence to sequence” (biseq2seq) model. Such sequence generator takes into consideration the information contained in not only the query but also the retrieved reply; hence, it alleviates the low-substance problem and can synthesize replies that are more meaningful. After that we use the scorer in the retrieval system again for post-reranking. This step can filter out less relevant retrieved replies or meaningless generated ones. The higher ranked candidate (either retrieved or generated) is returned to the user as the reply. Basically, the retrieval and generative systems are integrated by two mechanisms:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_81":{"id":"fb82ec0892d3_81","name":"e6f8","type":"P","href":null,"layout":null,"metadata":null,"text":"(1) The retrieved candidate is fed to the sequence generator to mitigate the “low-substance” problem; (2) The post-reranker can make better use of both the retrieved candidate and the generated utterance.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_82":{"id":"fb82ec0892d3_82","name":"4052","type":"P","href":null,"layout":null,"metadata":null,"text":"The following Figure depicts the overall framework of ensemble of retrieval and generative dialog systems.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_83":{"id":"fb82ec0892d3_83","name":"8772","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*esFyddkRdqjam4ETSgjDYQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_83.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*esFyddkRdqjam4ETSgjDYQ.jpeg":{"id":"1*esFyddkRdqjam4ETSgjDYQ.jpeg","originalHeight":269,"originalWidth":883,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_83.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1610.07149","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_84":{"id":"fb82ec0892d3_84","name":"f2c0","type":"P","href":null,"layout":null,"metadata":null,"text":"AIML Knowledge base (KB) Conversational AI","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_84.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_84.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_84.markups.0":{"type":"STRONG","start":0,"end":4,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_84.markups.1":{"type":"STRONG","start":5,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_85":{"id":"fb82ec0892d3_85","name":"4a59","type":"P","href":null,"layout":null,"metadata":null,"text":"A KB in this form is often called a Knowledge Graph (KG) due to its graphical representation, i.e., the entities are nodes and the relations the directed edges that link the nodes.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_86":{"id":"fb82ec0892d3_86","name":"8d63","type":"P","href":null,"layout":null,"metadata":null,"text":"The basic concept of Knowledge base is shown as following figure,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_87":{"id":"fb82ec0892d3_87","name":"994d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*XbtDQ5nGEKF1I739ovs7rQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*XbtDQ5nGEKF1I739ovs7rQ.jpeg":{"id":"1*XbtDQ5nGEKF1I739ovs7rQ.jpeg","originalHeight":400,"originalWidth":827,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_88":{"id":"fb82ec0892d3_88","name":"f167","type":"P","href":null,"layout":null,"metadata":null,"text":"Most state-of-the-art symbolic approaches to KB-QA are based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a KB query. The answers to the question can then be obtained by finding a set of paths in the KB that match the query and retrieving the end nodes of these paths. Knowledge based systems have been helping humans to solve problems which are intellectually difficult, but easy for machines. These problems typically are easily represented with a set of formal rules.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_89":{"id":"fb82ec0892d3_89","name":"c124","type":"P","href":null,"layout":null,"metadata":null,"text":"The following figure we have KB representation graph centred on the question Q1. In the graph nodes are patterns (P) and templates (T), and edges are P-T associations and T-P semantic recursions.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_90":{"id":"fb82ec0892d3_90","name":"f095","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ElhTTEFhEwiABXFL2OK3jg.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_90.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ElhTTEFhEwiABXFL2OK3jg.png":{"id":"1*ElhTTEFhEwiABXFL2OK3jg.png","originalHeight":573,"originalWidth":725,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_90.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F298129630_Building_an_AIML_chatter_bot_knowledge-base_starting_from_a_FAQ_and_a_glossary","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_91":{"id":"fb82ec0892d3_91","name":"cbc4","type":"P","href":null,"layout":null,"metadata":null,"text":"Knowledge bases (KB) are powerful tools that can be used to augment conversational models. Since knowledge bases usually entail some kind of domain specific information, these techniques are mainly used for task-oriented dialog systems. In a KB, information related to the task at hand can be stored, for example information about nearby restaurants or about public transportation routes. Simple dictionaries or look-up-tables can be used to match an entity with information about it. Since KBs store information discretely, their integration with neural network based encoder-decoder models is not trivial.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_92":{"id":"fb82ec0892d3_92","name":"2445","type":"P","href":null,"layout":null,"metadata":null,"text":"The following figure shown that how the KB searching happens,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_93":{"id":"fb82ec0892d3_93","name":"9402","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*jPVpqpuJr3-byFqHSAkddw.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*jPVpqpuJr3-byFqHSAkddw.png":{"id":"1*jPVpqpuJr3-byFqHSAkddw.png","originalHeight":364,"originalWidth":950,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_94":{"id":"fb82ec0892d3_94","name":"337f","type":"P","href":null,"layout":null,"metadata":null,"text":"In Restaurant finding Knowledge base mechanism example, the encoder-decoder model produces a response that also uses general tokens for locations and times, and a special placeholder token for the KB result. Finally, the general tokens are transformed back to actual words using the stored table, a KB is employed which uses these general tokens to search for a route between the two places and its output is incorporated in the response. One more similar KB augmented encoder-decoder model is used for the task of recommending restaurants. Here, besides a standard encoder RNN the source utterance is also processed with a belief tracker, implemented as a convolutional neural network (CNN). Convolutional neural networks applied to encoder-decoder models. Belief tracking is an important part of task oriented spoken dialog systems. The belief tracker network produces a query for a database containing information about restaurants. The final input to the decoder RNN is the weighted sum consisting of the last state of the decoder RNN and a categorical probability vector from the belief tracker. Then the decoder outputs a response in the same way as in the previous example, with lexicalised general tokens. These tokens are then replaced with the actual information that they point to in the KB.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_94.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_94.markups.0":{"type":"EM","start":757,"end":773,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_95":{"id":"fb82ec0892d3_95","name":"ad33","type":"P","href":null,"layout":null,"metadata":null,"text":"Self Learning: Recurrent Embedding Dialogue policy (REDP)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_95.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_95.markups.0":{"type":"STRONG","start":0,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_96":{"id":"fb82ec0892d3_96","name":"f604","type":"P","href":null,"layout":null,"metadata":null,"text":"Natural Language Processing with a Training Model to enable the bot to ‘learn’ to understand a sentence, Context to be able to perform a conversation and History to learn from previous conversations. A grand challenge in this field is to create software which is capable of holding extended conversations, carrying out tasks, keeping track of conversation history, and coherently responding to new information. The aim is to learn vector embeddings for dialogue states and system actions in a supervised setting.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_97":{"id":"fb82ec0892d3_97","name":"3b60","type":"P","href":null,"layout":null,"metadata":null,"text":"The following figure shown that how the chatbot response machine are associated with all components,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_98":{"id":"fb82ec0892d3_98","name":"e6e7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*uY-w_wt4cBSzHboZhR8j1A.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*uY-w_wt4cBSzHboZhR8j1A.jpeg":{"id":"1*uY-w_wt4cBSzHboZhR8j1A.jpeg","originalHeight":606,"originalWidth":1312,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_99":{"id":"fb82ec0892d3_99","name":"271a","type":"P","href":null,"layout":null,"metadata":null,"text":"When we ask a user “what price range are you looking for?”, they might respond with:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_100":{"id":"fb82ec0892d3_100","name":"8a16","type":"ULI","href":null,"layout":null,"metadata":null,"text":"“Why do you need to know that?” (narrow context)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_100.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_100.markups.0":{"type":"CODE","start":33,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_101":{"id":"fb82ec0892d3_101","name":"4f25","type":"ULI","href":null,"layout":null,"metadata":null,"text":"“Can you show me some restaurants yet?” (broad context)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_101.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_101.markups.0":{"type":"CODE","start":41,"end":54,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_102":{"id":"fb82ec0892d3_102","name":"c6eb","type":"ULI","href":null,"layout":null,"metadata":null,"text":"“Actually no I want Chinese food” (correction)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_102.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_102.markups.0":{"type":"CODE","start":35,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_103":{"id":"fb82ec0892d3_103","name":"b323","type":"ULI","href":null,"layout":null,"metadata":null,"text":"“I should probably cook for myself more” (chitchat)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_103.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_103.markups.0":{"type":"CODE","start":42,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_104":{"id":"fb82ec0892d3_104","name":"cf15","type":"P","href":null,"layout":null,"metadata":null,"text":"We call all of this uncooperative behaviour. There are many other ways a user might respond. Here’s an example conversation:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_104.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_104.markups.0":{"type":"EM","start":20,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_105":{"id":"fb82ec0892d3_105","name":"b4fa","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*JywMGefOdEiZKGAqaR9tVw.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_105.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*JywMGefOdEiZKGAqaR9tVw.jpeg":{"id":"1*JywMGefOdEiZKGAqaR9tVw.jpeg","originalHeight":621,"originalWidth":747,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_105.markups.0":{"type":"A","start":1,"end":2,"href":"https:\u002F\u002Fmedium.com\u002Frasa-blog\u002Fattention-dialogue-and-learning-reusable-patterns-5d6bd18ef9f0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_106":{"id":"fb82ec0892d3_106","name":"c80c","type":"P","href":null,"layout":null,"metadata":null,"text":"At inference time, the current state of the dialogue is compared to all possible system actions, and the one with the highest cosine similarity is selected.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_107":{"id":"fb82ec0892d3_107","name":"f02f","type":"P","href":null,"layout":null,"metadata":null,"text":"REDP, new dialogue policy, has two benefits: (1) it’s much better at learning how to deal with uncooperative behaviour, and (2) it can re-use this information when learning a new task.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_107.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_107.markups.0":{"type":"STRONG","start":0,"end":4,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_108":{"id":"fb82ec0892d3_108","name":"3c5f","type":"P","href":null,"layout":null,"metadata":null,"text":"It uses the same idea to deal with uncooperative users. After responding correctly to a user’s uncooperative message, the assistant should return to the original task and be able to continue as though the deviation never happened. REDP achieves this by adding an attention mechanism to the neural network, allowing it to ignore the irrelevant parts of the dialogue history. The image below is an illustration of the REDP architecture (a full description is in the paper). The attention mechanism is based on a modified version of the Neural Turing Machine, and instead of a classifier we use an embed-and-rank approach.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_108.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_108.markups.0":{"type":"EM","start":191,"end":229,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_109":{"id":"fb82ec0892d3_109","name":"d1c1","type":"P","href":null,"layout":null,"metadata":null,"text":"The following figure shown the working of REDP,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_110":{"id":"fb82ec0892d3_110","name":"7003","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*iQ928te8htdI4e4lCi0FMQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*iQ928te8htdI4e4lCi0FMQ.jpeg":{"id":"1*iQ928te8htdI4e4lCi0FMQ.jpeg","originalHeight":433,"originalWidth":610,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_111":{"id":"fb82ec0892d3_111","name":"e83f","type":"P","href":null,"layout":null,"metadata":null,"text":"Attention has been used in dialogue research before, but the embedding policy is the first model which uses attention specifically for dealing with uncooperative behaviour, and also to reuse that knowledge in a different task. One advantage of this approach is that target labels can be represented as a bag of multiple features, allowing us to represent system actions as a composition of features. In general, the features describing a particular action can come from a number of sources, including the class hierarchy, the name of the action, and even features derived from the code itself (such as which functions are called). Whatever the source of the features, similar actions should have more features in common than dissimilar actions, and ideally reflect the structure of the domains. In our experiments we only derive features from the action name, either taking the whole name as a single feature, or splitting the name into tokens and representing it as a bag of words.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_112":{"id":"fb82ec0892d3_112","name":"fd20","type":"P","href":null,"layout":null,"metadata":null,"text":"4. Intent Identification and Information Extraction","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_112.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_112.markups.0":{"type":"STRONG","start":0,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_113":{"id":"fb82ec0892d3_113","name":"951d","type":"P","href":null,"layout":null,"metadata":null,"text":"The machine algorithm for Intent Identification can be either supervised or unsupervised. If we implement the supervised approach, we need to manually give labels to hundreds of data for training purpose which going to be tiring and boring, but if we implement the unsupervised one, there were several critical knowledge gaps that we can’t cover in just 3 weeks especially regarding the design of training process. Therefore, even though we need to manually give labels to our data, we chose to go with the supervised one. This picture below illustrate how text classifier using supervised ML works:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_114":{"id":"fb82ec0892d3_114","name":"2606","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*1Aqe_iigspkY5wJ5HEX5EA.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*1Aqe_iigspkY5wJ5HEX5EA.png":{"id":"1*1Aqe_iigspkY5wJ5HEX5EA.png","originalHeight":363,"originalWidth":800,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_115":{"id":"fb82ec0892d3_115","name":"aed8","type":"P","href":null,"layout":null,"metadata":null,"text":"At this point, we have several machine learning algorithms that we could choose to implement supervised learning which is Naive Bayesian, LDA, SVM and Neural Network. But before we choose the algorithm, we need to find a method to translate words into an array (word embedding) since all algorithms that I mention previously need input in form of array or at least numbers. There are 2 options that we had to do that, by using one hot encoded bag of words (bow) or word2vec (CBOW). If we had more times, we definitely would choose word2vec to embed the input since the size of array would be significantly smaller compared to BOW, but we had limited time and to implement word2vec we need to use Java (deeplearn4j) or Python(gensim) which no one between us had any experience making an API using these languages. Actually, it is possible for us to create the classifier by using Python but the problem will occur in the process of making an API out of it, especially in the deployment process. To deploy Python in the live server, there are several configurations that need to be done and we don’t have the courage to play around with our company server since everyone else is also using it for other projects. So for the sake of familiarity, we decide to use BOW which we manage to find a node package to implement it called mimir.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_115.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_115.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_115.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_115.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_115.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_115.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_115.markups.0":{"type":"A","start":443,"end":461,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBag-of-words_model","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_115.markups.1":{"type":"A","start":465,"end":473,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FWord2vec","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_115.markups.2":{"type":"A","start":702,"end":713,"href":"https:\u002F\u002Fdeeplearning4j.org\u002Fword2vec.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_115.markups.3":{"type":"A","start":725,"end":731,"href":"https:\u002F\u002Fradimrehurek.com\u002Fgensim\u002Fmodels\u002Fword2vec.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_115.markups.4":{"type":"A","start":1049,"end":1063,"href":"https:\u002F\u002Fwww.analyticsvidhya.com\u002Fblog\u002F2017\u002F09\u002Fmachine-learning-models-as-apis-using-flask\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_115.markups.5":{"type":"A","start":1326,"end":1331,"href":"https:\u002F\u002Fwww.npmjs.com\u002Fpackage\u002Fmimir","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_116":{"id":"fb82ec0892d3_116","name":"c3cc","type":"P","href":null,"layout":null,"metadata":null,"text":"In Deep learning, Intent Identification works as a three layers of processing: encoder network, intention network, and decoder network. The encoder network has inputs from the current source side input. Because the source side in the current turn is also dependent on the previous turn, the source side encoder network is linked with the output from the previous target side. The encoder network creates a representation of the source side in the current turn. The intention network is dependent on its past state, so that it memories the history of intentions. It therefore is a recurrent network, taking a representation of the source side in the current turn and updating its hidden state. The decoder is a recurrent network for language modelling that outputs symbol at each time. This output is dependent on the current intention from the intention network. It also pays attention to particular words in the source side. In NLU, the functions \u002F dialogue acts are often domain specific. In other words, instead of asking whether the function of the user’s utterance is a question or answer, we ask whether the function is to, for example, find flights or cancel a reservation in a flight reservation program. Domain-specific dialogue acts are called intents. Intent identifying has been most prominently used by call centre bots, which ask the user “how can I help you?” and subsequently use intent identification to re-direct the user to one of N pre-defined re-direction options. Many of the same machine learning algorithms used for DA classification are used for intent identification.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_117":{"id":"fb82ec0892d3_117","name":"0a8d","type":"P","href":null,"layout":null,"metadata":null,"text":"Regarding Information Extraction, The primary responsibility of the NLU is not just to understand phrase function, but to understand the meaning of the text itself. To extract meaning from text, we convert unstructured text — text written into a text-only chatbot — into structured grammatical data objects, which will be further processed by the Dialogue Manager. The first step in this process is breaking down a sentence into tokens that represent each of its component parts: words, punctuation marks, numbers, etc. Tokenization is difficult because of the frequency of ambiguous or malformed inputs including: (i) phrases , (ii) contractions , abbreviations , and periods. These tokens can be analyzed using a number of techniques, described below, to create a number of different data structures that be processed by the dialogue manager.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_117.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_117.markups.0":{"type":"STRONG","start":0,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_118":{"id":"fb82ec0892d3_118","name":"4195","type":"P","href":null,"layout":null,"metadata":null,"text":"There are few approach which can be use for Information retrieval as below,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_119":{"id":"fb82ec0892d3_119","name":"bc52","type":"P","href":null,"layout":null,"metadata":null,"text":"Bag of Words: We ignore sentence structure, order, and syntax, and count the number of occurrences of each word. We use this to form a vector space model, in which stop words are removed, and morphological variants go through a process call lemmatization and are stored as instances of the basic lemma . In the dialogue manager phase, assuming a rule-based bot, these resulting words will be matched against documents stored in the bot’s knowledge database to find the documents with inputs containing similar keywords. The bag of words approach is simple because it does not require knowledge of syntax, but, for this same reason, is not precise enough to solve more complex problems.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_119.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_119.markups.0":{"type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_120":{"id":"fb82ec0892d3_120","name":"f86e","type":"P","href":null,"layout":null,"metadata":null,"text":"Latent Semantic Analysis : This approach is similar to the bag of words. Meanings \u002F concepts, however, not words, are the basic unit of comparison parsed from a given sentence or utterance. Second, groups of words that co-occur frequently are grouped together. In LSA, we create a matrix where each row represents a unique word, each column represents a document, and the value of each cell is the frequency of the word in the document. We compute the distance between the vector representing each utterance and document, using singular value decomposition to reduce the dimensionality of the matrix, and determine the closest document.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_120.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_120.markups.0":{"type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_121":{"id":"fb82ec0892d3_121","name":"8263","type":"P","href":null,"layout":null,"metadata":null,"text":"Regular Expressions: Sentences \u002F utterances can be treated as regular expressions, and can be pattern matched against the documents in the bot’s knowledge database. For example, imagine that one of the documents in the bot’s knowledge database handles the case where the user inputs the phrase: “my name is *”. “*” is the wildcard character, and indicates that this regular expression should be triggered whenever the bot hears the phrase “my name is” followed by anything. If the user says “my name is Jack”, this phrase will be parsed into a number of regular expressions, including “my name is *” and will trigger the retrieval of that document.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_121.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_121.markups.0":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_122":{"id":"fb82ec0892d3_122","name":"cf33","type":"P","href":null,"layout":null,"metadata":null,"text":"Part of Speech (POS) Tagging: POS tagging labels each word in the input string with its part of speech (e.g. noun, verb, adjective, etc.). These labels can be rule-based (a manually-created set of rules is created to specify part of speech for ambiguous words given their context). They can also be created using stochastic models which train on sentences labeled with correct POS. In the dialogue manager, POS can be used to store relevant information in the dialogue history. POS is also used in response generation to indicate the POS object type of the desired response.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_122.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_122.markups.0":{"type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_123":{"id":"fb82ec0892d3_123","name":"df87","type":"P","href":null,"layout":null,"metadata":null,"text":"Named\u002FRelation Entity Recognition: In named entity recognition (NER), the names of people, places, groups, and locations are extracted and labeled accordingly. NER-name pairs can be stored by the dialogue manager in the dialogue history to keep track of the context of the bot’s conversation. Relation extraction goes one step further to identity relations (e.g. “who did what to whom”) and label each word in these phrases.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_123.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_123.markups.0":{"type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_124":{"id":"fb82ec0892d3_124","name":"358b","type":"P","href":null,"layout":null,"metadata":null,"text":"Semantic Role Labelling: The arguments of a verb are labelled based on their semantic role (e.g. subject, theme, etc.). In this process, the predicate is labelled first followed by its arguments. Prominent classifiers for semantic role labelling have been trained on FrameNet and PropBank, databases with sentences already labelled with their semantic roles. These semantic role-word pairs can be stored by the dialogue manager in the dialogue history to keep track of context.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_124.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_124.markups.0":{"type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_125":{"id":"fb82ec0892d3_125","name":"1829","type":"P","href":null,"layout":null,"metadata":null,"text":"Creation of Grammatical Data Structures: Sentences and utterances can be stored in a structured way in grammar formalism such as context-free grammars (CFGs) and dependency grammars (DGs). Context-free grammars are tree-like data structures that represent sentences as containing noun phrases and verb phrases, each of which contain nouns, verbs, subjects, and other grammatical constructs. Dependency grammars, by contrast, focus on the relationships between words.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_125.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_125.markups.0":{"type":"STRONG","start":0,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_126":{"id":"fb82ec0892d3_126","name":"5c7a","type":"P","href":null,"layout":null,"metadata":null,"text":"Statistical Methods for Information Extraction","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_126.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_126.markups.0":{"type":"STRONG","start":0,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_127":{"id":"fb82ec0892d3_127","name":"1450","type":"P","href":null,"layout":null,"metadata":null,"text":"Hidden Vector State (HVS) Model: The goal of the statistical hidden vector state models is to automatically produce some accurate structured meaning. Consider an example as “I want to return to Dallas on Thursday.” The parse tree below represents one way of representing the structured meaning of the sentence. SS represents the initial node send_start, and SE represents the end node send_end. We view each leaf node as a vector state, described by its parent nodes: the vector state of Dallas is [CITY, TOLOC, RETURN, SS].","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_127.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_127.markups.0":{"type":"STRONG","start":0,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_128":{"id":"fb82ec0892d3_128","name":"5772","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*FCgLUOCm3XLCLljQhtwIdg.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_128.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*FCgLUOCm3XLCLljQhtwIdg.png":{"id":"1*FCgLUOCm3XLCLljQhtwIdg.png","originalHeight":412,"originalWidth":949,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_128.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fstatic1.squarespace.com\u002Fstatic\u002F569293741c1210fdda37b429\u002Ft\u002F59160b6bff7c50104e601a85\u002F1494616940469\u002FCHATBOT_thesis_final.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_129":{"id":"fb82ec0892d3_129","name":"eebe","type":"P","href":null,"layout":null,"metadata":null,"text":"The whole parse-tree can then be thought of a sequence of vector states, represented by the sequence of squares above. If each vector state is thought of as a hidden variable, then the sequence of vector states (e.g. squares above) can be thought of as a Hidden Markov Model: we start at SS, and have certain probabilities of reaching a number of possible hidden states as the next state. Each vector state can be thought of as a “push-down automaton” or stack. Support Vector Machine (SVM) Model: Support Vector Machines are a supervised machine learning tool. Given a set of labeled training data, the algorithm generates the optimal hyperplane that divides the sample into their proper labels. Traditionally, SVMs are thought of as solving binary classification problems, however multiple hyperplanes can be used to divide the data into more than two label categories. The optimal hyperplane is defined as the hyperplane that creates the maximum margin, or distance, between different-labeled data point sets.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_129.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_129.markups.0":{"type":"STRONG","start":462,"end":497,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_130":{"id":"fb82ec0892d3_130","name":"a4d9","type":"P","href":null,"layout":null,"metadata":null,"text":"Conditional Random Field Models: CRFs are log-linear statistical models often applied for structured prediction. Unlike the average classifier, which predicts a label for a single object and ignores context, CRF’s take into account previous features of the input sequence through the use of conditional probabilities. A number of different features can be used to train the model, including lexical information, prefixes and suffixes, capitalization and other features.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_130.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_130.markups.0":{"type":"STRONG","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_131":{"id":"fb82ec0892d3_131","name":"f3b9","type":"P","href":null,"layout":null,"metadata":null,"text":"Deep Learning: The most recent advancement in the use of statistical models for concept structure prediction is deep learning for natural language processing, or deep NLP. Deep learning neural network architectures differ from traditional neural networks in that they use more hidden layers, with each layer handling increasingly complex features. As a result, the networks can learn from patterns and unlabelled data, and deep learning can be used for unsupervised learning. Deep learning methods have been used to generate POS tags of sentences (chunk text into noun phrases, verb phrases, etc.) and for named-entity recognition and semantic role labelling.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_131.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_131.markups.0":{"type":"STRONG","start":0,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_132":{"id":"fb82ec0892d3_132","name":"ee18","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture illustrate the working of Deep learning based Statistical Model,","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_132.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_132.markups.0":{"type":"STRONG","start":64,"end":82,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_133":{"id":"fb82ec0892d3_133","name":"ebe5","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*KbdHfa8dGji_Kqhh0sd45Q.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_133.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*KbdHfa8dGji_Kqhh0sd45Q.png":{"id":"1*KbdHfa8dGji_Kqhh0sd45Q.png","originalHeight":428,"originalWidth":851,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_133.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fdl.acm.org\u002Fcitation.cfm?id=3210183","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_134":{"id":"fb82ec0892d3_134","name":"482a","type":"P","href":null,"layout":null,"metadata":null,"text":"5. Self-learning Based on Sentiment Analysis","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_134.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_134.markups.0":{"type":"STRONG","start":0,"end":44,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_135":{"id":"fb82ec0892d3_135","name":"92aa","type":"P","href":null,"layout":null,"metadata":null,"text":"Initially, the development of a bot was based on two fundamental components :","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_136":{"id":"fb82ec0892d3_136","name":"983e","type":"P","href":null,"layout":null,"metadata":null,"text":"Natural Language Understanding module, used by the Dialogue Manager, that processes the user input to search for keywords through which to understand the action to be taken.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_136.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_136.markups.0":{"type":"STRONG","start":0,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_137":{"id":"fb82ec0892d3_137","name":"55c0","type":"P","href":null,"layout":null,"metadata":null,"text":"Natural Language Generation module that generates answers from the information gathered by the Dialogue Manager.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_137.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_137.markups.0":{"type":"STRONG","start":0,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_138":{"id":"fb82ec0892d3_138","name":"2fb2","type":"P","href":null,"layout":null,"metadata":null,"text":"Over time, we have faced a real evolution in the development of task-oriented conversational agents because of the availability of deep learning techniques.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_139":{"id":"fb82ec0892d3_139","name":"dba7","type":"P","href":null,"layout":null,"metadata":null,"text":"This picture below illustrate the process of sentiment analysis in user generated content,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_140":{"id":"fb82ec0892d3_140","name":"d89d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*-dV66RkiDhpR-hnAeBSCNw.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_140.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-dV66RkiDhpR-hnAeBSCNw.jpeg":{"id":"1*-dV66RkiDhpR-hnAeBSCNw.jpeg","originalHeight":691,"originalWidth":648,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_140.markups.0":{"type":"A","start":0,"end":6,"href":"http:\u002F\u002Fceur-ws.org\u002FVol-2244\u002Fpaper_01.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_141":{"id":"fb82ec0892d3_141","name":"cc43","type":"P","href":null,"layout":null,"metadata":null,"text":"The training process for sentiment analysis it will provide for automatic labeling of new instances. In the sentiment analysis method each sentence is analyzed against two classification sub-systems: one for identifying the class of the answers, one for assessing the sentiment of the sentence. At the end of the processing of each sentence the learning model is updated according to the detected sentiment. This is based on a data structure formed by intents (An intent is a semantic label representing an intention of the end-user) . For each intent, there is a set of sentences that represent it. Each sentence that describes an intent contains entities (Entities are the parameters of the intent that help in defining the specific user request) that are attributes specific to the given intent.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_142":{"id":"fb82ec0892d3_142","name":"1d7b","type":"P","href":null,"layout":null,"metadata":null,"text":"An example taken from the dataset is shown below.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_143":{"id":"fb82ec0892d3_143","name":"db16","type":"P","href":null,"layout":null,"metadata":null,"text":"Take an example as, Request: ”I need days off from tomorrow to the day after tomorrow”.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_144":{"id":"fb82ec0892d3_144","name":"6d2d","type":"P","href":null,"layout":null,"metadata":null,"text":"Intent: LEAV E REQU EST","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_145":{"id":"fb82ec0892d3_145","name":"181d","type":"P","href":null,"layout":null,"metadata":null,"text":"Entities:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_146":{"id":"fb82ec0892d3_146","name":"fe23","type":"P","href":null,"layout":null,"metadata":null,"text":"– start date: tomorrow.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_147":{"id":"fb82ec0892d3_147","name":"9dc3","type":"P","href":null,"layout":null,"metadata":null,"text":"– end date: day after tomorrow.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_148":{"id":"fb82ec0892d3_148","name":"82de","type":"P","href":null,"layout":null,"metadata":null,"text":"In this example, the scenario of an employee requesting holidays is repre- sented by the LEAV E REQU EST intent and by the start date and end date entities.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_149":{"id":"fb82ec0892d3_149","name":"2fdc","type":"P","href":null,"layout":null,"metadata":null,"text":"An example of how the method works in case of positive sentiment is shown below.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_150":{"id":"fb82ec0892d3_150","name":"ed57","type":"P","href":null,"layout":null,"metadata":null,"text":"example: Time off request","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_151":{"id":"fb82ec0892d3_151","name":"23f3","type":"P","href":null,"layout":null,"metadata":null,"text":"Request: ”Hi” {intent: [HELLO] detected: [Hi] sentiment: [neutral]}","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_152":{"id":"fb82ec0892d3_152","name":"3cdc","type":"P","href":null,"layout":null,"metadata":null,"text":"Bot: ”Hi, Dave”","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_153":{"id":"fb82ec0892d3_153","name":"abe9","type":"P","href":null,"layout":null,"metadata":null,"text":"Request: ”I’m stuck in traffic, I’ll be there soon” {intent: [TIMEOFF REQUEST]","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_154":{"id":"fb82ec0892d3_154","name":"f8dc","type":"P","href":null,"layout":null,"metadata":null,"text":"detected: [there,soon] new words: [stuck,traffic] sentiment: [neutral]}","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_154.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_154.markups.0":{"type":"STRONG","start":50,"end":71,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_155":{"id":"fb82ec0892d3_155","name":"81f9","type":"P","href":null,"layout":null,"metadata":null,"text":"Bot: ”Ok, do you want to create a time off request?”","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_156":{"id":"fb82ec0892d3_156","name":"4580","type":"P","href":null,"layout":null,"metadata":null,"text":"Request: ”Yes, thank you!” {intent: [CONFIRM] detected: [Yes, thank, you] sentiment: [positive]}","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_156.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_156.markups.0":{"type":"STRONG","start":74,"end":96,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_157":{"id":"fb82ec0892d3_157","name":"b58a","type":"P","href":null,"layout":null,"metadata":null,"text":"In this example the agent detects the correct intent by the words there and soon and enriches the dictionary with stuck and traffic. If these words are often used for a time off request, they will become characteristic for this intent. (i.g. Dave in the future he will can write ”I’m stuck in traffic” or ”There is traffic” to request a time off).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_158":{"id":"fb82ec0892d3_158","name":"0743","type":"P","href":null,"layout":null,"metadata":null,"text":"An example of how the proposed method works in case of negative sentiment is as below.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_159":{"id":"fb82ec0892d3_159","name":"aa0d","type":"P","href":null,"layout":null,"metadata":null,"text":"example: Time off request","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_160":{"id":"fb82ec0892d3_160","name":"4613","type":"P","href":null,"layout":null,"metadata":null,"text":"Request: ”Hi” {intent: [HELLO] detected: [Hi] sentiment: [neutral]}","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_160.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_160.markups.0":{"type":"STRONG","start":46,"end":67,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_161":{"id":"fb82ec0892d3_161","name":"ce72","type":"P","href":null,"layout":null,"metadata":null,"text":"Bot: ”Hi, Dave”","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_162":{"id":"fb82ec0892d3_162","name":"008b","type":"P","href":null,"layout":null,"metadata":null,"text":"Request: ”Tomorrow I’ll be busy” {intent: [LEAVE REQUEST] detected: [to-","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_163":{"id":"fb82ec0892d3_163","name":"f05d","type":"P","href":null,"layout":null,"metadata":null,"text":"morrow,busy] sentiment: [neutral]}","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_163.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_163.markups.0":{"type":"STRONG","start":13,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_164":{"id":"fb82ec0892d3_164","name":"3e9a","type":"P","href":null,"layout":null,"metadata":null,"text":"Bot: ”Ok, do you want to create a leave request?”","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_165":{"id":"fb82ec0892d3_165","name":"f66a","type":"P","href":null,"layout":null,"metadata":null,"text":"Request: ”No, that’s not what I want!” {intent: [NOT CONFIRM] detected: [No,that’s not, what, I, want] sentiment: [negative]}","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_165.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_165.markups.0":{"type":"STRONG","start":103,"end":124,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_166":{"id":"fb82ec0892d3_166","name":"d699","type":"P","href":null,"layout":null,"metadata":null,"text":"In this example the agent detects the incorrect intent by the words tomorrow and busy. In the future, if the bot will always receive a negative response to the request that he proposes then the words found will no longer be characteristics of the intent found and can be totally eliminated.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_167":{"id":"fb82ec0892d3_167","name":"bc21","type":"P","href":null,"layout":null,"metadata":null,"text":"We can have also defined a dictionary that the bot uses to translate the type of some words. For example, the terms ”tomorrow” and ”day after tomorrow” are assigned to the type date.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_168":{"id":"fb82ec0892d3_168","name":"2023","type":"P","href":null,"layout":null,"metadata":null,"text":"Classification of intents","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_168.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_168.markups.0":{"type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_169":{"id":"fb82ec0892d3_169","name":"99e4","type":"P","href":null,"layout":null,"metadata":null,"text":"The classification problem considered in our method is determining the intent and the entities associated to a given user sentence. User sentences are represented with bag of words, without considering the order of the words. To improve classification accuracy, we also use a vocabulary of n-words with an N-gram model . The classification algorithm is based on Naive Bayes Text Classifier , a statistical technique able to estimate the probability of an element belonging to a certain class. The Naive Bayes technique estimates the conditional probabilities of each word given the classification category by associating every word that convey the same meaning in the intents, a numerical value that we will consider as a weight. The words that characterize an intent will have greater weight because they will only be found within that intent, so their occurrence is limited compared to non-characterizing words that we find in numerous intents.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_170":{"id":"fb82ec0892d3_170","name":"d7bd","type":"P","href":null,"layout":null,"metadata":null,"text":"Take an example as, Given an intent Leave representing requests from a user regarding leaves, we would like sentences such as ”I want go to holidays”, ”I’m tired, I need to rests”, ”I want holidays for this month”, etc. to be classified as Leave. The main idea developed is to provide the agent with the ability to automatically collect feedback about its answers in order to improve its knowledge base. To this end, we experimented the use of sentiment analysis. To detect the sentiment from user sentences, we have defined another classification problem from user input to three classes: Positive, Negative and Neutral and use again a Naive Bayes approach to train this classifier on a specific dataset. For any user’s sentence, we keep track of local and global sentiment score, local score is about the last sentence, global score is an average value across the dialogue. Furthermore, to improve the idea, we can define some particular intents that act as modifiers.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_171":{"id":"fb82ec0892d3_171","name":"07db","type":"P","href":null,"layout":null,"metadata":null,"text":"Take an example, when the user corrects the bot with phrases like ”I’m sorry I did not mean this”, this is considered as a negative feedback, while phrases containing specific thanks, such as ”Thank you! I was trying to do exactly this!” provide for positive feedback.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_172":{"id":"fb82ec0892d3_172","name":"ec93","type":"P","href":null,"layout":null,"metadata":null,"text":"6. Sequence2Sequence Model with Multihead Attention Mechanism","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_172.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_172.markups.0":{"type":"STRONG","start":0,"end":61,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_173":{"id":"fb82ec0892d3_173","name":"42fb","type":"P","href":null,"layout":null,"metadata":null,"text":"Recurrent Neural Network","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_173.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_173.markups.0":{"type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_174":{"id":"fb82ec0892d3_174","name":"ce58","type":"P","href":null,"layout":null,"metadata":null,"text":"Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many NLP tasks. The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a very bad idea. If you want to predict the next word in a sentence you better know which words came before it. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps. Here is what a typical RNN looks like:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_174.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_174.markups.0":{"type":"EM","start":421,"end":431,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_175":{"id":"fb82ec0892d3_175","name":"ffb2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*GqIz--afNilA1Pz9bAPrbQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*GqIz--afNilA1Pz9bAPrbQ.jpeg":{"id":"1*GqIz--afNilA1Pz9bAPrbQ.jpeg","originalHeight":345,"originalWidth":711,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_176":{"id":"fb82ec0892d3_176","name":"297f","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s consider the following sequence — Bangalore is the largest city of ______. It is easy to fill the blank with India. This means that there is information about the last word encoded in the previous elements of the sequence.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_176.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_176.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_176.markups.0":{"type":"EM","start":40,"end":79,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_176.markups.1":{"type":"EM","start":115,"end":120,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_177":{"id":"fb82ec0892d3_177","name":"fbf1","type":"P","href":null,"layout":null,"metadata":null,"text":"The idea behind this architecture is to exploit this sequential structure of the data. The name of this neural networks comes from the fact that they operate in a recurrent way. This means that the same operation is performed for every element of a sequence, with its output depending on the current input, and the previous operations.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_178":{"id":"fb82ec0892d3_178","name":"9f2e","type":"P","href":null,"layout":null,"metadata":null,"text":"The following picture shows the working of RNN for language modeling,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_179":{"id":"fb82ec0892d3_179","name":"e2ef","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*y-eIxknba2Hyv1wtSKKP8Q.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_179.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*y-eIxknba2Hyv1wtSKKP8Q.png":{"id":"1*y-eIxknba2Hyv1wtSKKP8Q.png","originalHeight":363,"originalWidth":953,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_179.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fblog.paperspace.com\u002Frecurrent-neural-networks-part-1-2\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_180":{"id":"fb82ec0892d3_180","name":"345a","type":"P","href":null,"layout":null,"metadata":null,"text":"Recurrent Neural Networks can be used in a variety of scenarios depending in how the inputs are fed and the outputs are interpreted. These scenarios can be divided into three main different classes:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_181":{"id":"fb82ec0892d3_181","name":"9b4d","type":"P","href":null,"layout":null,"metadata":null,"text":"Sequential input to sequential output","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_181.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_181.markups.0":{"type":"STRONG","start":0,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_182":{"id":"fb82ec0892d3_182","name":"e4ca","type":"P","href":null,"layout":null,"metadata":null,"text":"Machine translation \u002F part-of-speech tagging and language modeling tasks lie within this class.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_183":{"id":"fb82ec0892d3_183","name":"699c","type":"P","href":null,"layout":null,"metadata":null,"text":"Sequential input to single output","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_183.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_183.markups.0":{"type":"STRONG","start":0,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_184":{"id":"fb82ec0892d3_184","name":"9386","type":"P","href":null,"layout":null,"metadata":null,"text":"One task with this property is sentiment analysis, in which we fed a sentence and we want to classify it as positive, neutral or negative.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_185":{"id":"fb82ec0892d3_185","name":"c38b","type":"P","href":null,"layout":null,"metadata":null,"text":"Single input to sequential output","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_185.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_185.markups.0":{"type":"STRONG","start":0,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_186":{"id":"fb82ec0892d3_186","name":"c613","type":"P","href":null,"layout":null,"metadata":null,"text":"This is, for example, the case of image captioning: where we fed a picture to the RNN and want to generate a description of it.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_187":{"id":"fb82ec0892d3_187","name":"62e0","type":"P","href":null,"layout":null,"metadata":null,"text":"Deep RNN with Multilayer Perceptron","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_187.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_187.markups.0":{"type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_188":{"id":"fb82ec0892d3_188","name":"2de3","type":"P","href":null,"layout":null,"metadata":null,"text":"Deep architectures of neural networks can represent a function exponentially more efficient than shallow architectures. While recurrent networks are inherently deep in time given each hidden state is a function of all previous hidden states , it has been shown that the internal computation is in fact quite shallow. It is argued that adding one or more nonlinear layers in the transition stages of a RNN can improve overall performance by better disentangling the underlying variations the original input. The deep structures in RNNs with perceptron layers can fall under three categories: input to hidden, hidden to hidden, and hidden to output.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_189":{"id":"fb82ec0892d3_189","name":"1d67","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*tik7NsPCsIBLcUW1XyCxVg.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*tik7NsPCsIBLcUW1XyCxVg.jpeg":{"id":"1*tik7NsPCsIBLcUW1XyCxVg.jpeg","originalHeight":288,"originalWidth":495,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_190":{"id":"fb82ec0892d3_190","name":"73d2","type":"P","href":null,"layout":null,"metadata":null,"text":"Bi-Directional Recurrent Neural Network","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_190.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_190.markups.0":{"type":"STRONG","start":0,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_191":{"id":"fb82ec0892d3_191","name":"ddf8","type":"P","href":null,"layout":null,"metadata":null,"text":"The structure of BRNN is an to split the state neurons of a regular RNN in a part that is responsible for the positive time direction (forward states) and a part for the negative time direction (backward states). Outputs from forward states are not connected to inputs of backward states, and vice versa. If you might have to learn representations from future time steps to better understand the context and eliminate ambiguity. Take the following examples, “He said, Teddy bears are on sale” and “He said, Teddy Roosevelt was a great President”. In the above two sentences, when we are looking at the word “Teddy” and the previous two words “He said”, we might not be able to understand if the sentence refers to the President or Teddy bears. Therefore, to resolve this ambiguity, we need to look ahead. This is what Bidirectional RNNs accomplish.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_192":{"id":"fb82ec0892d3_192","name":"2573","type":"P","href":null,"layout":null,"metadata":null,"text":"The following picture illustrate the General structure of the bidirectional recurrent neural network,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_193":{"id":"fb82ec0892d3_193","name":"d109","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*B5NHtY8_Y4we0DE4Y-acBA.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*B5NHtY8_Y4we0DE4Y-acBA.jpeg":{"id":"1*B5NHtY8_Y4we0DE4Y-acBA.jpeg","originalHeight":270,"originalWidth":764,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_194":{"id":"fb82ec0892d3_194","name":"20e0","type":"P","href":null,"layout":null,"metadata":null,"text":"Multidimentional Recurrent Neural Network","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_194.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_194.markups.0":{"type":"STRONG","start":0,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_195":{"id":"fb82ec0892d3_195","name":"8a1a","type":"P","href":null,"layout":null,"metadata":null,"text":"The basic idea of multidimensional recurrent neural networks (MDRNNs) is to replace the single recurrent connection found in standard recurrent networks with as many connections as there are spatio-temporal dimensions in the data. These connections allow the network to create a flexible internal representation of surrounding context, which is robust to localised distortions. An MDRNN hidden layer scans through the input in 1D strips, storing its activations in a buffer. The strips are ordered in such a way that at every point the layer has already visited the points one step back along every dimension. The hidden activations at these previous points are fed to the current point through recurrent connections, along with the input.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_196":{"id":"fb82ec0892d3_196","name":"4184","type":"P","href":null,"layout":null,"metadata":null,"text":"RNN architectures used so far have been explicitly one dimensional, meaning that in order to use them for multi-dimensional tasks, the data must be preprocessed to one dimension, for example, by presenting one vertical line of an image at a time to the network. The most successful use of neural networks for multi-dimensional data has been the application of convolution networks to image processing tasks such as digit recognition . One disadvantage of convolution nets is that because they are not recurrent, they rely on hand specified kernel sizes to introduce context. Another disadvantage is that they don’t scale well to large images. For example, sequences of handwritten digits must be pre-segment.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_197":{"id":"fb82ec0892d3_197","name":"1ef7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*NM8sIB-FXDd6nF2HtPgIMQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*NM8sIB-FXDd6nF2HtPgIMQ.jpeg":{"id":"1*NM8sIB-FXDd6nF2HtPgIMQ.jpeg","originalHeight":205,"originalWidth":637,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_198":{"id":"fb82ec0892d3_198","name":"5399","type":"P","href":null,"layout":null,"metadata":null,"text":"Long Short-Term Memory or LSTM Network","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_198.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_198.markups.0":{"type":"STRONG","start":0,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_199":{"id":"fb82ec0892d3_199","name":"4e77","type":"P","href":null,"layout":null,"metadata":null,"text":"An LSTM network is a recurrent neural network that has LSTM cell blocks in place of our standard neural network layers. These cells have various components called the input gate, the forget gate and the output gate. RNNs are good in handling sequential data but they run into problem when the context is far away.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_200":{"id":"fb82ec0892d3_200","name":"f32f","type":"P","href":null,"layout":null,"metadata":null,"text":"Example: I live France and I know ____. The answer must be ‘French’ here but if the there are some more words in between ‘I live in France’ & ‘I know ____’. It’ll be difficult for RNNs to predict ‘French’. This is the problem of Long-Term Dependencies. Hence we come to LSTMs.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_200.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_200.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_200.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_200.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_200.markups.4","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_200.markups.0":{"type":"STRONG","start":0,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_200.markups.1":{"type":"STRONG","start":60,"end":66,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_200.markups.2":{"type":"STRONG","start":122,"end":138,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_200.markups.3":{"type":"STRONG","start":143,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_200.markups.4":{"type":"STRONG","start":206,"end":253,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_201":{"id":"fb82ec0892d3_201","name":"b9e0","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*AqkZ_mXi_5oy4MOjBdbSlg.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_201.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*AqkZ_mXi_5oy4MOjBdbSlg.png":{"id":"1*AqkZ_mXi_5oy4MOjBdbSlg.png","originalHeight":316,"originalWidth":772,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_201.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fhackernoon.com\u002Funderstanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_202":{"id":"fb82ec0892d3_202","name":"f46c","type":"P","href":null,"layout":null,"metadata":null,"text":"LSTMs are explicitly designed to avoid the long-term dependency problem. LSTMs also provide solution to Vanishing\u002FExploding Gradient problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_203":{"id":"fb82ec0892d3_203","name":"8567","type":"P","href":null,"layout":null,"metadata":null,"text":"The picture below illustrate that how input gate, forget get and output gate are working together,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_204":{"id":"fb82ec0892d3_204","name":"7fde","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*w8tI_s56iaq9rOpirydphA.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_204.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*w8tI_s56iaq9rOpirydphA.png":{"id":"1*w8tI_s56iaq9rOpirydphA.png","originalHeight":372,"originalWidth":842,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_204.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fstats.stackexchange.com\u002Fquestions\u002F185639\u002Fhow-does-lstm-prevent-the-vanishing-gradient-problem","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_205":{"id":"fb82ec0892d3_205","name":"ba94","type":"P","href":null,"layout":null,"metadata":null,"text":"Bidirectional LSTM","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_205.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_205.markups.0":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_206":{"id":"fb82ec0892d3_206","name":"84a0","type":"P","href":null,"layout":null,"metadata":null,"text":"The basic idea of bidirectional recurrent neural nets (BRNNs) is to present each training sequence forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer. (In some cases a third network is used in place of the output layer, but here we have used the simpler model). This means that for every point in a given sequence, the BRNN has complete, sequential information about all points before and after it. Also, because the net is free to use as much or as little of this context as necessary, there is no need to find a (task-dependent) time-window or target delay size. BRNNs have given improved results in sequence learning tasks, It is possible to increase capacity of BRNNs by stacking hidden layers of LSTM cells in space, called deep bidirectional LSTM (BLSTM) .","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_207":{"id":"fb82ec0892d3_207","name":"9b9a","type":"P","href":null,"layout":null,"metadata":null,"text":"BLSTM networks are more powerful than unidirectional LSTM networks. These networks theoretically involve all information of input sequences during computation. The distributed representation feature of BLSTM is crucial for different applications such as language understanding . In BLSTM, The forward and backward passes over the unfolded network over time are carried out in a similar way to regular network forward and backward passes, except that we need to unfold the hidden states for all time steps. We also need a special treatment at the beginning and the end of the data points.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_208":{"id":"fb82ec0892d3_208","name":"c51e","type":"P","href":null,"layout":null,"metadata":null,"text":"The picture below illustrate BLSTM for tagging named entities. Multiple tables look up word-level feature vectors,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_209":{"id":"fb82ec0892d3_209","name":"1935","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*qJAZ9tGiKfH6pwl3CTnVxg.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_209.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*qJAZ9tGiKfH6pwl3CTnVxg.png":{"id":"1*qJAZ9tGiKfH6pwl3CTnVxg.png","originalHeight":612,"originalWidth":593,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_209.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fwww.mitpressjournals.org\u002Fdoi\u002Fpdf\u002F10.1162\u002Ftacl_a_00104","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_210":{"id":"fb82ec0892d3_210","name":"58a9","type":"P","href":null,"layout":null,"metadata":null,"text":"The long-short term memory (LSTM) unit with the forget gate allows highly non-trivial long-distance dependencies to be easily learned . For sequential labelling tasks such as NER and speech recognition, a bi-directional LSTM model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context that applies to any feed-forward model. While LSTMs have been studied in the past for the NER task by Hammerton, the lack of computational power (which led to the use of very small models) and quality word embeddings limited their effectiveness.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_211":{"id":"fb82ec0892d3_211","name":"5fd0","type":"P","href":null,"layout":null,"metadata":null,"text":"The picture below illustrate fully connected LSTM works,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_212":{"id":"fb82ec0892d3_212","name":"ae20","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*jDHVuXdVjOhAxnzKuuuY3A.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*jDHVuXdVjOhAxnzKuuuY3A.jpeg":{"id":"1*jDHVuXdVjOhAxnzKuuuY3A.jpeg","originalHeight":396,"originalWidth":850,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_213":{"id":"fb82ec0892d3_213","name":"bbcb","type":"P","href":null,"layout":null,"metadata":null,"text":"LSTM-CRF networks","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_213.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_213.markups.0":{"type":"STRONG","start":0,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_214":{"id":"fb82ec0892d3_214","name":"5687","type":"P","href":null,"layout":null,"metadata":null,"text":"In the CRF networks there are two different ways to make use of neighbor tag information in predicting current tags. The first is to predict a distribution of tags for each time step and then use beam-like decoding to find optimal tag sequences. The work of maximum entropy classifier and Maximum entropy Markov models fall in this category. The second one is to focus on sentence level instead of individual positions, thus leading to Conditional Random Fields (CRF) models. Note that the inputs and outputs are directly connected, as opposed to LSTM and bidirectional LSTM networks where memory cells\u002Frecurrent components are employed. CRFs can produce higher tagging accuracy in general. It is interesting that the relation between these two ways of using tag information bears resemblance to two ways of using input features , and the results in this paper confirms the superiority of BI-LSTM compared to LSTM.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_214.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_214.markups.0":{"type":"STRONG","start":7,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_215":{"id":"fb82ec0892d3_215","name":"e4eb","type":"P","href":null,"layout":null,"metadata":null,"text":"The picture below illustrate working of CRF model,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_216":{"id":"fb82ec0892d3_216","name":"d412","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*-5wU43qeITguMirBlq6BbA.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_216.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-5wU43qeITguMirBlq6BbA.png":{"id":"1*-5wU43qeITguMirBlq6BbA.png","originalHeight":298,"originalWidth":575,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_216.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1508.01991.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_217":{"id":"fb82ec0892d3_217","name":"f4d9","type":"P","href":null,"layout":null,"metadata":null,"text":"In LSTM-CRF networks, It can efficiently use past input features via a LSTM layer and sentence level tag information via a CRF layer. A CRF layer is repre- sented by lines which connect consecutive output layers. A CRF layer has a state transition matrix as parameters. With such a layer, we can efficiently use past and future tags to predict the current tag,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_218":{"id":"fb82ec0892d3_218","name":"97f4","type":"P","href":null,"layout":null,"metadata":null,"text":"The picture below illustrate working of LSTM-CRF model,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_219":{"id":"fb82ec0892d3_219","name":"c28f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*0kD9s1iD9LhJA13Rmq99Rg.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_219.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*0kD9s1iD9LhJA13Rmq99Rg.png":{"id":"1*0kD9s1iD9LhJA13Rmq99Rg.png","originalHeight":null,"originalWidth":null,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_219.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1508.01991.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_220":{"id":"fb82ec0892d3_220","name":"1906","type":"P","href":null,"layout":null,"metadata":null,"text":"The sequence of word representation is regarded as inputs to a bi-directional LSTM, and its output results from the right and left context for each word in a sentence. The output representation from bi-directional LSTM fed onto a CRF layer, the size of representation and its labels are equivalent. In order to consider the neighboring labels, instead of the softmax, we chose CRF as a decision function to yield final label sequence.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_221":{"id":"fb82ec0892d3_221","name":"838f","type":"P","href":null,"layout":null,"metadata":null,"text":"The picture below illustrate working of character level vector concatenated with word embedding as word representation with BLSTM with CRF model,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_222":{"id":"fb82ec0892d3_222","name":"cf4f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*qbBNzLwB3xDa-OQ8o4t7-A.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_222.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*qbBNzLwB3xDa-OQ8o4t7-A.jpeg":{"id":"1*qbBNzLwB3xDa-OQ8o4t7-A.jpeg","originalHeight":449,"originalWidth":550,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_222.markups.0":{"type":"A","start":4,"end":5,"href":"https:\u002F\u002Fwww.mdpi.com\u002F1099-4300\u002F19\u002F6\u002F283\u002Fhtm","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_223":{"id":"fb82ec0892d3_223","name":"b829","type":"P","href":null,"layout":null,"metadata":null,"text":"Gated Recurrent Unit","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_223.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_223.markups.0":{"type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_224":{"id":"fb82ec0892d3_224","name":"90d8","type":"P","href":null,"layout":null,"metadata":null,"text":"A GRU has two gates, a reset gate , and an update gate . Intuitively, the reset gate determines how to combine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. If we set the reset to all 1’s and update gate to all 0’s we again arrive at our plain RNN model. The basic idea of using a gating mechanism to learn long-term dependencies is the same as in a LSTM.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_225":{"id":"fb82ec0892d3_225","name":"7f95","type":"P","href":null,"layout":null,"metadata":null,"text":"In GRU the RNN cell as a computation in which we update the memory vector deciding, at each timestep, which information we want to keep, which information is not relevant anymore and we would like to forget and which information to add from the new input. The RNN cell also creates an output vector which is tightly related to the current hidden state (or memory vector).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_226":{"id":"fb82ec0892d3_226","name":"6e90","type":"P","href":null,"layout":null,"metadata":null,"text":"The picture below illustrate the working of GRU,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_227":{"id":"fb82ec0892d3_227","name":"fe1c","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*rbW4F0626s4-zrgExdmYKQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*rbW4F0626s4-zrgExdmYKQ.jpeg":{"id":"1*rbW4F0626s4-zrgExdmYKQ.jpeg","originalHeight":472,"originalWidth":1060,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_228":{"id":"fb82ec0892d3_228","name":"1524","type":"P","href":null,"layout":null,"metadata":null,"text":"Comparison between LSTM and GRU,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_229":{"id":"fb82ec0892d3_229","name":"479c","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*4qaHeoX6MvJxqdC1Eyu9rw.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_229.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*4qaHeoX6MvJxqdC1Eyu9rw.jpeg":{"id":"1*4qaHeoX6MvJxqdC1Eyu9rw.jpeg","originalHeight":310,"originalWidth":2268,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_229.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.07778.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_230":{"id":"fb82ec0892d3_230","name":"d42c","type":"P","href":null,"layout":null,"metadata":null,"text":"In Emotion classification example from noisy speech, we simulate noisy speech upon superimposing various environmental noises on clean speech. Features are extracted from the noisy speech and feed to the GRU for emotion classification.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_231":{"id":"fb82ec0892d3_231","name":"ede1","type":"P","href":null,"layout":null,"metadata":null,"text":"The picture below illustrate an example of emotion classification from noisy speech using LSTM-GRU,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_232":{"id":"fb82ec0892d3_232","name":"d071","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*MgHI83_gsoi5kP7K1tsGaw.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_232.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*MgHI83_gsoi5kP7K1tsGaw.jpeg":{"id":"1*MgHI83_gsoi5kP7K1tsGaw.jpeg","originalHeight":284,"originalWidth":1095,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_232.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.07778.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_233":{"id":"fb82ec0892d3_233","name":"70e2","type":"P","href":null,"layout":null,"metadata":null,"text":"Character based convolutional gated recurrent encoder with word based gated recurrent decoder with attention (CCEAD)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_233.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_233.markups.0":{"type":"STRONG","start":0,"end":116,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_234":{"id":"fb82ec0892d3_234","name":"ecf7","type":"P","href":null,"layout":null,"metadata":null,"text":"This model has the similar underlying architecture of the sequence-to sequence models . In this model a character based sequence-to-sequence architecture with a convolutional neural network(CNN)-gated recurrent unit (GRU) encoder that captures error representations in noisy text. The decoder of this model is a word based gated recurrent unit (GRU) that gets its initial state from the character encoder and implicitly behaves like a language model.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_235":{"id":"fb82ec0892d3_235","name":"5d70","type":"P","href":null,"layout":null,"metadata":null,"text":"The following is the Architectural diagram of our character based convolutional gated recurrent encoder with word based gated recurrent\ndecoder with attention (CCED),","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_236":{"id":"fb82ec0892d3_236","name":"5d54","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*S9ueXPSmOLAOwhXSMWRtTQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_236.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*S9ueXPSmOLAOwhXSMWRtTQ.jpeg":{"id":"1*S9ueXPSmOLAOwhXSMWRtTQ.jpeg","originalHeight":605,"originalWidth":1115,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_236.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1709.06429","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_237":{"id":"fb82ec0892d3_237","name":"a963","type":"P","href":null,"layout":null,"metadata":null,"text":"The following picture Illustrate the CNN module comprising the encoder of our CCEAD model used for capturing hidden representations in data as,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_238":{"id":"fb82ec0892d3_238","name":"1082","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*jFH3gLT1r3ucKXgJO7GLww.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_238.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*jFH3gLT1r3ucKXgJO7GLww.png":{"id":"1*jFH3gLT1r3ucKXgJO7GLww.png","originalHeight":424,"originalWidth":535,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_238.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1709.06429","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_239":{"id":"fb82ec0892d3_239","name":"533e","type":"P","href":null,"layout":null,"metadata":null,"text":"Word Embedding","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_239.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_239.markups.0":{"type":"STRONG","start":0,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_240":{"id":"fb82ec0892d3_240","name":"151a","type":"P","href":null,"layout":null,"metadata":null,"text":"Word Embedding is a technique for learning dense representation of words in a low dimensional vector space. Each word can be seen as a point in this space, represented by a fixed length vector. Semantic relations between words are captured by this technique. Word Embedding is typically done in the first layer of the network : Embedding layer, that maps a word (index to word in vocabulary) from vocabulary to a dense vector of given size.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_241":{"id":"fb82ec0892d3_241","name":"3057","type":"P","href":null,"layout":null,"metadata":null,"text":"Word2Vec","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_241.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_241.markups.0":{"type":"STRONG","start":0,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_242":{"id":"fb82ec0892d3_242","name":"742b","type":"P","href":null,"layout":null,"metadata":null,"text":"Word2Vec is a method to construct word embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_243":{"id":"fb82ec0892d3_243","name":"d141","type":"P","href":null,"layout":null,"metadata":null,"text":"We can perform some amazing tasks from word embeddings of Word2Vec.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_244":{"id":"fb82ec0892d3_244","name":"460c","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Finding the degree of similarity between two words.\nmodel.similarity('woman','man')\n0.73723527","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_244.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_244.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_244.markups.0":{"type":"CODE","start":52,"end":83,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_244.markups.1":{"type":"CODE","start":84,"end":94,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_245":{"id":"fb82ec0892d3_245","name":"e2e9","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Finding odd one out.\nmodel.doesnt_match('breakfast cereal dinner lunch';.split())\n'cereal'","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_245.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_245.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_245.markups.0":{"type":"CODE","start":21,"end":81,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_245.markups.1":{"type":"CODE","start":82,"end":90,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_246":{"id":"fb82ec0892d3_246","name":"9c4a","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Amazing things like woman+king-man =queen\nmodel.most_similar(positive=['woman','king'],negative=['man'],topn=1)\nqueen: 0.508","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_246.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_246.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_246.markups.0":{"type":"CODE","start":42,"end":111,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_246.markups.1":{"type":"CODE","start":112,"end":124,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_247":{"id":"fb82ec0892d3_247","name":"c612","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Probability of a text under the model\nmodel.score(['The fox jumped over the lazy dog'.split()])\n0.21","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_247.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_247.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_247.markups.0":{"type":"CODE","start":38,"end":95,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_247.markups.1":{"type":"CODE","start":96,"end":100,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_248":{"id":"fb82ec0892d3_248","name":"e6d1","type":"P","href":null,"layout":null,"metadata":null,"text":"GloVe","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_248.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_248.markups.0":{"type":"STRONG","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_249":{"id":"fb82ec0892d3_249","name":"56e9","type":"P","href":null,"layout":null,"metadata":null,"text":"GloVe is a new model for word representation , for Global Vectors, because the global corpus statistics are captured directly by the model. It is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. It is Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval , document classification , question answering , named entity recognition. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favours models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_250":{"id":"fb82ec0892d3_250","name":"94fb","type":"H4","href":null,"layout":null,"metadata":null,"text":"FastText","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_251":{"id":"fb82ec0892d3_251","name":"bd53","type":"P","href":null,"layout":null,"metadata":null,"text":"FastText is an extension to Word2Vec proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word apple is app, ppl, and ple (ignoring the starting and ending of boundaries of words). The word embedding vector for apple will be the sum of all these n grams. After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_251.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_251.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_251.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_251.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_251.markups.0":{"type":"EM","start":225,"end":230,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_251.markups.1":{"type":"EM","start":233,"end":242,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_251.markups.2":{"type":"EM","start":248,"end":251,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_251.markups.3":{"type":"EM","start":341,"end":346,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_252":{"id":"fb82ec0892d3_252","name":"e8af","type":"P","href":null,"layout":null,"metadata":null,"text":"There are many different types of word embeddings:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_253":{"id":"fb82ec0892d3_253","name":"eb87","type":"P","href":null,"layout":null,"metadata":null,"text":"i. Frequency based embedding","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_254":{"id":"fb82ec0892d3_254","name":"25e1","type":"P","href":null,"layout":null,"metadata":null,"text":"ii. Prediction based embedding","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_255":{"id":"fb82ec0892d3_255","name":"7f0c","type":"P","href":null,"layout":null,"metadata":null,"text":"Frequency based embedding","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_255.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_255.markups.0":{"type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_256":{"id":"fb82ec0892d3_256","name":"5607","type":"H4","href":null,"layout":null,"metadata":null,"text":"Count vector","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_257":{"id":"fb82ec0892d3_257","name":"7a83","type":"P","href":null,"layout":null,"metadata":null,"text":"count vector model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_258":{"id":"fb82ec0892d3_258","name":"75ee","type":"P","href":null,"layout":null,"metadata":null,"text":"TF-IDF vectorization","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_258.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_258.markups.0":{"type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_259":{"id":"fb82ec0892d3_259","name":"cd3c","type":"P","href":null,"layout":null,"metadata":null,"text":"Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_259.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_259.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_259.markups.0":{"type":"STRONG","start":9,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_259.markups.1":{"type":"STRONG","start":64,"end":91,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_260":{"id":"fb82ec0892d3_260","name":"37e6","type":"P","href":null,"layout":null,"metadata":null,"text":"In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_261":{"id":"fb82ec0892d3_261","name":"096f","type":"P","href":null,"layout":null,"metadata":null,"text":"In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform. This method takes into account not just the occurrence of a word in a single document but in the entire corpus. lets take a business article this article will contain more business related terms like Stock-market, Prices, shares etc in comparison to any other article. but terms like “a, an, the” will come in each article with high frequency. so this method will penalize these type of high frequency words.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_262":{"id":"fb82ec0892d3_262","name":"ae5f","type":"H4","href":null,"layout":null,"metadata":null,"text":"Co-Occurrence Matrix with a fixed context window","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_263":{"id":"fb82ec0892d3_263","name":"5006","type":"P","href":null,"layout":null,"metadata":null,"text":"Words co-occurrence matrix describes how words occur together that in turn captures the relationships between words. Words co-occurrence matrix is computed simply by counting how two or more words occur together in a given corpus.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_264":{"id":"fb82ec0892d3_264","name":"8287","type":"P","href":null,"layout":null,"metadata":null,"text":"Prediction based embedding","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_264.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_264.markups.0":{"type":"STRONG","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_265":{"id":"fb82ec0892d3_265","name":"62f1","type":"P","href":null,"layout":null,"metadata":null,"text":"Continuous Bag of Words(CBOW)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_265.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_265.markups.0":{"type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_266":{"id":"fb82ec0892d3_266","name":"df5e","type":"P","href":null,"layout":null,"metadata":null,"text":"CBOW is learning to predict the word by the context. A context may be single word or multiple word for a given target words.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_266.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_266.markups.0":{"type":"STRONG","start":0,"end":4,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_267":{"id":"fb82ec0892d3_267","name":"ffa9","type":"P","href":null,"layout":null,"metadata":null,"text":"lets see this by an example “The cat jumped over the puddle.”","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_268":{"id":"fb82ec0892d3_268","name":"4b28","type":"P","href":null,"layout":null,"metadata":null,"text":"So one approach is to treat {“The”, “cat”, ’over”, “the’, “puddle”} as a context and from these words, be able to predict or generate the centre word “jumped”. This type of model we call a Continuous Bag of Words (CBOW) Model.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_269":{"id":"fb82ec0892d3_269","name":"16cd","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture illustrate the representation of CBOW,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_270":{"id":"fb82ec0892d3_270","name":"8ed7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*pgVXxtiHPqzRCQXPtKEBRA.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_270.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*pgVXxtiHPqzRCQXPtKEBRA.png":{"id":"1*pgVXxtiHPqzRCQXPtKEBRA.png","originalHeight":740,"originalWidth":768,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_270.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fiksinc.online\u002Ftag\u002Fcontinuous-bag-of-words-cbow\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_271":{"id":"fb82ec0892d3_271","name":"8fcb","type":"H4","href":null,"layout":null,"metadata":null,"text":"Skip-gram","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_272":{"id":"fb82ec0892d3_272","name":"d78c","type":"P","href":null,"layout":null,"metadata":null,"text":"For skip-gram, the input is the target word, while the outputs are the words surrounding the target words. For instance, in the sentence “I have a cute dog”, the input would be “a”, whereas the output is “I”, “have”, “cute”, and “dog”, assuming the window size is 5. All the input and output data are of the same dimension and one-hot encoded. The network contains 1 hidden layer whose dimension is equal to the embedding size, which is smaller than the input\u002F output vector size. At the end of the output layer, a softmax activation function is applied so that each element of the output vector describes how likely a specific word will appear in the context. The graph below visualizes the network structure.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_272.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_272.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_272.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_272.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_272.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_272.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_272.markups.0":{"type":"EM","start":138,"end":155,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_272.markups.1":{"type":"EM","start":178,"end":179,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_272.markups.2":{"type":"EM","start":205,"end":206,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_272.markups.3":{"type":"EM","start":210,"end":214,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_272.markups.4":{"type":"EM","start":218,"end":222,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_272.markups.5":{"type":"EM","start":230,"end":233,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_273":{"id":"fb82ec0892d3_273","name":"3fb7","type":"P","href":null,"layout":null,"metadata":null,"text":"given the sentence above (“The fluffy dog barked as it chased a cat”) as input a run of the model would look like this:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_273.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_273.markups.0":{"type":"EM","start":26,"end":70,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_274":{"id":"fb82ec0892d3_274","name":"f15d","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s the architecture of our neural network of Skip-gram model,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_275":{"id":"fb82ec0892d3_275","name":"0e56","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*aE6pVoYwKqtaNmANw1WVhQ.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_275.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*aE6pVoYwKqtaNmANw1WVhQ.png":{"id":"1*aE6pVoYwKqtaNmANw1WVhQ.png","originalHeight":571,"originalWidth":778,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_275.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fmedium.com\u002F@jayeshbahire\u002Fintroduction-to-word-vectors-ea1d4e4b84bf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_276":{"id":"fb82ec0892d3_276","name":"be3c","type":"P","href":null,"layout":null,"metadata":null,"text":"Tensorflow implementation of word Embedding","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_276.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_276.markups.0":{"type":"STRONG","start":0,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_277":{"id":"fb82ec0892d3_277","name":"a0e8","type":"P","href":null,"layout":null,"metadata":null,"text":"You can create word embeddings in TensorFlow, we first split the text into words and then assign an integer to every word in the vocabulary. For example, the sentence “I have a cat.” could be split into [“I”, “have”, “a”, “cat”, “.”] and then the corresponding word_ids tensor would have shape [5] and consist of 5 integers. To map these word ids to vectors, we need to create the embedding variable and use thetf.nn.embedding_lookup function as follows:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_277.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_277.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_277.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_277.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_277.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_277.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_277.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_277.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_277.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_277.markups.0":{"type":"CODE","start":203,"end":233,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_277.markups.1":{"type":"CODE","start":261,"end":269,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_277.markups.2":{"type":"CODE","start":294,"end":297,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_277.markups.3":{"type":"CODE","start":411,"end":433,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_277.markups.4":{"type":"A","start":411,"end":433,"href":"https:\u002F\u002Fwww.tensorflow.org\u002Fapi_docs\u002Fpython\u002Ftf\u002Fnn\u002Fembedding_lookup","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_277.markups.5":{"type":"STRONG","start":203,"end":233,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_277.markups.6":{"type":"STRONG","start":261,"end":269,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_277.markups.7":{"type":"STRONG","start":294,"end":297,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_277.markups.8":{"type":"STRONG","start":411,"end":433,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_278":{"id":"fb82ec0892d3_278","name":"7314","type":"PRE","href":null,"layout":null,"metadata":null,"text":"word_embeddings = tf.get_variable(“word_embeddings”,\n    [vocabulary_size, embedding_size])\nembedded_word_ids = tf.nn.embedding_lookup(word_embeddings, word_ids)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_278.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_278.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_278.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_278.markups.0":{"type":"CODE","start":0,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_278.markups.1":{"type":"CODE","start":53,"end":91,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_278.markups.2":{"type":"CODE","start":92,"end":161,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_279":{"id":"fb82ec0892d3_279","name":"842d","type":"P","href":null,"layout":null,"metadata":null,"text":"After this, the tensor embedded_word_ids will have shape [5,embedding_size] in our example and contain the embeddings (dense vectors) for each of the 5 words. At the end of training, word_embeddings will contain the embeddings for all words in the vocabulary.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_279.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_279.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_279.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_279.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_279.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_279.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_279.markups.0":{"type":"CODE","start":23,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_279.markups.1":{"type":"CODE","start":57,"end":75,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_279.markups.2":{"type":"CODE","start":183,"end":198,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_279.markups.3":{"type":"STRONG","start":23,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_279.markups.4":{"type":"STRONG","start":57,"end":75,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_279.markups.5":{"type":"STRONG","start":183,"end":198,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_280":{"id":"fb82ec0892d3_280","name":"3fb5","type":"P","href":null,"layout":null,"metadata":null,"text":"For vector representation of word on Tensorboard, we use following code,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_281":{"id":"fb82ec0892d3_281","name":"95ae","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Merge all the summaries and write them out to \u002Ftmp\u002Flogs (by default)\nmerged = tf.summary.merge_all()\ntrain_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '\u002Ftrain',\n                                      sess.graph)\ntest_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '\u002Ftest')\ntf.global_variables_initializer().run()","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_281.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_281.markups.0":{"type":"CODE","start":0,"end":328,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_282":{"id":"fb82ec0892d3_282","name":"55d1","type":"H4","href":null,"layout":null,"metadata":null,"text":"Launching TensorBoard","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_283":{"id":"fb82ec0892d3_283","name":"a78e","type":"P","href":null,"layout":null,"metadata":null,"text":"To run TensorBoard, use the following command (alternatively python -m tensorboard.main)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_283.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_283.markups.0":{"type":"CODE","start":61,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_284":{"id":"fb82ec0892d3_284","name":"ed92","type":"PRE","href":null,"layout":null,"metadata":null,"text":"tensorboard --logdir=path\u002Fto\u002Flog-directory","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_284.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_284.markups.0":{"type":"CODE","start":0,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_285":{"id":"fb82ec0892d3_285","name":"5eb7","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture shows that vector representation of word on Tensorboard,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_286":{"id":"fb82ec0892d3_286","name":"7b3b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*F-AMMSFrPJ-LfbYmKFtbew.jpeg","typename":"ImageMetadata"},"text":"screenshot","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*F-AMMSFrPJ-LfbYmKFtbew.jpeg":{"id":"1*F-AMMSFrPJ-LfbYmKFtbew.jpeg","originalHeight":768,"originalWidth":1366,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_287":{"id":"fb82ec0892d3_287","name":"3876","type":"P","href":null,"layout":null,"metadata":null,"text":"Word representation is central to natural language processing. The default approach of representing words as discrete and distinct symbols is insufficient for many tasks, and suffers from poor generalization. For example, the symbolic representation of the words “pizza” and “hamburger” are completely unrelated: even if we know that the word “pizza” is a good argument for the verb “eat”, we cannot infer that “hamburger” is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_288":{"id":"fb82ec0892d3_288","name":"754d","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture illustrate the vector representation of word,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_289":{"id":"fb82ec0892d3_289","name":"74be","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*zw-uVf2VANA9VC_Qg7fbKQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*zw-uVf2VANA9VC_Qg7fbKQ.jpeg":{"id":"1*zw-uVf2VANA9VC_Qg7fbKQ.jpeg","originalHeight":529,"originalWidth":631,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_290":{"id":"fb82ec0892d3_290","name":"a3e3","type":"P","href":null,"layout":null,"metadata":null,"text":"7. Topic aware Sequence to Sequence Model with Multihead Attention Mechanism","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_290.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_290.markups.0":{"type":"STRONG","start":0,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_291":{"id":"fb82ec0892d3_291","name":"5f23","type":"P","href":null,"layout":null,"metadata":null,"text":"The Sequence to Sequence model (seq2seq) consists of two RNNs — an encoder and a decoder. The encoder reads the input sequence, word by word and emits a context (a function of final hidden state of encoder), which would ideally capture the essence (semantic summary) of the input sequence. Based on this context, the decoder generates the output sequence, one word at a time while looking at the context and the previous word during each timestep. This is a ridiculous over simplification, but it gives you an idea of what happens in seq2seq.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_292":{"id":"fb82ec0892d3_292","name":"6bef","type":"P","href":null,"layout":null,"metadata":null,"text":"Sequence To Sequence model introduced in Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation has since then, become the Go-To model for Dialogue Systems and Machine Translation. It consists of two RNNs (Recurrent Neural Network) : An Encoder and a Decoder. The encoder takes a sequence(sentence) as input and processes one symbol(word) at each timestep. Its objective is to convert a sequence of symbols into a fixed size feature vector that encodes only the important information in the sequence while losing the unnecessary information. You can visualise data flow in the encoder along the time axis, as the flow of local information from one end of the sequence to another.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_293":{"id":"fb82ec0892d3_293","name":"eeb9","type":"P","href":null,"layout":null,"metadata":null,"text":"Topic aware sequence-to-sequence (TA-Seq2Seq) model in order to leverage topic information as prior knowledge in response generation. TA-Seq2Seq is built on the sequence-to-sequence framework. In encoding, the model represents an input message as hidden vectors by a message encoder, and acquires embeddings of the topic words of the message from a pre-trained LDA model. The topic words are used as a simulation of topical concepts in people’s minds, and obtained from a LDA model which is pre-trained using large scale social media data outside the conversation data. In decoding, each word is generated according to both the message and the topics through a joint attention mechanism. In joint attention, hidden vectors of the message are summarized as context vectors by message attention which follows the existing attention techniques, and embeddings of topic words are synthesized as topic vectors by topic attention. Different from existing attention, in topic attention, the weights of the topic words are calculated by taking the final state of the message as an extra input in order to strengthen the effect of the topic words relevant to the message. The joint attention lets the context vectors and the topic vectors jointly affect response generation, and makes words in responses not only relevant to the input message, but also relevant to the correlated topic information of the message. To model the behavior of people using topical concepts as “building blocks” of their responses, we modify the generation probability of a topic word by adding another probability item which biases the overall distribution and further increases the possibility of the topic word appearing in the response. The results on both automatic evaluation metrics and human annotations show that TA-Seq2Seq can generate more informative, diverse, and topic relevant responses and significantly outperforms state-of-the-art methods for response generation.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_293.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_293.markups.0":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_294":{"id":"fb82ec0892d3_294","name":"54d0","type":"P","href":null,"layout":null,"metadata":null,"text":"Seq2Seq Attention mechanism","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_294.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_294.markups.0":{"type":"STRONG","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_295":{"id":"fb82ec0892d3_295","name":"3b09","type":"P","href":null,"layout":null,"metadata":null,"text":"The traditional Seq2Seq model assumes that every word is generated from the same context vector. In practice, however, different words in Y could be semantically related to different parts of X. To tackle this issue, attention mechanism is introduced into Seq2Seq.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_296":{"id":"fb82ec0892d3_296","name":"0880","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture illustrate the seq2seq attention mechanism,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_297":{"id":"fb82ec0892d3_297","name":"bd0f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*vUmtgeeWvgfSUqj5_xpvyg.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*vUmtgeeWvgfSUqj5_xpvyg.jpeg":{"id":"1*vUmtgeeWvgfSUqj5_xpvyg.jpeg","originalHeight":483,"originalWidth":1002,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_298":{"id":"fb82ec0892d3_298","name":"00c4","type":"P","href":null,"layout":null,"metadata":null,"text":"Sequence-to-sequence model (Seq2Seq) was first proposed in machine translation. The idea was to translate one sequence to another sequence through an encoder-decoder neural architecture. Recently, dialog generation has been treated as sequence translation from a query to a reply.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_299":{"id":"fb82ec0892d3_299","name":"61ee","type":"P","href":null,"layout":null,"metadata":null,"text":"Multi-head Attention Mechanism","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_299.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_299.markups.0":{"type":"STRONG","start":0,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_300":{"id":"fb82ec0892d3_300","name":"744b","type":"P","href":null,"layout":null,"metadata":null,"text":"The context vector obtained by traditional attention mechanism focuses on a specific representation subspace of the input sequence. Such context vector is expected to reflect one aspect of the semantics in the input. However, a sentence usually involves multiple semantics spaces, especially for a long sentence. In multi-head attention mechanism for Seq2Seq model to allow the decoder RNN to jointly attend to information from different representation subspaces of the encoder hidden states at the decoding process. The idea of multi-head has been applied to learn the sentence representation in self-attention.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_301":{"id":"fb82ec0892d3_301","name":"bf86","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture illustrate the working of Multihead encoder-decoder attention mechanism,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_302":{"id":"fb82ec0892d3_302","name":"5c9d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*98QyMYjfclvFXWzG_7YrQA.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*98QyMYjfclvFXWzG_7YrQA.jpeg":{"id":"1*98QyMYjfclvFXWzG_7YrQA.jpeg","originalHeight":354,"originalWidth":682,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_303":{"id":"fb82ec0892d3_303","name":"5f3b","type":"P","href":null,"layout":null,"metadata":null,"text":"Dual Encoder LSTM (DE)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_303.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_303.markups.0":{"type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_304":{"id":"fb82ec0892d3_304","name":"8e29","type":"P","href":null,"layout":null,"metadata":null,"text":"The DE model consists of two RNNs which respectively compute the vector representation of an input context and response.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_305":{"id":"fb82ec0892d3_305","name":"62de","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*TmFYsMQp31j1YZOrAtyL5g.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*TmFYsMQp31j1YZOrAtyL5g.jpeg":{"id":"1*TmFYsMQp31j1YZOrAtyL5g.jpeg","originalHeight":876,"originalWidth":1300,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_306":{"id":"fb82ec0892d3_306","name":"1aa2","type":"P","href":null,"layout":null,"metadata":null,"text":"Dual Encoder LSTM network is just one of many we could apply to this problem and it’s not necessarily the best one. You can come up with all kinds of Deep Learning architectures that haven’t been tried yet — it’s an active research area. For example, the seq2seq model often used in Machine Translation would probably do well on this task. The reason we are going for the Dual Encoder is because it has been reported to give decent performance on this data set. This means we know what to expect and can be sure that our implementation is correct.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_306.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_306.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_306.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_306.markups.0":{"type":"A","start":255,"end":268,"href":"https:\u002F\u002Fwww.tensorflow.org\u002Fversions\u002Fr0.9\u002Ftutorials\u002Fseq2seq\u002Findex.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_306.markups.1":{"type":"A","start":408,"end":416,"href":"http:\u002F\u002Farxiv.org\u002Fabs\u002F1510.03753","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_306.markups.2":{"type":"EM","start":0,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_307":{"id":"fb82ec0892d3_307","name":"edff","type":"P","href":null,"layout":null,"metadata":null,"text":"The following are the working of Dual Encoder,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_308":{"id":"fb82ec0892d3_308","name":"c07a","type":"P","href":null,"layout":null,"metadata":null,"text":"i. Both the context and the response text are split by words, and each word is embedded into a vector. The word embeddings are initialized with Stanford’s GloVe vectors and are fine-tuned during training.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_308.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_308.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_308.markups.0":{"type":"A","start":79,"end":87,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FWord_embedding","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_308.markups.1":{"type":"A","start":155,"end":160,"href":"http:\u002F\u002Fnlp.stanford.edu\u002Fprojects\u002Fglove\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_309":{"id":"fb82ec0892d3_309","name":"050a","type":"P","href":null,"layout":null,"metadata":null,"text":"ii. Both the embedded context and response are fed into the same Recurrent Neural Network word-by-word. The RNN generates a vector representation that, loosely speaking, captures the “meaning” of the context and response.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_310":{"id":"fb82ec0892d3_310","name":"04cc","type":"P","href":null,"layout":null,"metadata":null,"text":"iii. We measure the similarity of the predicted response r' and the actual response r by taking the dot product of these two vectors. A large dot product means the vectors are similar and that the response should receive a high score. We then apply a sigmoid function to convert that score into a probability.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_310.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_310.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_310.markups.0":{"type":"CODE","start":57,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_310.markups.1":{"type":"CODE","start":84,"end":85,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_311":{"id":"fb82ec0892d3_311","name":"44bb","type":"P","href":null,"layout":null,"metadata":null,"text":"8. Neural Response Generation via Generative Adversarial Network","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_311.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_311.markups.0":{"type":"STRONG","start":0,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_312":{"id":"fb82ec0892d3_312","name":"1479","type":"P","href":null,"layout":null,"metadata":null,"text":"Generative Adversarial Nets (GANs) offers an effective architecture of jointly training a generative model and a discriminative classifier to generate sharp and realistic images. This architecture could also potentially be applied to conversational response generation to relieve the safe response problem, where the generative part can be an Seq2Seq-based model that generates response utterances for given queries, and the discriminative part can evaluate the quality of the generated utterances from diverse dimensions according to human-produced responses. However, unlike the image generation problems, training such a GAN for text generation here is not straightforward. The decoding phase of the Seq2Seq model usually involves sampling discrete words from the predicted distributions, which will be fed into the training of the discriminator. The sampling procedure is non-differentiable, and will therefore break the back-propagation. Inspired by recent advances in Neural Machine Translation (NMT). Earlier works focused on paired word sequences only, now we have the mechanism that the comprehensibility of the generated responses can benefit from multiview training with respect to words, coarse tokens and utterances.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_313":{"id":"fb82ec0892d3_313","name":"f708","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*m8aJLFGpxaeKrwIWT4ageQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_313.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*m8aJLFGpxaeKrwIWT4ageQ.jpeg":{"id":"1*m8aJLFGpxaeKrwIWT4ageQ.jpeg","originalHeight":471,"originalWidth":1011,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_313.markups.0":{"type":"A","start":0,"end":6,"href":"http:\u002F\u002Fwww.aclweb.org\u002Fanthology\u002FD17-1065","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_314":{"id":"fb82ec0892d3_314","name":"9ab1","type":"P","href":null,"layout":null,"metadata":null,"text":"Generative model","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_314.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_314.markups.0":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_315":{"id":"fb82ec0892d3_315","name":"9569","type":"P","href":null,"layout":null,"metadata":null,"text":"The generative model G defines the policy that generates a response y given dialogue history x. It takes a form similar to seq2seq models, which first map the source input to a vector representation using a recurrent net and then compute the probability of generating each token in the target using a softmax function.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_316":{"id":"fb82ec0892d3_316","name":"d243","type":"P","href":null,"layout":null,"metadata":null,"text":"Discriminative model","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_316.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_316.markups.0":{"type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_317":{"id":"fb82ec0892d3_317","name":"3af3","type":"P","href":null,"layout":null,"metadata":null,"text":"The discriminative model D is a binary classifier that takes as input a sequence of dialogue utterances {x, y} and outputs a label indicating whether the input is generated by humans or machines. The input dialogue is encoded into a vector representation using a hierarchical encoder 2 which is then fed to a 2-class softmax function, returning the probability of the input dialogue episode being a machine-generated dialogue (denoted Q − ({x, y})) or a human-generated dialogue (denoted Q + ({x, y})).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_318":{"id":"fb82ec0892d3_318","name":"4b63","type":"P","href":null,"layout":null,"metadata":null,"text":"Policy Gradient Training","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_318.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_318.markups.0":{"type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_319":{"id":"fb82ec0892d3_319","name":"aa5e","type":"P","href":null,"layout":null,"metadata":null,"text":"The key idea of the system is to encourage the generator to generate utterances that are indistinguishable from human generated dialogues. We use policy gradient methods to achieve such a goal, in which the score of current utterances being human-generated ones assigned by the discriminator is used as a reward for the generator, which is trained to maximize the expected reward of generated utterance(s) using the REINFORCE algorithm.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_320":{"id":"fb82ec0892d3_320","name":"0106","type":"P","href":null,"layout":null,"metadata":null,"text":"Reward for Every Generation Step","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_320.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_320.markups.0":{"type":"STRONG","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_321":{"id":"fb82ec0892d3_321","name":"37f4","type":"P","href":null,"layout":null,"metadata":null,"text":"Suppose, for example, the input history is what’s your name, the human-generated response is I am John, and the machine-generated response is I don’t know. The vanilla REINFORCE model assigns the same negative reward to all tokens within the human-generated response (i.e., I, don’t, know), whereas proper credit assignment in training would give separate rewards, most likely a neutral reward for the token I, and negative rewards to don’t and know. We call this reward for every generation step, abbreviated REGS. Rewards for intermediate steps or partially decoded sequences are thus necessary. Unfortunately, the discriminator is trained to assign scores to fully generated sequences, but not partially decoded ones. We propose two strategies for computing intermediate step rewards by (1) using Monte Carlo (MC) search and (2) training a discriminator that is able to assign rewards to partially decoded sequences.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_322":{"id":"fb82ec0892d3_322","name":"c78c","type":"P","href":null,"layout":null,"metadata":null,"text":"9. Machine Reading for Question Answering","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_322.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_322.markups.0":{"type":"STRONG","start":0,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_323":{"id":"fb82ec0892d3_323","name":"a01e","type":"P","href":null,"layout":null,"metadata":null,"text":"Machine Reading Comprehension (MRC) is a challenging task: the goal is to have machines read a (set of) text passage(s) and then answer any question about the passage(s). The MRC model is the core component of text-QA agents.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_324":{"id":"fb82ec0892d3_324","name":"a4e4","type":"P","href":null,"layout":null,"metadata":null,"text":"Consider an example as given the question “will I qualify for OSAP if I’m new in Canada”, one might first locate the relevant passage that include: “you must be a 1 Canadian citizen; 2 permanent resident; or 3 protected person…” and reason that being new to the country is usually the opposite of citizen, permanent resident etc., thus determine the correct answer: “no, you won’t qualify”.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_324.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_324.markups.0":{"type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_325":{"id":"fb82ec0892d3_325","name":"5fc6","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*gG78r87vdPVl69NN0w8wLA.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_325.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*gG78r87vdPVl69NN0w8wLA.jpeg":{"id":"1*gG78r87vdPVl69NN0w8wLA.jpeg","originalHeight":341,"originalWidth":859,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_325.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fdl.acm.org\u002Fcitation.cfm?id=3210183","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_326":{"id":"fb82ec0892d3_326","name":"55ea","type":"P","href":null,"layout":null,"metadata":null,"text":"Neural MRC Models","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_326.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_326.markups.0":{"type":"STRONG","start":0,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_327":{"id":"fb82ec0892d3_327","name":"2579","type":"P","href":null,"layout":null,"metadata":null,"text":"In spite of the variety of model structures and attention types , a typical neural MRC model performs reading comprehension in three steps, as (1) encoding the symbolic representation of the questions and passages into a set of vectors in a neural space; (2) reasoning in the neural space to identify the answer vector (e.g., in SQuAD, this is equivalent to ranking and re-ranking the embedded vectors of all possible text spans in P ). and (3) decoding the answer vector into a natural language output in the symbolic space (e.g., this is equivalent to mapping the answer vector to its text span in P ).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_328":{"id":"fb82ec0892d3_328","name":"a49d","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture illustrate the working of Machine reading comprehension,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_329":{"id":"fb82ec0892d3_329","name":"8930","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*FWNrrd1l4xlaAitmly6_QQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_329.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*FWNrrd1l4xlaAitmly6_QQ.jpeg":{"id":"1*FWNrrd1l4xlaAitmly6_QQ.jpeg","originalHeight":412,"originalWidth":889,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_329.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fdl.acm.org\u002Fcitation.cfm?id=3210183","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_330":{"id":"fb82ec0892d3_330","name":"976a","type":"P","href":null,"layout":null,"metadata":null,"text":"Encoding in MRC","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_330.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_330.markups.0":{"type":"STRONG","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_331":{"id":"fb82ec0892d3_331","name":"95b7","type":"P","href":null,"layout":null,"metadata":null,"text":"Most MRC models encode questions and passages through three layers: lexicon embedding layer,contextual embedding layer and attention layer.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_332":{"id":"fb82ec0892d3_332","name":"2877","type":"P","href":null,"layout":null,"metadata":null,"text":"Lexicon Embedding Layer","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_332.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_332.markups.0":{"type":"STRONG","start":0,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_333":{"id":"fb82ec0892d3_333","name":"85c2","type":"P","href":null,"layout":null,"metadata":null,"text":"It extracts information from Q and P at the word level and normalizes for lexical variants. It typically maps each word to a vector space using a pre-trained word embedding model, such as word2vec or GloVe. such that semantically similar words are mapped to the vectors that are close to each other in the neural space. Word embedding can be enhanced by concatenating each word embedding vector with other linguistic embeddings such as those derived from characters, Part-Of-Speech (POS) tags, and named entities etc.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_334":{"id":"fb82ec0892d3_334","name":"0e6a","type":"P","href":null,"layout":null,"metadata":null,"text":"Contextual Embedding Layer","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_334.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_334.markups.0":{"type":"STRONG","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_335":{"id":"fb82ec0892d3_335","name":"d11d","type":"P","href":null,"layout":null,"metadata":null,"text":"It utilizes contextual cues from surrounding words to refine the embedding of the words. As a result, the same word might map to different vectors in a neural space depending on its context, such as “bank of a river” vs. “ bank of America”. This is typically achieved by using a Bi-directional Long Short-Term Memory (BiLSTM) network.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_336":{"id":"fb82ec0892d3_336","name":"c03c","type":"P","href":null,"layout":null,"metadata":null,"text":"Attention Layer","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_336.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_336.markups.0":{"type":"STRONG","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_337":{"id":"fb82ec0892d3_337","name":"e8cd","type":"P","href":null,"layout":null,"metadata":null,"text":"It couples the question and passage vectors and produces a set of query-aware feature vectors for each word in the passage, and generates the working memory M over which reasoning is performed.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_338":{"id":"fb82ec0892d3_338","name":"86a4","type":"P","href":null,"layout":null,"metadata":null,"text":"Reasoning","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_338.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_338.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_339":{"id":"fb82ec0892d3_339","name":"61f0","type":"P","href":null,"layout":null,"metadata":null,"text":"MRC models can be grouped into different categories based on how they perform reasoning to generate the answer: single-step and multi-step models.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_340":{"id":"fb82ec0892d3_340","name":"a17f","type":"P","href":null,"layout":null,"metadata":null,"text":"Single-Step Reasoning","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_340.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_340.markups.0":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_341":{"id":"fb82ec0892d3_341","name":"a2d8","type":"P","href":null,"layout":null,"metadata":null,"text":"A single-step reasoning model matches the question and document only once and produce the final answers.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_342":{"id":"fb82ec0892d3_342","name":"9f2a","type":"P","href":null,"layout":null,"metadata":null,"text":"Multi-Step Reasoning.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_342.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_342.markups.0":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_343":{"id":"fb82ec0892d3_343","name":"d8f8","type":"P","href":null,"layout":null,"metadata":null,"text":"Multi-step reasoning models are the dynamic multi-step reasoning models have to be trained using RL methods, e.g., policy gradient, which are tricky to implement due to the instability issue. SAN combines the strengths of both types of multi-step reasoning models.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_344":{"id":"fb82ec0892d3_344","name":"7bca","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Y9YLiuCn6zXeQhs9W-QAWA.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_344.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Y9YLiuCn6zXeQhs9W-QAWA.jpeg":{"id":"1*Y9YLiuCn6zXeQhs9W-QAWA.jpeg","originalHeight":358,"originalWidth":854,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_344.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fdl.acm.org\u002Fcitation.cfm?id=3210183","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_345":{"id":"fb82ec0892d3_345","name":"c618","type":"P","href":null,"layout":null,"metadata":null,"text":"10. Goal-Oriented Dialog Management for Conversational AI with Transfer Learning","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_345.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_345.markups.0":{"type":"STRONG","start":0,"end":80,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_346":{"id":"fb82ec0892d3_346","name":"3ad2","type":"P","href":null,"layout":null,"metadata":null,"text":"Transfer Learning","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_346.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_346.markups.0":{"type":"STRONG","start":0,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_347":{"id":"fb82ec0892d3_347","name":"1f47","type":"P","href":null,"layout":null,"metadata":null,"text":"The main goal of this work is to study the impact of a widely used technique Transfer Learning on goal oriented bots. As the name suggests, transfer learning transfers knowledge from one neural network to another. The former is known as the source, while the latter is the target. The goal of the transfer is to achieve better performance on the target domain with limited amount of training data, while benefiting from additional information from the source domain. In the case of dialogue systems, the input space for both source and target nets are their respective dialogue spaces.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_348":{"id":"fb82ec0892d3_348","name":"f96d","type":"P","href":null,"layout":null,"metadata":null,"text":"The klGoal-oriented bots contain an initial natural understanding (NLU) component, that is tasked with determining the user’s intent and its parameters, also known as slots . The usual practice in the RL-based Goal-Oriented Chatbots is to define the user-bot interactions as semantic frames. The entire dialogue can be reduced to a set of slot-value pairs, called semantic frames. Consequently, the conversation can be executed on two distinct levels: Semantic level: In this level the user sends and receives only a semantic frames as messages.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_348.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_348.markups.0":{"type":"STRONG","start":452,"end":467,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_349":{"id":"fb82ec0892d3_349","name":"7d75","type":"P","href":null,"layout":null,"metadata":null,"text":"Natural language level: In this level the user sends and receives natural language sentences, which are reduced to, or derived from a semantic frame by using Natural Language Understanding (NLU) and Natural Language Generation (NLG) units respectively. It consists of two independent units which are the User Simulator on the left side and the Dialogue Manager (DM) on the right side. We operate on the semantic level, removing the noise introduced by the NLU and NLG units.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_349.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_349.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_349.markups.0":{"type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_349.markups.1":{"type":"STRONG","start":37,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_350":{"id":"fb82ec0892d3_350","name":"a44d","type":"P","href":null,"layout":null,"metadata":null,"text":"There are some mechanism which is used in Goal-Oriented Dialog Management for Conversational AI with Transfer Learning which explain as below","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_351":{"id":"fb82ec0892d3_351","name":"ba3e","type":"P","href":null,"layout":null,"metadata":null,"text":"User Simulator","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_351.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_351.markups.0":{"type":"STRONG","start":0,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_352":{"id":"fb82ec0892d3_352","name":"6ded","type":"P","href":null,"layout":null,"metadata":null,"text":"The User Simulator creates a user — bot conversation, given the semantic frames. Because the model is based on Reinforcement Learning, a dialogue simulation is necessary to successfully train the model. The user goal consists of two different sets of slots as inform slots and request slots. Inform slots are the slots for which the user knows the value, i.e. they represent the user constraints (e.g. {movie name: “avengers”, number of people: “3”, date: “tomorrow”}) and Request slots are ones for which the user is looking for an answer (e.g. { city, theater, start time } }).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_352.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_352.markups.0":{"type":"STRONG","start":259,"end":260,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_353":{"id":"fb82ec0892d3_353","name":"200b","type":"P","href":null,"layout":null,"metadata":null,"text":"Dialogue Manager","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_353.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_353.markups.0":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_354":{"id":"fb82ec0892d3_354","name":"d6c3","type":"P","href":null,"layout":null,"metadata":null,"text":"The Dialogue Manager (DM), as its name suggests, manages the dialogue flow in order to conduct a proper dialogue with the user. The DM is composed by two trainable sub components: the Dialogue State Tracker (DST) and the Policy Learning Module, i.e. the agent. Additionally, the Dialogue Manager exploits an external Knowledge Base (KB), to find and suggest values for the user requests. Therefore, it plays a central role in the entire Dialogue System.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_355":{"id":"fb82ec0892d3_355","name":"942f","type":"P","href":null,"layout":null,"metadata":null,"text":"Dialogue State Tracker","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_355.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_355.markups.0":{"type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_356":{"id":"fb82ec0892d3_356","name":"c17e","type":"P","href":null,"layout":null,"metadata":null,"text":"The responsibility of the Dialogue State Tracker (DST) is to build a reliable and robust representation of the current state of the dialogue. All system actions are based on the current dialogue state. It keeps track of the history of the user utterances, system actions and the querying results from the Knowledge Base. It extracts features and creates a vector embedding of the current dialogue state, which is exposed and used by the Policy Learning module later on. In order to produce the embeddings, the Dialogue State Tracker must know the type of all slots and intents that might occur during the dialogue. Since we operate on a semantic level (i.e. not introducing any additional noise), we employ a rule-based state tracker.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_357":{"id":"fb82ec0892d3_357","name":"a402","type":"P","href":null,"layout":null,"metadata":null,"text":"Policy Learning","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_357.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_357.markups.0":{"type":"STRONG","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_358":{"id":"fb82ec0892d3_358","name":"e131","type":"P","href":null,"layout":null,"metadata":null,"text":"The Policy Learning module selects the next system actions to drive the user towards the goal in the smallest number of steps. It does that by using the deep reinforcement neural networks, called Deep Q-Networks (DQN) .","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_359":{"id":"fb82ec0892d3_359","name":"36dc","type":"P","href":null,"layout":null,"metadata":null,"text":"The picture illustrate below shows the difference between with and without Transfer Learning Technique,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_360":{"id":"fb82ec0892d3_360","name":"37d3","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*poRTfXSDPUAofHti5QiYPw.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_360.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*poRTfXSDPUAofHti5QiYPw.jpeg":{"id":"1*poRTfXSDPUAofHti5QiYPw.jpeg","originalHeight":398,"originalWidth":1097,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_360.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fdl.acm.org\u002Fcitation.cfm?id=3210183","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_361":{"id":"fb82ec0892d3_361","name":"a90b","type":"P","href":null,"layout":null,"metadata":null,"text":"11. Deep Reinforcement Learning Chatbot Model","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_361.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_361.markups.0":{"type":"STRONG","start":0,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_362":{"id":"fb82ec0892d3_362","name":"46c9","type":"P","href":null,"layout":null,"metadata":null,"text":"The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowd sourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A\u002FB testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data. Our system consists of an ensemble of response models. The response models take as input a dialogue and output a response in natural language text. In addition, the response models may also output one or several scalar values, indicating their internal confidence. As will be explained later, the response models have been engineered to generate responses on a diverse set of topics using a variety of strategies.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_363":{"id":"fb82ec0892d3_363","name":"711c","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture illustrate the work flow of response generation and evaluation of in Deep reinforcement learning algorithm,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_364":{"id":"fb82ec0892d3_364","name":"a5f2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*zKtuBsBSnpPzOfasZ2yx-A.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_364.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*zKtuBsBSnpPzOfasZ2yx-A.jpeg":{"id":"1*zKtuBsBSnpPzOfasZ2yx-A.jpeg","originalHeight":347,"originalWidth":769,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_364.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fdl.acm.org\u002Fcitation.cfm?id=3210183","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_365":{"id":"fb82ec0892d3_365","name":"2905","type":"P","href":null,"layout":null,"metadata":null,"text":"The dialogue manager is responsible for combining the response models together. As input, the dialogue manager expects to be given a dialogue history (i.e. all utterances recorded in the dialogue so far, including the current user utterance) and confidence values of the automatic speech recognition system (ASR confidences) or text based generated response. To generate a response, the dialogue manager follows a three-step procedure. First, it uses all response models to generate a set of candidate responses. Second, if there exists a priority response in the set of candidate responses (i.e. a response which takes precedence over other responses), this response will be returned by the system. For example, for the question “What is your name?”, the response “I am an Alexa Prize socialbot” is a priority response. Third, if there are no priority responses, the response is selected by the model selection policy. For example, the model selection policy may select a response by scoring all candidate responses and picking the highest-scored response.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_366":{"id":"fb82ec0892d3_366","name":"ae9d","type":"P","href":null,"layout":null,"metadata":null,"text":"Response Models","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_366.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_366.markups.0":{"type":"STRONG","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_367":{"id":"fb82ec0892d3_367","name":"6d85","type":"P","href":null,"layout":null,"metadata":null,"text":"There are 22 response models in the system, including retrieval-based neural networks, generation-based neural networks, knowledge base question answering systems and template-based systems.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_368":{"id":"fb82ec0892d3_368","name":"01dc","type":"P","href":null,"layout":null,"metadata":null,"text":"Template-based Models","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_368.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_368.markups.0":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_369":{"id":"fb82ec0892d3_369","name":"cb70","type":"P","href":null,"layout":null,"metadata":null,"text":"Templates to produce a response given the dialogue history and user utterance By default all templates generate non-priority responses, so we configure templates related to the socialbot’s name, age and location to output priority responses. We modify a few templates further to make them consistent with the challenge (e.g. to avoid obscene language and to encourage the user to discuss certain topics, such as news, politics and movies). The majority of templates remain unchanged.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_370":{"id":"fb82ec0892d3_370","name":"44f2","type":"P","href":null,"layout":null,"metadata":null,"text":"Knowledge Base-based Question Answering","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_370.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_370.markups.0":{"type":"STRONG","start":0,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_371":{"id":"fb82ec0892d3_371","name":"be42","type":"P","href":null,"layout":null,"metadata":null,"text":"They use a policy-based agent with continuous states based on KB embeddings to traverse the knowledge graph to identify the answer node (entity) for an input query. The RL-based methods are as robust as the neural methods due to the use of continuous vectors for state representation, and are as interpretable as symbolic methods because the agents explicitly traverse the paths in the graph.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_372":{"id":"fb82ec0892d3_372","name":"483e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*xgti_ctfHtVY_p9hsgY8WQ.jpeg","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_372.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*xgti_ctfHtVY_p9hsgY8WQ.jpeg":{"id":"1*xgti_ctfHtVY_p9hsgY8WQ.jpeg","originalHeight":319,"originalWidth":808,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_372.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Fdl.acm.org\u002Fcitation.cfm?id=3210183","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_373":{"id":"fb82ec0892d3_373","name":"84d3","type":"P","href":null,"layout":null,"metadata":null,"text":"Retrieval-based Neural Networks","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_373.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_373.markups.0":{"type":"STRONG","start":0,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_374":{"id":"fb82ec0892d3_374","name":"7c45","type":"P","href":null,"layout":null,"metadata":null,"text":"VHRED models: The system contains several VHRED models (Latent Variable Hierarchical Recurrent Encoder-Decoder) , sequence-to-sequence models with Gaussian latent variables trained as variational auto-encoders . The trained VHRED models generate candidate responses as follows. First, a set of K model responses are retrieved from a dataset using cosine similarity between the current dialogue history and the dialogue history in the dataset based on bag-of-words TF-IDF Glove word embeddings. An approximation of the log-likelihood for each of the 20 responses is computed by VHRED, and the response with the highest log-likelihood is returned.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_374.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_374.markups.0":{"type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_375":{"id":"fb82ec0892d3_375","name":"5093","type":"P","href":null,"layout":null,"metadata":null,"text":"Bag-of-words Retrieval Models: The system contains three bag-of-words retrieval models based on TF-IDF Glove word embeddings and Word2Vec embeddings. Similar to the VHRED models, these models retrieve the response with the highest cosine similarity.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_375.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_375.markups.0":{"type":"STRONG","start":0,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_376":{"id":"fb82ec0892d3_376","name":"576d","type":"P","href":null,"layout":null,"metadata":null,"text":"Retrieval-based Logistic Regression","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_376.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_376.markups.0":{"type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_377":{"id":"fb82ec0892d3_377","name":"169a","type":"P","href":null,"layout":null,"metadata":null,"text":"The system contains a response model, called BoWEscapePlan, which returns a response from a set of 35 topic-independent, generic pre-defined responses, such as “Could you repeat that again”, “I don’t know” and “Was that a question?”. Its main purpose is to maintain user engagement and keep the conversation going, when other models are unable to provide meaningful responses. This model uses a logistic regression classifier to select its response based on a set of higher-level features.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_378":{"id":"fb82ec0892d3_378","name":"2458","type":"P","href":null,"layout":null,"metadata":null,"text":"Search Engine-based Neural Networks","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_378.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_378.markups.0":{"type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_379":{"id":"fb82ec0892d3_379","name":"6343","type":"P","href":null,"layout":null,"metadata":null,"text":"The system contains a deep classifier model, called LSTMClassifierMSMarco, which chooses its response from a set of search engine results. The system searches the web with the last user utterance. as query, and retrieves the first 10 search snippets. The retrieved snippets are preprocessed by stripping trailing words, removing unnecessary punctuation and truncating to the last full sentence. The model uses a bidirectional LSTM to separately map the last dialogue utterance and the snippet to their own embedding vectors. The resulting two representations are concatenated and passed through an MLP to predict a scalar-value between 0 − 1 indicating how appropriate the snippet is as a response to the utterance.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_380":{"id":"fb82ec0892d3_380","name":"b93c","type":"P","href":null,"layout":null,"metadata":null,"text":"Generation-based Neural Networks","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_380.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_380.markups.0":{"type":"STRONG","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_381":{"id":"fb82ec0892d3_381","name":"38fa","type":"P","href":null,"layout":null,"metadata":null,"text":"The system contains a generative recurrent neural network language model, called GRUQuestion-Generator, which can generate follow-up questions word-by-word, conditioned on the dialogue history. The input to the model consists of three components: a one-hot vector of the current word, a binary question label and a binary speaker label. The model contains two GRU layers and softmax output layer.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_382":{"id":"fb82ec0892d3_382","name":"d064","type":"P","href":null,"layout":null,"metadata":null,"text":"Model Selection Policy and Architecture","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_382.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_382.markups.0":{"type":"STRONG","start":0,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_383":{"id":"fb82ec0892d3_383","name":"388d","type":"P","href":null,"layout":null,"metadata":null,"text":"After generating the candidate response set, the dialogue manager uses a model selection policy to select the response it returns to the user. The dialogue manager must select a response which increases the satisfaction of the user for the entire dialogue. It must make a trade-off between immediate and long-term user satisfaction. For example, suppose the user asks to talk about politics. If the dialogue manager chooses to respond with a political joke, the user may be pleased for one turn. Afterwards, however, the user may be disappointed with the system’s inability to debate political topics. Instead, if the dialogue manager chooses to respond with a short news story, the user may be less pleased for one turn. However, the news story may influence the user to follow up with factual questions, which the system may be better adept at handling. To make the trade-off between immediate and long-term user satisfaction, we consider selecting the appropriate response as a sequential decision making problem. This section describes five approaches to learn the model selection policy. These approaches are all evaluated with real-world users in the next section.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_384":{"id":"fb82ec0892d3_384","name":"38c4","type":"P","href":null,"layout":null,"metadata":null,"text":"Action-value Parametrization","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_384.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_384.markups.0":{"type":"STRONG","start":0,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_385":{"id":"fb82ec0892d3_385","name":"a386","type":"P","href":null,"layout":null,"metadata":null,"text":"The use of an action-value function for selecting dialogue responses is closely related to where a model is learned to predict the quality of a dialogue system response.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_386":{"id":"fb82ec0892d3_386","name":"c5c1","type":"P","href":null,"layout":null,"metadata":null,"text":"Model Architecture","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_386.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_386.markups.0":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_387":{"id":"fb82ec0892d3_387","name":"695b","type":"P","href":null,"layout":null,"metadata":null,"text":"The below diagram represents the flow of a single round, one complete loop, in training with Reinforcement Learning algorithm,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_388":{"id":"fb82ec0892d3_388","name":"bdbd","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*nXo_4fkIorrGdsFGRdhJ2w.png","typename":"ImageMetadata"},"text":"source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_388.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*nXo_4fkIorrGdsFGRdhJ2w.png":{"id":"1*nXo_4fkIorrGdsFGRdhJ2w.png","originalHeight":510,"originalWidth":800,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_388.markups.0":{"type":"A","start":0,"end":6,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Ftraining-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_389":{"id":"fb82ec0892d3_389","name":"1b72","type":"P","href":null,"layout":null,"metadata":null,"text":"Chatbot Query and Response","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_389.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_389.markups.0":{"type":"STRONG","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_390":{"id":"fb82ec0892d3_390","name":"4432","type":"P","href":null,"layout":null,"metadata":null,"text":"Example 1 (A & B are both a trained chatbot)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_390.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_390.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_391":{"id":"fb82ec0892d3_391","name":"f51c","type":"P","href":null,"layout":null,"metadata":null,"text":"A: Can you do me a favor?","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_392":{"id":"fb82ec0892d3_392","name":"6194","type":"P","href":null,"layout":null,"metadata":null,"text":"B: Anything.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_393":{"id":"fb82ec0892d3_393","name":"b1b4","type":"P","href":null,"layout":null,"metadata":null,"text":"A: I want you to bend a father.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_394":{"id":"fb82ec0892d3_394","name":"8220","type":"P","href":null,"layout":null,"metadata":null,"text":"B: Of course.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_395":{"id":"fb82ec0892d3_395","name":"7cce","type":"P","href":null,"layout":null,"metadata":null,"text":"A: I’m so sorry I didn’t mean to be rude but.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_396":{"id":"fb82ec0892d3_396","name":"f5ab","type":"P","href":null,"layout":null,"metadata":null,"text":"B: Don’t get it at least it’s about as much as I can.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_397":{"id":"fb82ec0892d3_397","name":"b92c","type":"P","href":null,"layout":null,"metadata":null,"text":"Example 2 (A & B are both a trained chatbot)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_397.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_397.markups.0":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_398":{"id":"fb82ec0892d3_398","name":"32e3","type":"P","href":null,"layout":null,"metadata":null,"text":"A: I didn’t do it! I haven’t got a gun!","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_399":{"id":"fb82ec0892d3_399","name":"6fe7","type":"P","href":null,"layout":null,"metadata":null,"text":"B: You liar. You still want to get your money back.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_400":{"id":"fb82ec0892d3_400","name":"a2a3","type":"P","href":null,"layout":null,"metadata":null,"text":"A: Something wrong with the truth.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_401":{"id":"fb82ec0892d3_401","name":"9e2a","type":"P","href":null,"layout":null,"metadata":null,"text":"B: I don’t want any money just a little I can’t take just out of this.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_402":{"id":"fb82ec0892d3_402","name":"b849","type":"P","href":null,"layout":null,"metadata":null,"text":"A: What’s wrong.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_403":{"id":"fb82ec0892d3_403","name":"0345","type":"P","href":null,"layout":null,"metadata":null,"text":"B: Listen to me. I’m scared now.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_404":{"id":"fb82ec0892d3_404","name":"4ab5","type":"P","href":null,"layout":null,"metadata":null,"text":"A: Ah you’ll need all the money I need your help.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_405":{"id":"fb82ec0892d3_405","name":"95ef","type":"P","href":null,"layout":null,"metadata":null,"text":"12. Coding and Implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_405.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_405.markups.0":{"type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_406":{"id":"fb82ec0892d3_406","name":"c13e","type":"H4","href":null,"layout":null,"metadata":null,"text":"Dataset","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_407":{"id":"fb82ec0892d3_407","name":"bfc9","type":"P","href":null,"layout":null,"metadata":null,"text":"In this post I’ll work with the Cornell_Movie-Dialogs_Corpus (link).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_407.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_407.markups.0":{"type":"A","start":62,"end":66,"href":"http:\u002F\u002Fwww.cs.cornell.edu\u002F~cristian\u002FCornell_Movie-Dialogs_Corpus.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_408":{"id":"fb82ec0892d3_408","name":"c571","type":"P","href":null,"layout":null,"metadata":null,"text":"DESCRIPTION:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_408.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_408.markups.0":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_409":{"id":"fb82ec0892d3_409","name":"5a57","type":"P","href":null,"layout":null,"metadata":null,"text":"This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_410":{"id":"fb82ec0892d3_410","name":"444c","type":"P","href":null,"layout":null,"metadata":null,"text":"- 220,579 conversational exchanges between 10,292 pairs of movie characters","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_411":{"id":"fb82ec0892d3_411","name":"464d","type":"P","href":null,"layout":null,"metadata":null,"text":"- involves 9,035 characters from 617 movies","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_412":{"id":"fb82ec0892d3_412","name":"24b7","type":"P","href":null,"layout":null,"metadata":null,"text":"- in total 304,713 utterances","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_413":{"id":"fb82ec0892d3_413","name":"033e","type":"P","href":null,"layout":null,"metadata":null,"text":"- movie metadata included:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_414":{"id":"fb82ec0892d3_414","name":"dd67","type":"P","href":null,"layout":null,"metadata":null,"text":"- genres","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_415":{"id":"fb82ec0892d3_415","name":"860c","type":"P","href":null,"layout":null,"metadata":null,"text":"- release year","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_416":{"id":"fb82ec0892d3_416","name":"b40a","type":"P","href":null,"layout":null,"metadata":null,"text":"- IMDB rating","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_417":{"id":"fb82ec0892d3_417","name":"c54d","type":"P","href":null,"layout":null,"metadata":null,"text":"- number of IMDB votes","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_418":{"id":"fb82ec0892d3_418","name":"467d","type":"P","href":null,"layout":null,"metadata":null,"text":"- IMDB rating","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_419":{"id":"fb82ec0892d3_419","name":"41ff","type":"P","href":null,"layout":null,"metadata":null,"text":"- character metadata included:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_420":{"id":"fb82ec0892d3_420","name":"f269","type":"P","href":null,"layout":null,"metadata":null,"text":"- gender (for 3,774 characters)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_421":{"id":"fb82ec0892d3_421","name":"48ce","type":"P","href":null,"layout":null,"metadata":null,"text":"- position on movie credits (3,321 characters)","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_422":{"id":"fb82ec0892d3_422","name":"0b8a","type":"P","href":null,"layout":null,"metadata":null,"text":"Preprocessing","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_422.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_422.markups.0":{"type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_423":{"id":"fb82ec0892d3_423","name":"dc93","type":"P","href":null,"layout":null,"metadata":null,"text":"The Cornell_Movie-Dialogs_Corpus dataset is a natural language dataset and can’t be used in its exact form. It needs to be converted in a suitable data structure in order to use it for further computation and processing.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_423.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_423.markups.0":{"type":"A","start":4,"end":32,"href":"http:\u002F\u002Fwww.cs.cornell.edu\u002F~cristian\u002FCornell_Movie-Dialogs_Corpus.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_424":{"id":"fb82ec0892d3_424","name":"9414","type":"P","href":null,"layout":null,"metadata":null,"text":"Tokenize- First step in the pre-processing is to tokenize the sentences into different words. For example, ‘Bob dropped the apple. Where is the apple?’ is tokenized to [‘Bob’, ‘dropped’, ‘the’, ‘apple’, ‘.’, ‘Where’, ‘is’, ‘the’, ‘apple’, ‘ ?’]","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_424.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_424.markups.0":{"type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_425":{"id":"fb82ec0892d3_425","name":"c105","type":"P","href":null,"layout":null,"metadata":null,"text":"Splitting into Story, Questions, and answers: Next, the sentences were split into stories, questions and answers so that they can be fed to the proposed models.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_425.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_425.markups.0":{"type":"STRONG","start":0,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_426":{"id":"fb82ec0892d3_426","name":"e2ec","type":"P","href":null,"layout":null,"metadata":null,"text":"Combining all the stories- All the stories were then combined up to the point that the question was asked. This finally becomes the story for that particular question.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_426.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_426.markups.0":{"type":"STRONG","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_427":{"id":"fb82ec0892d3_427","name":"08c5","type":"P","href":null,"layout":null,"metadata":null,"text":"Indexing the stories, questions, and answers- Finally, the questions and stories are indexed according to their time of occurrence and are eventually processed via word2vec model. The answers are transformed to one hot encoded vector.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_427.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_427.markups.0":{"type":"STRONG","start":0,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_428":{"id":"fb82ec0892d3_428","name":"02ed","type":"H4","href":null,"layout":null,"metadata":null,"text":"Creating the model","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_428.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_428.markups.0":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_429":{"id":"fb82ec0892d3_429","name":"1845","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we have inputs, parsing, evaluation and training it’s time to write code for our Dual LSTM neural network. Because we have different formats of training and evaluation data I’ve written a chatbot_model.py wrapper that takes care of bringing the data into the right format for us. It takes a model_impl argument, which is a function that actually makes predictions.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_429.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_429.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_429.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_429.markups.0":{"type":"CODE","start":197,"end":213,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_429.markups.1":{"type":"CODE","start":300,"end":310,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_429.markups.2":{"type":"A","start":197,"end":198,"href":"https:\u002F\u002Fgithub.com\u002Fdennybritz\u002Fchatbot-retrieval\u002Fblob\u002Fmaster\u002Fudc_model.py","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_430":{"id":"fb82ec0892d3_430","name":"2874","type":"P","href":null,"layout":null,"metadata":null,"text":"Defining Evaluation Metrics","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_430.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_430.markups.0":{"type":"STRONG","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_431":{"id":"fb82ec0892d3_431","name":"5788","type":"P","href":null,"layout":null,"metadata":null,"text":"Tensorflow already comes with many standard evaluation metrics that we can use. To use these metrics we need to create a dictionary that maps from a metric name to a function that takes the predictions and label.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_432":{"id":"fb82ec0892d3_432","name":"7ae0","type":"PRE","href":null,"layout":null,"metadata":null,"text":"tf.metrics.accuracy(\n    labels,\n    predictions,\n    weights=None,\n    metrics_collections=None,\n    updates_collections=None,\n    name=None\n)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_432.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_432.markups.0":{"type":"CODE","start":0,"end":143,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_433":{"id":"fb82ec0892d3_433","name":"e312","type":"P","href":null,"layout":null,"metadata":null,"text":"The code is available on my github Profile: github link","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_433.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_433.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_433.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_433.markups.0":{"type":"A","start":28,"end":34,"href":"https:\u002F\u002Fgithub.com\u002FkunalBhashkar\u002Fseq2seq_chatbot_tensorflow","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_433.markups.1":{"type":"A","start":43,"end":55,"href":"https:\u002F\u002Fgithub.com\u002FkunalBhashkar\u002Fseq2seq_chatbot_tensorflow","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_433.markups.2":{"type":"EM","start":43,"end":55,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_434":{"id":"fb82ec0892d3_434","name":"598f","type":"P","href":null,"layout":null,"metadata":null,"text":"The below picture are the some Screenshots of the output,","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_435":{"id":"fb82ec0892d3_435","name":"81ac","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*OfWZKmm36T3W2UK2lekPPg.png","typename":"ImageMetadata"},"text":"screenshot","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_435.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*OfWZKmm36T3W2UK2lekPPg.png":{"id":"1*OfWZKmm36T3W2UK2lekPPg.png","originalHeight":null,"originalWidth":null,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_435.markups.0":{"type":"A","start":0,"end":10,"href":"https:\u002F\u002Fgithub.com\u002FkunalBhashkar\u002Fseq2seq_chatbot_tensorflow\u002Ftree\u002Fmaster\u002Fchatbot_screenshots","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_436":{"id":"fb82ec0892d3_436","name":"a876","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*bNsR9wdKfNE_pt1k-tkCMg.png","typename":"ImageMetadata"},"text":"screenshot","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_436.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*bNsR9wdKfNE_pt1k-tkCMg.png":{"id":"1*bNsR9wdKfNE_pt1k-tkCMg.png","originalHeight":null,"originalWidth":null,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_436.markups.0":{"type":"A","start":0,"end":10,"href":"https:\u002F\u002Fgithub.com\u002FkunalBhashkar\u002Fseq2seq_chatbot_tensorflow","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_437":{"id":"fb82ec0892d3_437","name":"01c6","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Tf-kCVxWEbb2Wgq7xG7WbA.png","typename":"ImageMetadata"},"text":"screenshots","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_437.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Tf-kCVxWEbb2Wgq7xG7WbA.png":{"id":"1*Tf-kCVxWEbb2Wgq7xG7WbA.png","originalHeight":null,"originalWidth":null,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_437.markups.0":{"type":"A","start":0,"end":11,"href":"https:\u002F\u002Fgithub.com\u002FkunalBhashkar\u002Fseq2seq_chatbot_tensorflow","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_438":{"id":"fb82ec0892d3_438","name":"f644","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*vlrl52ujalDaTAxdyXzEdQ.png","typename":"ImageMetadata"},"text":"screenshots","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_438.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*vlrl52ujalDaTAxdyXzEdQ.png":{"id":"1*vlrl52ujalDaTAxdyXzEdQ.png","originalHeight":null,"originalWidth":null,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:fb82ec0892d3_438.markups.0":{"type":"A","start":0,"end":11,"href":"https:\u002F\u002Fgithub.com\u002FkunalBhashkar\u002Fseq2seq_chatbot_tensorflow","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_439":{"id":"fb82ec0892d3_439","name":"2f0b","type":"P","href":null,"layout":null,"metadata":null,"text":"REFERENCES","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_439.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_439.markups.0":{"type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_440":{"id":"fb82ec0892d3_440","name":"afc5","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Deep Learning for Chatbots (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_440.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_440.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_440.markups.0":{"type":"A","start":0,"end":26,"href":"https:\u002F\u002Fscholarworks.sjsu.edu\u002Fcgi\u002Fviewcontent.cgi?article=1645&context=etd_projects","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_440.markups.1":{"type":"A","start":28,"end":33,"href":"https:\u002F\u002Fscholarworks.sjsu.edu\u002Fetd_projects\u002F630\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_441":{"id":"fb82ec0892d3_441","name":"9898","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Dialogue Intent Classification with Long Short-Term Memory Networks (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_441.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_441.markups.0":{"type":"A","start":69,"end":74,"href":"http:\u002F\u002Ftcci.ccf.org.cn\u002Fconference\u002F2017\u002Fpapers\u002F1158.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_442":{"id":"fb82ec0892d3_442","name":"6c15","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Sequence to Sequence Learning with Neural Networks(paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_442.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_442.markups.0":{"type":"A","start":51,"end":56,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1409.3215","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_443":{"id":"fb82ec0892d3_443","name":"869b","type":"ULI","href":null,"layout":null,"metadata":null,"text":"A Neural Conversational Model (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_443.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_443.markups.0":{"type":"A","start":31,"end":36,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1506.05869","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_444":{"id":"fb82ec0892d3_444","name":"69d2","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Neural Machine Translation by Jointly Learning to Align and Translate (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_444.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_444.markups.0":{"type":"A","start":71,"end":76,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1409.0473","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_445":{"id":"fb82ec0892d3_445","name":"5bf3","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Effective Approaches to Attention-based Neural Machine Translation (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_445.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_445.markups.0":{"type":"A","start":68,"end":73,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1508.04025","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_446":{"id":"fb82ec0892d3_446","name":"fe8c","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Neural Approaches to Conversational AI (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_446.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_446.markups.0":{"type":"A","start":40,"end":45,"href":"https:\u002F\u002Fdl.acm.org\u002Fcitation.cfm?id=3210183","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_447":{"id":"fb82ec0892d3_447","name":"6d06","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Neural Response Generation via GAN with an Approximate Embedding Layer (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_447.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_447.markups.0":{"type":"A","start":72,"end":77,"href":"http:\u002F\u002Fwww.aclweb.org\u002Fanthology\u002FD17-1065","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_448":{"id":"fb82ec0892d3_448","name":"46e3","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Task-oriented Conversational Agent Self-learning Based on Sentiment Analysis (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_448.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_448.markups.0":{"type":"A","start":78,"end":83,"href":"http:\u002F\u002Fceur-ws.org\u002FVol-2244\u002Fpaper_01.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_449":{"id":"fb82ec0892d3_449","name":"c1aa","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Deep Reinforcement Learning for Dialogue Generation (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_449.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_449.markups.0":{"type":"A","start":53,"end":58,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1606.01541","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_450":{"id":"fb82ec0892d3_450","name":"c8c8","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Topic Aware Neural Response Generation (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_450.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_450.markups.0":{"type":"A","start":40,"end":45,"href":"http:\u002F\u002Fwww.aaai.org\u002Focs\u002Findex.php\u002FAAAI\u002FAAAI17\u002Fpaper\u002Fdownload\u002F14563\u002F14260","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_451":{"id":"fb82ec0892d3_451","name":"79af","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Response Selection with Topic Clues for Retrieval-based Chatbots (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_451.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_451.markups.0":{"type":"A","start":66,"end":71,"href":"https:\u002F\u002Fwww.sciencedirect.com\u002Fscience\u002Farticle\u002Fpii\u002FS0925231218309093","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_452":{"id":"fb82ec0892d3_452","name":"4175","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Bidirectional Recurrent Neural Networks as Generative Models (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_452.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_452.markups.0":{"type":"A","start":62,"end":67,"href":"http:\u002F\u002Fpapers.nips.cc\u002Fpaper\u002F5651-bidirectional-recurrent-neural-networks-as-generative-models","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_453":{"id":"fb82ec0892d3_453","name":"0078","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Adversarial-Learning-for-Generative-Conversational-Agents (link)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_453.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_453.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_453.markups.0":{"type":"A","start":0,"end":57,"href":"https:\u002F\u002Fgithub.com\u002Foswaldoludwig\u002FAdversarial-Learning-for-Generative-Conversational-Agents","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_453.markups.1":{"type":"A","start":59,"end":63,"href":"https:\u002F\u002Fgithub.com\u002Foswaldoludwig\u002FAdversarial-Learning-for-Generative-Conversational-Agents","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_454":{"id":"fb82ec0892d3_454","name":"f9dc","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Few-Shot Generalization Across Dialogue Tasks (link)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_454.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_454.markups.0":{"type":"A","start":47,"end":51,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1811.11707","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:fb82ec0892d3_455":{"id":"fb82ec0892d3_455","name":"e239","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Neural Networks for Text Correction and Completion in Keyboard Decoding (paper)","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:fb82ec0892d3_455.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:fb82ec0892d3_455.markups.0":{"type":"A","start":73,"end":78,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1709.06429","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"$Post:38dc5cf5a5a3.previewContent":{"subtitle":"keywords: NLU, NLG, Word Embedding, RNN, Bi-directional LSTM, Generative Adversarial Network, Machine Reading Comprehension, Transfer…","__typename":"PreviewContent"},"$Post:38dc5cf5a5a3.responses":{"stream":[],"__typename":"StreamConnection"}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.fa68a4a6.js"></script><script src="https://cdn-client.medium.com/lite/static/js/vendors~main.34c849d5.chunk.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.9983ec64.chunk.js"></script><script src="https://cdn-client.medium.com/lite/static/js/vendors~screen.collection.packageBuilder~screen.collection.styleEditor~screen.landingpages.pres45~sc~643621df.0eae2f52.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/vendors~screen.post~screen.post.amp~screen.post.series~screen.sequence.post.b90b8296.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/screen.collection.packageBuilder~screen.collection.styleEditor~screen.landingpages.pres45~screen.lan~dc41d166.98208092.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/screen.collection.packageBuilder~screen.collection.styleEditor~screen.landingpages.pres45~screen.lan~83d9351d.0554d7e3.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/screen.post.796bd162.chunk.js"></script><script>window.main();</script></body></html>